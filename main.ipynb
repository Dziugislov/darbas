{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf42847e",
   "metadata": {},
   "source": [
    "# SMA Strategy Backtesting System\n",
    "This notebook combines the functionality from multiple files into a single interactive notebook format.\n",
    "The system implements a Simple Moving Average (SMA) crossover strategy with ATR-based position sizing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984e469",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "Import required libraries and set up basic configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92117b47",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import struct\n",
    "from typing import Dict, Tuple, List\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.patches import Circle\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import openpyxl\n",
    "import input  # Import the module instead of individual variables\n",
    "from scipy.spatial.distance import cdist\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.spatial.distance import pdist\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Import configuration\n",
    "from input import MIN_TRADES, MAX_TRADES, MIN_ELEMENTS_PER_CLUSTER, DEFAULT_NUM_CLUSTERS\n",
    "from input import TRAIN_TEST_SPLIT, ATR_PERIOD, TRADING_CAPITAL, TICKER\n",
    "\n",
    "# Define SYMBOL globally\n",
    "SYMBOL = TICKER.replace('=F', '')\n",
    "\n",
    "# Setup paths using relative directories\n",
    "WORKING_DIR = \".\"\n",
    "DATA_DIR = os.path.join(WORKING_DIR, \"data\")\n",
    "\n",
    "def save_results(data_frame, file_name):\n",
    "    \"\"\"Save results DataFrame to a CSV file\"\"\"\n",
    "    data_frame.to_csv(file_name, index=False)\n",
    "    print(f\"Results saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef351f",
   "metadata": {},
   "source": [
    "## Utility Functions from read_ts.py\n",
    "Only including the essential functions that are used in the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c2fb9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class mdata:\n",
    "    \"\"\"Data structure for holding market data and metadata\"\"\"\n",
    "    tick_size = None\n",
    "    big_point_value = None\n",
    "    country = None\n",
    "    exchange = None\n",
    "    symbol = None\n",
    "    description = None\n",
    "    interval_type = None\n",
    "    interval_span = None\n",
    "    time_zone = None\n",
    "    session = None\n",
    "    data = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str((self.symbol, self.interval_type, self.interval_span, self.data.shape))\n",
    "\n",
    "def read_ts_ohlcv_dat_one_stream(byte_stream) -> mdata:\n",
    "    \"\"\"Read a single OHLCV data stream\"\"\"\n",
    "    def read_string(f):\n",
    "        sz = struct.unpack('i', f.read(4))[0]\n",
    "        s = f.read(sz).decode('ascii')\n",
    "        return s\n",
    "    \n",
    "    d = mdata()\n",
    "    try:\n",
    "        f = byte_stream\n",
    "        (ones, type_format) = struct.unpack('ii', f.read(8))\n",
    "        if (ones != 1111111111):\n",
    "            print(\"format not supported, must be 1111111111\")\n",
    "            return None\n",
    "        if (type_format != 3):\n",
    "            print(\"type_format not supported, must be 3\")\n",
    "            return None\n",
    "        d.tick_size = struct.unpack('d', f.read(8))[0]\n",
    "        d.big_point_value = struct.unpack('d', f.read(8))[0]\n",
    "        d.country = read_string(f)\n",
    "        d.exchange = read_string(f)\n",
    "        d.symbol = read_string(f)\n",
    "        d.description = read_string(f)\n",
    "        d.interval_type = read_string(f)\n",
    "        d.interval_span = struct.unpack('i', f.read(4))[0]\n",
    "        d.time_zone = read_string(f)\n",
    "        d.session = read_string(f)\n",
    "\n",
    "        dt = np.dtype([('date', 'f8'), ('open', 'f8'), ('high', 'f8'), ('low', 'f8'), ('close', 'f8'), ('volume', 'f8')])\n",
    "        z = f.read()\n",
    "        data = pd.DataFrame.from_records(np.frombuffer(z, dtype=dt))\n",
    "    finally:\n",
    "        pass\n",
    "\n",
    "    arr = ((data['date']-25569)*24*60*60).round(0).astype(np.int64)*1000000000\n",
    "    z2 = pd.to_datetime(arr)\n",
    "    data.insert(0, 'datetime', z2)\n",
    "    del data['date']\n",
    "    d.data = data\n",
    "    return d\n",
    "\n",
    "def read_ts_ohlcv_dat_one(fname) -> mdata:\n",
    "    \"\"\"Read a single OHLCV data file\"\"\"\n",
    "    try:\n",
    "        f = open(fname, \"rb\")\n",
    "        d = read_ts_ohlcv_dat_one_stream(f)\n",
    "    finally:\n",
    "        f.close()\n",
    "    return d\n",
    "\n",
    "def read_ts_ohlcv_dat(fnames) -> List[mdata]:\n",
    "    \"\"\"Read multiple OHLCV data files\"\"\"\n",
    "    r = []\n",
    "    for name in glob.glob(fnames):\n",
    "        z = read_ts_ohlcv_dat_one(name)\n",
    "        r.append(z)\n",
    "    return r\n",
    "\n",
    "def find_futures_file(symbol, data_dir):\n",
    "    \"\"\"Find a data file for the specified futures symbol\"\"\"\n",
    "    pattern = f\"*@{symbol}_*.dat\"\n",
    "    files = glob.glob(os.path.join(data_dir, pattern))\n",
    "    \n",
    "    if not files:\n",
    "        pattern = f\"*_@{symbol}_*.dat\"\n",
    "        files = glob.glob(os.path.join(data_dir, pattern))\n",
    "    \n",
    "    if not files:\n",
    "        pattern = f\"*_{symbol}_*.dat\"\n",
    "        files = glob.glob(os.path.join(data_dir, pattern))\n",
    "    \n",
    "    if not files:\n",
    "        pattern = f\"*@{symbol}*.dat\"\n",
    "        files = glob.glob(os.path.join(data_dir, pattern))\n",
    "    \n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No data file found for {symbol} in {data_dir}\")\n",
    "        \n",
    "    return files[0]\n",
    "\n",
    "def mavg(d, period):\n",
    "    \"\"\"Calculate moving average\"\"\"\n",
    "    return np.concatenate([np.zeros((period-1)), np.convolve(d, np.ones((period,))/period, mode='valid')]) \n",
    "\n",
    "def analyze_sma_results(file_path='sma_all_results.txt'):\n",
    "    \"\"\"Analyze SMA optimization results and return best parameters\"\"\"\n",
    "    try:\n",
    "        # Load the data from the CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"Loaded {len(data)} simulation results\")\n",
    "        print(f\"Short SMA range: {data['short_SMA'].min()} to {data['short_SMA'].max()}\")\n",
    "        print(f\"Long SMA range: {data['long_SMA'].min()} to {data['long_SMA'].max()}\")\n",
    "        print(f\"Sharpe ratio range: {data['sharpe_ratio'].min():.4f} to {data['sharpe_ratio'].max():.4f}\")\n",
    "        \n",
    "        # Apply filtering before finding best parameters\n",
    "        filtered_data = data[\n",
    "            (data['short_SMA'] < data['long_SMA']) &  # Ensure short_SMA < long_SMA\n",
    "            (data['trades'] >= MIN_TRADES) &  # Minimum trades requirement\n",
    "            (data['trades'] <= MAX_TRADES)    # Maximum trades requirement\n",
    "        ]\n",
    "\n",
    "        if len(filtered_data) == 0:\n",
    "            print(f\"No data points meet the criteria after filtering! Adjust min_trades ({MIN_TRADES}) and max_trades ({MAX_TRADES}).\")\n",
    "            return None, None, None, None, None\n",
    "\n",
    "        print(f\"\\nAfter filtering:\")\n",
    "        print(f\"Remaining data points: {len(filtered_data)}\")\n",
    "        print(f\"Short SMA range: {filtered_data['short_SMA'].min()} to {filtered_data['short_SMA'].max()}\")\n",
    "        print(f\"Long SMA range: {filtered_data['long_SMA'].min()} to {filtered_data['long_SMA'].max()}\")\n",
    "        print(f\"Sharpe ratio range: {filtered_data['sharpe_ratio'].min():.4f} to {filtered_data['sharpe_ratio'].max():.4f}\")\n",
    "        print(f\"Trades range: {filtered_data['trades'].min()} to {filtered_data['trades'].max()}\")\n",
    "        \n",
    "        # Find the best Sharpe ratio from filtered data\n",
    "        max_sharpe_idx = filtered_data['sharpe_ratio'].idxmax()\n",
    "        best_row = filtered_data.iloc[max_sharpe_idx]\n",
    "        \n",
    "        print(f\"\\nBest parameters (after filtering):\")\n",
    "        print(f\"Short SMA: {int(best_row['short_SMA'])}\")\n",
    "        print(f\"Long SMA: {int(best_row['long_SMA'])}\")\n",
    "        print(f\"Sharpe Ratio: {best_row['sharpe_ratio']:.6f}\")\n",
    "        print(f\"Number of Trades: {int(best_row['trades'])}\")\n",
    "        \n",
    "        return (filtered_data, \n",
    "                int(best_row['short_SMA']), \n",
    "                int(best_row['long_SMA']), \n",
    "                best_row['sharpe_ratio'], \n",
    "                int(best_row['trades']))\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing SMA results: {e}\")\n",
    "        return None, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309debd",
   "metadata": {},
   "source": [
    "## SMA Strategy Implementation\n",
    "Implementation of the Simple Moving Average strategy with ATR-based position sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6055e73",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SMAStrategy:\n",
    "    \"\"\"\n",
    "    Vectorized SMA (Simple Moving Average) trading strategy with ATR-based position sizing\n",
    "    \n",
    "    Features:\n",
    "    - Basic SMA crossover entry signals (long when short SMA crosses above long SMA, short when it crosses below)\n",
    "    - Pure long/short positions based on crossover direction\n",
    "    - Position size based on ATR volatility\n",
    "    - Slippage modeling for realistic execution\n",
    "    - P&L calculated in absolute dollar terms using big point value\n",
    "    - Fully vectorized implementation for efficiency\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, short_sma, long_sma, big_point_value, slippage=0, capital=6000, atr_period=50):\n",
    "        \"\"\"\n",
    "        Initialize the SMA strategy with specific parameters\n",
    "\n",
    "        Parameters:\n",
    "        short_sma (int): Short SMA period in days\n",
    "        long_sma (int): Long SMA period in days\n",
    "        big_point_value (int): Contract big point value for calculating dollar P&L\n",
    "        slippage (float): Slippage in price units added/subtracted from execution price\n",
    "        capital (float): The capital allocation for position sizing\n",
    "        atr_period (int): Period for ATR calculation for position sizing\n",
    "        \"\"\"\n",
    "        self.short_sma = short_sma\n",
    "        self.long_sma = long_sma\n",
    "        self.big_point_value = big_point_value\n",
    "        self.slippage = slippage\n",
    "        self.capital = capital\n",
    "        self.atr_period = atr_period\n",
    "\n",
    "    def calculate_atr(self, data, period=None):\n",
    "        \"\"\"\n",
    "        Calculate Average True Range (ATR)\n",
    "        \n",
    "        Parameters:\n",
    "        data: DataFrame with OHLC data\n",
    "        period: ATR calculation period, defaults to self.atr_period if None\n",
    "        \n",
    "        Returns:\n",
    "        Series: ATR values\n",
    "        \"\"\"\n",
    "        if period is None:\n",
    "            period = self.atr_period\n",
    "            \n",
    "        # Calculate True Range\n",
    "        high = data['High'].copy()\n",
    "        prev_close = data['Close'].shift(1)\n",
    "        true_high = pd.DataFrame({'high': high, 'prev_close': prev_close}).max(axis=1)\n",
    "        \n",
    "        low = data['Low'].copy()\n",
    "        true_low = pd.DataFrame({'low': low, 'prev_close': prev_close}).min(axis=1)\n",
    "        \n",
    "        true_range = true_high - true_low\n",
    "        atr = true_range.rolling(window=period).mean()\n",
    "        \n",
    "        return atr\n",
    "\n",
    "    def apply_strategy(self, data, strategy_name=\"Strategy\"):\n",
    "        \"\"\"\n",
    "        Apply the simple SMA crossover strategy to the price data using vectorized operations\n",
    "        with ATR-based position sizing\n",
    "        \"\"\"\n",
    "        sim_data = data.copy()\n",
    "        \n",
    "        sim_data[f'SMA_Short_{strategy_name}'] = mavg(sim_data['Close'].values, self.short_sma)\n",
    "        sim_data[f'SMA_Long_{strategy_name}'] = mavg(sim_data['Close'].values, self.long_sma)\n",
    "        \n",
    "        sim_data[f'ATR_{strategy_name}'] = self.calculate_atr(sim_data, self.atr_period)\n",
    "        \n",
    "        sim_data[f'Position_Size_{strategy_name}'] = np.round(\n",
    "            self.capital / (sim_data[f'ATR_{strategy_name}'] * self.big_point_value) + 0.5\n",
    "        )\n",
    "        \n",
    "        sim_data[f'Position_Dir_{strategy_name}'] = np.where(\n",
    "            sim_data[f'SMA_Short_{strategy_name}'] > sim_data[f'SMA_Long_{strategy_name}'], 1, -1\n",
    "        )\n",
    "        \n",
    "        sim_data[f'Position_Dir_{strategy_name}'] = sim_data[f'Position_Dir_{strategy_name}'].fillna(0)\n",
    "        \n",
    "        sim_data[f'Position_Change_{strategy_name}'] = sim_data[f'Position_Dir_{strategy_name}'].diff() != 0\n",
    "        \n",
    "        market_pnl = sim_data['Close'].diff() * self.big_point_value\n",
    "        sim_data[f'Market_PnL_{strategy_name}'] = market_pnl\n",
    "        \n",
    "        sim_data[f'Daily_PnL_{strategy_name}'] = (\n",
    "            market_pnl * \n",
    "            sim_data[f'Position_Dir_{strategy_name}'].shift(1) * \n",
    "            sim_data[f'Position_Size_{strategy_name}'].shift(1)\n",
    "        )\n",
    "        \n",
    "        position_changed = sim_data[f'Position_Change_{strategy_name}']\n",
    "        sim_data.loc[position_changed, f'Daily_PnL_{strategy_name}'] -= (\n",
    "            self.slippage * sim_data[f'Position_Size_{strategy_name}'][position_changed]\n",
    "        )\n",
    "        \n",
    "        sim_data[f'Daily_PnL_{strategy_name}'] = sim_data[f'Daily_PnL_{strategy_name}'].fillna(0)\n",
    "        \n",
    "        sim_data[f'Cumulative_PnL_{strategy_name}'] = sim_data[f'Daily_PnL_{strategy_name}'].cumsum()\n",
    "        \n",
    "        return sim_data \n",
    "\n",
    "    def optimize(self, data, sma_range, train_test_split=0.7, results_file=None, warm_up_idx=None):\n",
    "        \"\"\"\n",
    "        Find the optimal SMA parameters and record all simulations using vectorized operations\n",
    "\n",
    "        Parameters:\n",
    "        data: DataFrame with market data\n",
    "        sma_range: Range of SMA periods to test\n",
    "        train_test_split: Portion of data to use for in-sample testing\n",
    "        results_file: Path to save simulation results\n",
    "        warm_up_idx: Index to trim warm-up period (if provided)\n",
    "\n",
    "        Returns:\n",
    "        best_sma_params: Tuple with (short_sma, long_sma)\n",
    "        best_sharpe: Best Sharpe ratio found\n",
    "        best_trades: Number of trades with best parameters\n",
    "        all_results: List of tuples with all simulation results\n",
    "        \"\"\"\n",
    "        best_sharpe = -np.inf\n",
    "        best_sma = (0, 0)\n",
    "        best_trades = 0\n",
    "        best_sim_data = None\n",
    "        all_results = []\n",
    "\n",
    "        if results_file:\n",
    "            with open(results_file, 'w') as f:\n",
    "                f.write(\"short_SMA,long_SMA,trades,sharpe_ratio\\n\")\n",
    "\n",
    "        total_combinations = sum(1 for a, b in [(s, l) for s in sma_range for l in sma_range] if a < b)\n",
    "        completed = 0\n",
    "        total_sim_time = 0\n",
    "        \n",
    "        print(f\"Running {total_combinations} simulations...\")\n",
    "        \n",
    "        for short_sma in sma_range:\n",
    "            for long_sma in sma_range:\n",
    "                if short_sma >= long_sma:\n",
    "                    continue\n",
    "\n",
    "                sim_start_time = time.time()\n",
    "                orig_short_sma = self.short_sma\n",
    "                orig_long_sma = self.long_sma\n",
    "                self.short_sma = short_sma\n",
    "                self.long_sma = long_sma\n",
    "\n",
    "                sim_data = self.apply_strategy(data.copy(), strategy_name=\"Sim\")\n",
    "\n",
    "                if warm_up_idx is not None:\n",
    "                    sim_data_eval = sim_data.iloc[warm_up_idx:].copy()\n",
    "                    split_index = int(len(sim_data_eval) * train_test_split)\n",
    "                else:\n",
    "                    sim_data_eval = sim_data\n",
    "                    split_index = int(len(sim_data_eval) * train_test_split)\n",
    "\n",
    "                trade_entries = sim_data_eval['Position_Change_Sim']\n",
    "                trade_count = trade_entries.sum()\n",
    "\n",
    "                sim_data_eval.loc[:, 'Daily_Returns'] = sim_data_eval['Daily_PnL_Sim']\n",
    "                in_sample_returns = sim_data_eval['Daily_Returns'].iloc[:split_index]\n",
    "\n",
    "                if len(in_sample_returns.dropna()) == 0 or in_sample_returns.std() == 0:\n",
    "                    sharpe_ratio = 0\n",
    "                else:\n",
    "                    sharpe_ratio = in_sample_returns.mean() / in_sample_returns.std() * np.sqrt(252)\n",
    "\n",
    "                if results_file:\n",
    "                    with open(results_file, 'a') as f:\n",
    "                        f.write(f\"{short_sma},{long_sma},{trade_count},{sharpe_ratio:.6f}\\n\")\n",
    "\n",
    "                result = (short_sma, long_sma, trade_count, sharpe_ratio)\n",
    "                all_results.append(result)\n",
    "\n",
    "                if sharpe_ratio > best_sharpe:\n",
    "                    best_sharpe = sharpe_ratio\n",
    "                    best_sma = (short_sma, long_sma)\n",
    "                    best_trades = trade_count\n",
    "                    best_sim_data = sim_data_eval.copy()\n",
    "\n",
    "                self.short_sma = orig_short_sma\n",
    "                self.long_sma = orig_long_sma\n",
    "\n",
    "                sim_end_time = time.time()\n",
    "                sim_time = sim_end_time - sim_start_time\n",
    "                total_sim_time += sim_time\n",
    "                \n",
    "                completed += 1\n",
    "                if completed % 100 == 0 or completed == total_combinations:\n",
    "                    avg_sim_time = total_sim_time / completed\n",
    "                    est_remaining = avg_sim_time * (total_combinations - completed)\n",
    "                    print(\n",
    "                        f\"Progress: {completed}/{total_combinations} simulations completed ({(completed / total_combinations * 100):.1f}%)\"\n",
    "                        f\" - Avg sim time: {avg_sim_time:.4f}s - Est. remaining: {est_remaining:.1f}s\")\n",
    "\n",
    "        if best_sim_data is not None:\n",
    "            print(\"\\n--- OPTIMIZATION SHARPE VERIFICATION ---\")\n",
    "            verify_split_idx = int(len(best_sim_data) * train_test_split)\n",
    "            verify_returns = best_sim_data['Daily_Returns'].iloc[:verify_split_idx]\n",
    "            \n",
    "            if verify_returns.std() > 0:\n",
    "                verify_sharpe = verify_returns.mean() / verify_returns.std() * np.sqrt(252)\n",
    "                print(f\"Optimization best Sharpe = {best_sharpe:.6f}\")\n",
    "                print(f\"Verification Sharpe = {verify_sharpe:.6f}\")\n",
    "                print(f\"Data points used: {len(best_sim_data)}\")\n",
    "                print(f\"In-sample data points: {len(verify_returns)}\")\n",
    "            else:\n",
    "                print(\"Cannot verify Sharpe (std = 0)\")\n",
    "\n",
    "        return best_sma, best_sharpe, best_trades, all_results\n",
    "\n",
    "    def calculate_performance_metrics(self, data, strategy_name=\"Strategy\", train_test_split=0.7):\n",
    "        \"\"\"\n",
    "        Calculate detailed performance metrics for the strategy\n",
    "\n",
    "        Parameters:\n",
    "        data: DataFrame with strategy results\n",
    "        strategy_name: Name suffix for the strategy columns\n",
    "        train_test_split: Portion of data used for in-sample testing\n",
    "\n",
    "        Returns:\n",
    "        dict: Dictionary with performance metrics\n",
    "        \"\"\"\n",
    "        # Create an explicit copy of the data to avoid SettingWithCopyWarning\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        split_index = int(len(data_copy) * train_test_split)\n",
    "        split_date = data_copy.index[split_index]\n",
    "\n",
    "        daily_pnl = data_copy[f'Daily_PnL_{strategy_name}']\n",
    "        returns_in_sample = daily_pnl.iloc[:split_index]\n",
    "        returns_out_sample = daily_pnl.iloc[split_index:]\n",
    "\n",
    "        sharpe_in_sample = returns_in_sample.mean() / returns_in_sample.std() * np.sqrt(\n",
    "            252) if returns_in_sample.std() > 0 else 0\n",
    "        sharpe_out_sample = returns_out_sample.mean() / returns_out_sample.std() * np.sqrt(\n",
    "            252) if returns_out_sample.std() > 0 else 0\n",
    "        sharpe_full = daily_pnl.mean() / daily_pnl.std() * np.sqrt(252) if daily_pnl.std() > 0 else 0\n",
    "\n",
    "        print(\"\\n--- PERFORMANCE METRICS SHARPE VERIFICATION ---\")\n",
    "        print(f\"In-sample Sharpe = {sharpe_in_sample:.6f}\")\n",
    "        print(f\"Data points used: {len(data_copy)}\")\n",
    "        print(f\"In-sample data points: {len(returns_in_sample)}\")\n",
    "        print(f\"Mean: {returns_in_sample.mean():.6f}, Std: {returns_in_sample.std():.6f}\")\n",
    "\n",
    "        position_changes = data_copy[f'Position_Change_{strategy_name}']\n",
    "        total_trades = position_changes.sum()\n",
    "        in_sample_trades = position_changes.iloc[:split_index].sum()\n",
    "        out_sample_trades = position_changes.iloc[split_index:].sum()\n",
    "\n",
    "        pnl_series = data_copy[f'Cumulative_PnL_{strategy_name}']\n",
    "        \n",
    "        # Calculate drawdown metrics on the copy\n",
    "        data_copy['Peak'] = pnl_series.cummax()\n",
    "        data_copy['Drawdown_Dollars'] = pnl_series - data_copy['Peak']\n",
    "        max_drawdown_dollars = data_copy['Drawdown_Dollars'].min()\n",
    "\n",
    "        total_pnl = data_copy[f'Cumulative_PnL_{strategy_name}'].iloc[-1]\n",
    "        in_sample_pnl = data_copy[f'Daily_PnL_{strategy_name}'].iloc[:split_index].sum()\n",
    "        out_sample_pnl = data_copy[f'Daily_PnL_{strategy_name}'].iloc[split_index:].sum()\n",
    "\n",
    "        avg_position_size = data_copy[f'Position_Size_{strategy_name}'].mean()\n",
    "        max_position_size = data_copy[f'Position_Size_{strategy_name}'].max()\n",
    "\n",
    "        metrics = {\n",
    "            'split_date': split_date,\n",
    "            'total_pnl': total_pnl,\n",
    "            'sharpe_full': sharpe_full,\n",
    "            'sharpe_in_sample': sharpe_in_sample,\n",
    "            'sharpe_out_sample': sharpe_out_sample,\n",
    "            'max_drawdown_dollars': max_drawdown_dollars,\n",
    "            'total_trades': total_trades,\n",
    "            'in_sample_trades': in_sample_trades,\n",
    "            'out_sample_trades': out_sample_trades,\n",
    "            'in_sample_pnl': in_sample_pnl,\n",
    "            'out_sample_pnl': out_sample_pnl,\n",
    "            'avg_position_size': avg_position_size,\n",
    "            'max_position_size': max_position_size\n",
    "        }\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb41c0",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Strategy parameters and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02baf3d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "TICKER = 'MME'\n",
    "START_DATE = '2014-01-01'\n",
    "END_DATE = '2025-01-01'\n",
    "\n",
    "# SMA range parameters\n",
    "SMA_MIN = 10\n",
    "SMA_MAX = 300\n",
    "SMA_STEP = 5\n",
    "\n",
    "# Data splitting ratio\n",
    "TRAIN_TEST_SPLIT = 0.7\n",
    "\n",
    "# ATR-based position sizing parameters\n",
    "ATR_PERIOD = 30\n",
    "TRADING_CAPITAL = 6000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe69b7",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Execute the strategy with the defined configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b32f4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Start overall execution timer\n",
    "    overall_start_time = time.time()\n",
    "    \n",
    "    # Setup paths using relative directories\n",
    "    WORKING_DIR = \".\"\n",
    "    DATA_DIR = os.path.join(WORKING_DIR, \"data\")\n",
    "\n",
    "    # Define SYMBOL based on TICKER\n",
    "    SYMBOL = TICKER.replace('=F', '')\n",
    "\n",
    "    def save_parameters():\n",
    "        parameters = {\n",
    "            \"big_point_value\": big_point_value,\n",
    "            \"slippage\": slippage,\n",
    "            \"capital\": TRADING_CAPITAL,\n",
    "            \"atr_period\": ATR_PERIOD\n",
    "        }\n",
    "        with open(\"parameters.json\", \"w\") as file:\n",
    "            json.dump(parameters, file)\n",
    "\n",
    "    def get_slippage_from_excel(symbol, data_dir):\n",
    "        excel_path = os.path.join(data_dir, \"sessions_slippages.xlsx\")\n",
    "        \n",
    "        if not os.path.exists(excel_path):\n",
    "            raise FileNotFoundError(f\"Slippage Excel file not found at {excel_path}\")\n",
    "        \n",
    "        lookup_symbol = symbol.replace('=F', '')\n",
    "        df = pd.read_excel(excel_path)\n",
    "        \n",
    "        if df.shape[1] < 4:\n",
    "            raise ValueError(f\"Excel file has fewer than 4 columns: {df.columns.tolist()}\")\n",
    "            \n",
    "        df['SymbolUpper'] = df.iloc[:, 1].astype(str).str.upper()\n",
    "        lookup_symbol_upper = lookup_symbol.upper()\n",
    "        \n",
    "        matching_rows = df[df['SymbolUpper'] == lookup_symbol_upper]\n",
    "        \n",
    "        if matching_rows.empty:\n",
    "            raise ValueError(f\"Symbol '{lookup_symbol}' not found in column B of Excel file\")\n",
    "            \n",
    "        slippage_value = matching_rows.iloc[0, 3]\n",
    "        \n",
    "        if pd.isna(slippage_value) or not isinstance(slippage_value, (int, float)):\n",
    "            raise ValueError(f\"Invalid slippage value for symbol '{lookup_symbol}': {slippage_value}\")\n",
    "            \n",
    "        print(f\"Found slippage for {lookup_symbol} in column D: {slippage_value}\")\n",
    "        return slippage_value\n",
    "\n",
    "    def find_futures_file(symbol, data_dir):\n",
    "        \"\"\"Find a data file for the specified futures symbol\"\"\"\n",
    "        pattern = f\"*@{symbol}_*.dat\"\n",
    "        files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            pattern = f\"*_@{symbol}_*.dat\"\n",
    "            files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            pattern = f\"*_{symbol}_*.dat\"\n",
    "            files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            pattern = f\"*@{symbol}*.dat\"\n",
    "            files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No data file found for {symbol} in {data_dir}\")\n",
    "            \n",
    "        return files[0]\n",
    "\n",
    "    # Get symbol from the TICKER variable\n",
    "    SYMBOL = TICKER.replace('=F', '')\n",
    "\n",
    "    # Load the futures data file\n",
    "    print(\"Loading data file...\")\n",
    "    load_start_time = time.time()\n",
    "    \n",
    "    data_files = glob.glob(os.path.join(DATA_DIR, f\"*@{SYMBOL}*.dat\"))\n",
    "    if not data_files:\n",
    "        data_files = glob.glob(os.path.join(DATA_DIR, f\"*_{SYMBOL}_*.dat\"))\n",
    "    \n",
    "    if not data_files:\n",
    "        raise FileNotFoundError(f\"No data file found for {SYMBOL} in {DATA_DIR}\")\n",
    "    \n",
    "    all_data = read_ts_ohlcv_dat(data_files[0])\n",
    "    load_end_time = time.time()\n",
    "    load_time = load_end_time - load_start_time\n",
    "    print(f\"Data loaded successfully in {load_time:.2f} seconds! Number of items: {len(all_data)}\")\n",
    "    \n",
    "    # Extract metadata and OHLCV data\n",
    "    data_obj = all_data[0]\n",
    "    tick_size = data_obj.big_point_value * data_obj.tick_size\n",
    "    big_point_value = data_obj.big_point_value\n",
    "    ohlc_data = data_obj.data.copy()\n",
    "\n",
    "    # Fetch slippage value from Excel\n",
    "    slippage_value = get_slippage_from_excel(TICKER, DATA_DIR)\n",
    "    slippage = slippage_value\n",
    "    print(f\"Using slippage from Excel column D: {slippage}\")\n",
    "\n",
    "    # Save the parameters\n",
    "    save_parameters()\n",
    "    \n",
    "    # Start timing data preparation\n",
    "    prep_start_time = time.time()\n",
    "    \n",
    "    # Print information about the data\n",
    "    print(f\"\\nSymbol: {data_obj.symbol}\")\n",
    "    print(f\"Description: {data_obj.description}\")\n",
    "    print(f\"Exchange: {data_obj.exchange}\")\n",
    "    print(f\"Interval: {data_obj.interval_type} {data_obj.interval_span}\")\n",
    "    print(f\"Tick size: {tick_size}\")\n",
    "    print(f\"Big point value: {big_point_value}\")\n",
    "    print(f\"Data shape: {ohlc_data.shape}\")\n",
    "    print(f\"Date range: {ohlc_data['datetime'].min()} to {ohlc_data['datetime'].max()}\")\n",
    "    \n",
    "    # Display the first few rows of data\n",
    "    print(\"\\nFirst few rows of OHLCV data:\")\n",
    "    print(ohlc_data.head())\n",
    "    \n",
    "    # Convert the OHLCV data to the expected format\n",
    "    data = ohlc_data.rename(columns={\n",
    "        'datetime': 'Date',\n",
    "        'open': 'Open',\n",
    "        'high': 'High',\n",
    "        'low': 'Low',\n",
    "        'close': 'Close',\n",
    "        'volume': 'Volume'\n",
    "    })\n",
    "    \n",
    "    data.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Add warm-up period for SMA calculation\n",
    "    original_start_idx = None\n",
    "    \n",
    "    # Filter data to match the date range\n",
    "    if START_DATE and END_DATE:\n",
    "        warm_up_days = SMA_MAX + ATR_PERIOD + 50\n",
    "        \n",
    "        start_date = pd.to_datetime(START_DATE)\n",
    "        end_date = pd.to_datetime(END_DATE)\n",
    "        \n",
    "        adjusted_start = start_date - pd.Timedelta(days=warm_up_days)\n",
    "        \n",
    "        data = data[(data.index >= adjusted_start) & \n",
    "                    (data.index <= end_date)]\n",
    "        \n",
    "        if data.empty:\n",
    "            raise ValueError(f\"No data available for the specified date range: {START_DATE} to {END_DATE}\")\n",
    "            \n",
    "        original_start_idx = data.index.get_indexer([start_date], method='nearest')[0]\n",
    "        \n",
    "        print(f\"Loaded extended data with {warm_up_days} days warm-up period\")\n",
    "        print(f\"Original date range: {START_DATE} to {END_DATE}\")\n",
    "        print(f\"Adjusted date range: {adjusted_start.strftime('%Y-%m-%d')} to {END_DATE}\")\n",
    "        print(f\"Original start index: {original_start_idx}\")\n",
    "    \n",
    "    prep_end_time = time.time()\n",
    "    prep_time = prep_end_time - prep_start_time\n",
    "    print(f\"Data preparation completed in {prep_time:.2f} seconds\")\n",
    "    \n",
    "    # Define the range of SMA periods to test\n",
    "    sma_range = range(SMA_MIN, SMA_MAX + 1, SMA_STEP)\n",
    "    \n",
    "    print(f\"Optimizing SMA parameters using range from {SMA_MIN} to {SMA_MAX} with step {SMA_STEP}...\")\n",
    "    print(f\"Trading with big point value from data: {big_point_value}\")\n",
    "    print(f\"Using capital allocation: ${TRADING_CAPITAL:,} with ATR period: {ATR_PERIOD}\")\n",
    "    \n",
    "    # Initialize the ATR-based strategy\n",
    "    strategy = SMAStrategy(\n",
    "        short_sma=0,\n",
    "        long_sma=0,\n",
    "        big_point_value=big_point_value,\n",
    "        slippage=slippage,\n",
    "        capital=TRADING_CAPITAL,\n",
    "        atr_period=ATR_PERIOD\n",
    "    )\n",
    "    \n",
    "    # Start optimization process\n",
    "    print(\"\\nStarting optimization process...\")\n",
    "    optimization_start_time = time.time()\n",
    "    \n",
    "    best_sma, best_sharpe, best_trades, all_results = strategy.optimize(\n",
    "        data.copy(),\n",
    "        sma_range,\n",
    "        train_test_split=TRAIN_TEST_SPLIT,\n",
    "        results_file='sma_all_results.txt',\n",
    "        warm_up_idx=original_start_idx\n",
    "    )\n",
    "    \n",
    "    optimization_end_time = time.time()\n",
    "    optimization_time = optimization_end_time - optimization_start_time\n",
    "    print(f\"\\nOptimization completed in {optimization_time:.2f} seconds ({optimization_time/60:.2f} minutes)\")\n",
    "    \n",
    "    print(f\"Optimal SMA parameters: Short = {best_sma[0]} days, Long = {best_sma[1]} days\")\n",
    "    print(f\"In-sample Sharpe ratio = {best_sharpe:.4f}\")\n",
    "    print(f\"Number of trades with optimal parameters = {best_trades}\")\n",
    "    print(f\"Optimization results saved to 'sma_all_results.txt' for further analysis\")\n",
    "    \n",
    "    # Update strategy with the best parameters\n",
    "    strategy.short_sma = best_sma[0]\n",
    "    strategy.long_sma = best_sma[1]\n",
    "    \n",
    "    # Apply the best strategy parameters\n",
    "    print(\"\\nApplying best strategy parameters...\")\n",
    "    apply_start_time = time.time()\n",
    "    \n",
    "    data = strategy.apply_strategy(data.copy())\n",
    "    data_for_evaluation = data.copy()\n",
    "    \n",
    "    apply_end_time = time.time()\n",
    "    apply_time = apply_end_time - apply_start_time\n",
    "    print(f\"Strategy application completed in {apply_time:.2f} seconds\")\n",
    "    \n",
    "    # Start visualization process\n",
    "    viz_start_time = time.time()\n",
    "    \n",
    "    if original_start_idx is not None:\n",
    "        print(\"Trimming warm-up period for final evaluation and visualization...\")\n",
    "        data_for_evaluation = data.iloc[original_start_idx:]\n",
    "        print(f\"Original data length: {len(data)}, Evaluation data length: {len(data_for_evaluation)}\")\n",
    "    else:\n",
    "        data_for_evaluation = data\n",
    "    \n",
    "    # Create visualization plots\n",
    "    plt.figure(figsize=(14, 16))\n",
    "    \n",
    "    # Plot price and SMAs\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(data_for_evaluation.index, data_for_evaluation['Close'], label=f'{data_obj.symbol} Price', color='blue')\n",
    "    plt.plot(data_for_evaluation.index, data_for_evaluation['SMA_Short_Strategy'], label=f'{best_sma[0]}-day SMA', color='orange')\n",
    "    plt.plot(data_for_evaluation.index, data_for_evaluation['SMA_Long_Strategy'], label=f'{best_sma[1]}-day SMA', color='red')\n",
    "    \n",
    "    long_entries = (data_for_evaluation['Position_Dir_Strategy'] == 1) & data_for_evaluation['Position_Change_Strategy']\n",
    "    short_entries = (data_for_evaluation['Position_Dir_Strategy'] == -1) & data_for_evaluation['Position_Change_Strategy']\n",
    "    \n",
    "    plt.scatter(data_for_evaluation.index[long_entries], data_for_evaluation.loc[long_entries, 'Close'], \n",
    "                color='green', marker='^', s=50, label='Long Entry')\n",
    "    plt.scatter(data_for_evaluation.index[short_entries], data_for_evaluation.loc[short_entries, 'Close'], \n",
    "                color='red', marker='v', s=50, label='Short Entry')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f'{data_obj.symbol} with Optimized SMA Strategy ({best_sma[0]}, {best_sma[1]})')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot position size and ATR\n",
    "    ax1 = plt.subplot(3, 1, 2)\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    ax1.plot(data_for_evaluation.index, data_for_evaluation['Position_Size_Strategy'], \n",
    "             label='Position Size (# Contracts)', color='purple')\n",
    "    ax1.set_ylabel('Position Size (# Contracts)', color='purple')\n",
    "    ax1.tick_params(axis='y', colors='purple')\n",
    "    \n",
    "    ax2.plot(data_for_evaluation.index, data_for_evaluation['ATR_Strategy'], \n",
    "             label=f'ATR ({ATR_PERIOD}-day)', color='orange')\n",
    "    ax2.set_ylabel(f'ATR ({ATR_PERIOD}-day)', color='orange')\n",
    "    ax2.tick_params(axis='y', colors='orange')\n",
    "    \n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    plt.title(f'Position Sizing Based on {ATR_PERIOD}-day ATR')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot performance\n",
    "    plt.subplot(3, 1, 3)\n",
    "    \n",
    "    strategy_pnl_cumulative = data_for_evaluation['Cumulative_PnL_Strategy'] - data_for_evaluation['Cumulative_PnL_Strategy'].iloc[0]\n",
    "    \n",
    "    plt.plot(data_for_evaluation.index, strategy_pnl_cumulative, \n",
    "             label='Strategy P&L (full period)', color='green')\n",
    "    \n",
    "    split_index = int(len(data_for_evaluation) * TRAIN_TEST_SPLIT)\n",
    "    \n",
    "    plt.plot(data_for_evaluation.index[split_index:], strategy_pnl_cumulative.iloc[split_index:],\n",
    "            label=f'Strategy P&L (last {int((1 - TRAIN_TEST_SPLIT) * 100)}% out-of-sample)', color='purple')\n",
    "    \n",
    "    plt.axvline(x=data_for_evaluation.index[split_index], color='black', linestyle='--',\n",
    "                label=f'Train/Test Split ({int(TRAIN_TEST_SPLIT * 100)}%/{int((1 - TRAIN_TEST_SPLIT) * 100)}%)')\n",
    "    plt.axhline(y=0.0, color='gray', linestyle='-', label='Break-even')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('Strategy Performance (Dollar P&L)')\n",
    "    plt.ylabel('P&L ($)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()  # Display the plot instead of saving it\n",
    "    \n",
    "    viz_end_time = time.time()\n",
    "    viz_time = viz_end_time - viz_start_time\n",
    "    print(f\"Visualization completed in {viz_time:.2f} seconds\")\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    metrics_start_time = time.time()\n",
    "    metrics = strategy.calculate_performance_metrics(\n",
    "        data_for_evaluation,\n",
    "        strategy_name=\"Strategy\",\n",
    "        train_test_split=TRAIN_TEST_SPLIT\n",
    "    )\n",
    "    metrics_end_time = time.time()\n",
    "    metrics_time = metrics_end_time - metrics_start_time\n",
    "    print(f\"Performance metrics calculation completed in {metrics_time:.2f} seconds\")\n",
    "    \n",
    "    # Calculate market performance for comparison\n",
    "    market_cumulative_pnl = data_for_evaluation['Market_PnL_Strategy'].cumsum().iloc[-1]\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n--- PERFORMANCE SUMMARY OF ATR-BASED SMA STRATEGY ---\")\n",
    "    print(f\"Symbol: {data_obj.symbol}\")\n",
    "    print(f\"Big Point Value (from data): {big_point_value}\")\n",
    "    print(f\"ATR Period for Position Sizing: {ATR_PERIOD} days\")\n",
    "    print(f\"Capital Allocation: ${TRADING_CAPITAL:,}\")\n",
    "    print(f\"Average Position Size: {metrics['avg_position_size']:.2f} contracts\")\n",
    "    print(f\"Maximum Position Size: {metrics['max_position_size']:.0f} contracts\")\n",
    "    print(f\"Strategy Total P&L: ${metrics['total_pnl']:,.2f}\")\n",
    "    print(f\"Market Buy & Hold P&L: ${market_cumulative_pnl:,.2f}\")\n",
    "    print(f\"Outperformance: ${(metrics['total_pnl'] - market_cumulative_pnl):,.2f}\")\n",
    "    \n",
    "    print(\"\\n--- SHARPE RATIO COMPARISON VERIFICATION ---\")\n",
    "    print(f\"Optimization in-sample Sharpe ratio: {best_sharpe:.6f}\")\n",
    "    print(f\"Final in-sample Sharpe ratio: {metrics['sharpe_in_sample']:.6f}\")\n",
    "    print(f\"Difference: {abs(best_sharpe - metrics['sharpe_in_sample']):.6f}\")\n",
    "    if abs(best_sharpe - metrics['sharpe_in_sample']) < 0.001:\n",
    "        print(\"✓ SHARPE RATIOS MATCH (within 0.001 tolerance)\")\n",
    "    else:\n",
    "        print(\"✗ SHARPE RATIOS DO NOT MATCH\")\n",
    "    \n",
    "    print(f\"Sharpe ratio (entire period, annualized): {metrics['sharpe_full']:.4f}\")\n",
    "    print(f\"Sharpe ratio (in-sample, annualized): {metrics['sharpe_in_sample']:.4f}\")\n",
    "    print(f\"Sharpe ratio (out-of-sample, annualized): {metrics['sharpe_out_sample']:.4f}\")\n",
    "    print(f\"Maximum Drawdown: ${abs(metrics['max_drawdown_dollars']):,.2f}\")\n",
    "    \n",
    "    print(\"\\n--- TRADE COUNT SUMMARY ---\")\n",
    "    print(f\"In-sample period trades: {metrics['in_sample_trades']}\")\n",
    "    print(f\"Out-of-sample period trades: {metrics['out_sample_trades']}\")\n",
    "    print(f\"Total trades: {metrics['total_trades']}\")\n",
    "    print(f\"In-sample P&L: ${metrics['in_sample_pnl']:,.2f}\")\n",
    "    print(f\"Out-of-sample P&L: ${metrics['out_sample_pnl']:,.2f}\")\n",
    "    \n",
    "    print(f\"\\nBest parameters: Short SMA = {best_sma[0]}, Long SMA = {best_sma[1]}, Sharpe = {best_sharpe:.6f}, Trades = {best_trades}\")\n",
    "    \n",
    "    # Calculate overall execution time\n",
    "    overall_end_time = time.time()\n",
    "    overall_time = overall_end_time - overall_start_time\n",
    "    \n",
    "    # Print timing summary\n",
    "    print(\"\\n--- EXECUTION TIME SUMMARY (Vectorized Implementation) ---\")\n",
    "    print(f\"Data loading time: {load_time:.2f} seconds\")\n",
    "    print(f\"Data preparation time: {prep_time:.2f} seconds\")\n",
    "    print(f\"Optimization time: {optimization_time:.2f} seconds ({optimization_time/60:.2f} minutes)\")\n",
    "    print(f\"Strategy application time: {apply_time:.2f} seconds\")\n",
    "    print(f\"Visualization time: {viz_time:.2f} seconds\")\n",
    "    print(f\"Metrics calculation time: {metrics_time:.2f} seconds\")\n",
    "    print(f\"Total execution time: {overall_time:.2f} seconds ({overall_time/60:.2f} minutes)\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "\n",
    "    # After the existing strategy application and visualization, add the new analysis:\n",
    "    print(\"\\nStarting additional analysis...\")\n",
    "    \n",
    "    # Run the basic analysis first\n",
    "    data, best_short, best_long, best_sharpe, best_trades = analyze_sma_results()\n",
    "\n",
    "    if data is None:\n",
    "        print(\"Error: Failed to load or analyze SMA results data.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nProceeding with cluster analysis...\")\n",
    "\n",
    "    # Run the cluster analysis to get medoids\n",
    "    X_filtered, medoids, top_medoids, centroids, max_sharpe_point = cluster_analysis()\n",
    "\n",
    "    if X_filtered is None or medoids is None:\n",
    "        print(\"Error: Cluster analysis failed.\")\n",
    "        return\n",
    "\n",
    "    # Re-sort top_medoids to ensure they're in the right order\n",
    "    if top_medoids is not None:\n",
    "        print(\"Re-sorting top medoids by Sharpe ratio...\")\n",
    "        top_medoids = sorted(top_medoids, key=lambda x: float(x[2]), reverse=True)\n",
    "        for idx, medoid in enumerate(top_medoids, 1):\n",
    "            print(f\"Verified Medoid {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "                f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "    print(\"\\nPlotting strategy performance...\")\n",
    "\n",
    "    # Plot strategy performance with the best parameters AND top medoids using ATR-based position sizing\n",
    "    market_data = plot_strategy_performance(\n",
    "        best_short, best_long, top_medoids, \n",
    "        big_point_value=big_point_value,\n",
    "        slippage=slippage,\n",
    "        capital=TRADING_CAPITAL,\n",
    "        atr_period=ATR_PERIOD\n",
    "    )\n",
    "    \n",
    "    # Run the bimonthly out-of-sample comparison between best Sharpe and top medoids\n",
    "    if top_medoids and len(top_medoids) > 0:\n",
    "        print(\"\\nPerforming bimonthly out-of-sample comparison...\")\n",
    "        bimonthly_sharpe_df = bimonthly_out_of_sample_comparison(\n",
    "            market_data, \n",
    "            best_short, \n",
    "            best_long, \n",
    "            top_medoids,  # Pass the entire top_medoids list\n",
    "            big_point_value=big_point_value,\n",
    "            slippage=slippage,\n",
    "            capital=TRADING_CAPITAL,\n",
    "            atr_period=ATR_PERIOD\n",
    "        )\n",
    "        \n",
    "        # Calculate win percentage from bimonthly comparison\n",
    "        if bimonthly_sharpe_df is not None:\n",
    "            total_periods = len(bimonthly_sharpe_df)\n",
    "            if total_periods > 0:\n",
    "                rounded_wins = sum(bimonthly_sharpe_df['Avg_Medoid_sharpe_rounded'] > bimonthly_sharpe_df['Best_sharpe_rounded'])\n",
    "                win_percentage = (rounded_wins / total_periods) * 100\n",
    "                \n",
    "                # Save kmeans results to Excel\n",
    "                save_to_excel(SYMBOL, win_percentage, top_medoids, best_short, best_long, is_kmeans=True)\n",
    "    else:\n",
    "        print(\"No top medoids found. Cannot run bimonthly comparison.\")\n",
    "\n",
    "    print(\"\\nProceeding with hierarchical cluster analysis...\")\n",
    "    \n",
    "    # Run the hierarchical cluster analysis\n",
    "    X_filtered_h, medoids_h, top_medoids_h, max_sharpe_point_h, labels_h = hierarchical_cluster_analysis()\n",
    "\n",
    "    if X_filtered_h is None or medoids_h is None:\n",
    "        print(\"Error: Hierarchical cluster analysis failed.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Re-sort top_medoids to ensure they're in the right order\n",
    "    if top_medoids_h is not None:\n",
    "        print(\"Re-sorting hierarchical top medoids by Sharpe ratio...\")\n",
    "        top_medoids_h = sorted(top_medoids_h, key=lambda x: float(x[2]), reverse=True)\n",
    "        for idx, medoid in enumerate(top_medoids_h, 1):\n",
    "            print(f\"Verified Hierarchical Medoid {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "                f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "    print(\"\\nPlotting hierarchical strategy performance...\")\n",
    "\n",
    "    # Plot strategy performance with hierarchical results\n",
    "    market_data_h = plot_strategy_performance(\n",
    "        best_short, best_long, top_medoids_h, \n",
    "        big_point_value=big_point_value,\n",
    "        slippage=slippage,\n",
    "        capital=TRADING_CAPITAL,\n",
    "        atr_period=ATR_PERIOD,\n",
    "        analysis_type=\"hierarchical\"\n",
    "    )\n",
    "    \n",
    "    # Run the bimonthly out-of-sample comparison for hierarchical\n",
    "    if top_medoids_h and len(top_medoids_h) > 0:\n",
    "        print(\"\\nPerforming bimonthly out-of-sample comparison with hierarchical clustering...\")\n",
    "        bimonthly_sharpe_df_h = bimonthly_out_of_sample_comparison(\n",
    "            market_data_h, \n",
    "            best_short, \n",
    "            best_long, \n",
    "            top_medoids_h,\n",
    "            big_point_value=big_point_value,\n",
    "            slippage=slippage,\n",
    "            capital=TRADING_CAPITAL,\n",
    "            atr_period=ATR_PERIOD,\n",
    "            analysis_type=\"hierarchical\"\n",
    "        )\n",
    "        \n",
    "        # Calculate win percentage from bimonthly comparison\n",
    "        if bimonthly_sharpe_df_h is not None:\n",
    "            total_periods = len(bimonthly_sharpe_df_h)\n",
    "            if total_periods > 0:\n",
    "                rounded_wins = sum(bimonthly_sharpe_df_h['Avg_Medoid_sharpe_rounded'] > bimonthly_sharpe_df_h['Best_sharpe_rounded'])\n",
    "                win_percentage = (rounded_wins / total_periods) * 100\n",
    "                \n",
    "                # Save hierarchical results to Excel\n",
    "                save_to_excel(SYMBOL, win_percentage, top_medoids_h, best_short, best_long, is_kmeans=False)\n",
    "    else:\n",
    "        print(\"No hierarchical top medoids found. Cannot run bimonthly comparison.\")\n",
    "\n",
    "    print(\"\\nAnalysis complete! All plots and result files have been saved.\")\n",
    "\n",
    "def compute_medoids(X, labels, valid_clusters):\n",
    "    \"\"\"Compute medoids for each cluster (point with minimum distance to all other points in cluster)\"\"\"\n",
    "    medoids = []\n",
    "\n",
    "    for cluster_id in valid_clusters:\n",
    "        # Get points in this cluster\n",
    "        cluster_points = X[labels == cluster_id]\n",
    "\n",
    "        if len(cluster_points) == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate pairwise distances within cluster\n",
    "        min_total_distance = float('inf')\n",
    "        medoid = None\n",
    "\n",
    "        for i, point1 in enumerate(cluster_points):\n",
    "            total_distance = 0\n",
    "            for j, point2 in enumerate(cluster_points):\n",
    "                # Calculate Euclidean distance between points\n",
    "                distance = np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "                total_distance += distance\n",
    "\n",
    "            if total_distance < min_total_distance:\n",
    "                min_total_distance = total_distance\n",
    "                medoid = point1\n",
    "\n",
    "        if medoid is not None:\n",
    "            medoids.append(medoid)\n",
    "\n",
    "    return medoids\n",
    "\n",
    "def cluster_analysis():\n",
    "    \"\"\"\n",
    "    Perform K-means clustering analysis on SMA optimization results\n",
    "    \"\"\"\n",
    "    print(f\"\\n----- CLUSTER ANALYSIS -----\")\n",
    "    \n",
    "    # First get the filtered data from analyze_sma_results\n",
    "    data, _, _, _, _ = analyze_sma_results()\n",
    "    \n",
    "    if data is None:\n",
    "        print(\"Error: Failed to load or analyze SMA results data.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # Convert filtered data to numpy array for clustering\n",
    "    X_filtered = data[['short_SMA', 'long_SMA', 'sharpe_ratio', 'trades']].values\n",
    "\n",
    "    print(f\"Using {len(X_filtered)} filtered data points for clustering\")\n",
    "\n",
    "    # Scale the data for clustering - using StandardScaler for each dimension\n",
    "    scaler = StandardScaler()\n",
    "    X_for_clustering = X_filtered[:, 0:3]  # Only use short_SMA, long_SMA, and sharpe_ratio for clustering\n",
    "    X_scaled = scaler.fit_transform(X_for_clustering)\n",
    "\n",
    "    # Print scaling info for verification\n",
    "    print(\"\\nScaled data information:\")\n",
    "    scaled_short = X_scaled[:, 0]\n",
    "    scaled_long = X_scaled[:, 1]\n",
    "    scaled_sharpe = X_scaled[:, 2]\n",
    "\n",
    "    print(f\"Scaled Short SMA range: {scaled_short.min():.4f} to {scaled_short.max():.4f}\")\n",
    "    print(f\"Scaled Long SMA range: {scaled_long.min():.4f} to {scaled_long.max():.4f}\")\n",
    "    print(f\"Scaled Sharpe ratio range: {scaled_sharpe.min():.4f} to {scaled_sharpe.max():.4f}\")\n",
    "\n",
    "    # Determine number of clusters\n",
    "    print(f\"Using default number of clusters: {DEFAULT_NUM_CLUSTERS}\")\n",
    "    k = DEFAULT_NUM_CLUSTERS\n",
    "\n",
    "    # Apply KMeans clustering\n",
    "    print(f\"Performing KMeans clustering with k={k}...\")\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "\n",
    "    # Get cluster labels\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Count elements per cluster\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    cluster_sizes = dict(zip(unique_labels, counts))\n",
    "\n",
    "    print(\"\\nCluster sizes:\")\n",
    "    for cluster_id, size in cluster_sizes.items():\n",
    "        print(f\"Cluster {cluster_id}: {size} elements\")\n",
    "\n",
    "    # Filter clusters with enough elements\n",
    "    valid_clusters = {i for i, count in cluster_sizes.items() if count >= MIN_ELEMENTS_PER_CLUSTER}\n",
    "    if not valid_clusters:\n",
    "        print(f\"No clusters have at least {MIN_ELEMENTS_PER_CLUSTER} elements! Reducing threshold to 1...\")\n",
    "        valid_clusters = set(unique_labels)\n",
    "    else:\n",
    "        print(f\"Using {len(valid_clusters)} clusters with at least {MIN_ELEMENTS_PER_CLUSTER} elements\")\n",
    "\n",
    "    filtered_indices = np.array([i in valid_clusters for i in labels])\n",
    "\n",
    "    # Filter data to only include points in valid clusters\n",
    "    X_valid = X_filtered[filtered_indices]\n",
    "    labels_valid = labels[filtered_indices]\n",
    "\n",
    "    # Compute medoids using the existing method that expects 4D data\n",
    "    print(\"Computing medoids...\")\n",
    "    medoids = compute_medoids(X_valid, labels_valid, valid_clusters)\n",
    "\n",
    "    # Compute centroids and inverse transform to get back to original scale\n",
    "    centroids_scaled = kmeans.cluster_centers_\n",
    "    centroids = scaler.inverse_transform(centroids_scaled)  # This will give us 3D centroids\n",
    "\n",
    "    # Print raw centroids for debugging\n",
    "    print(\"\\nCluster Centroids (in original space):\")\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        if i in valid_clusters:  # Only show valid clusters\n",
    "            print(f\"Centroid {i}: Short SMA={centroid[0]:.2f}, Long SMA={centroid[1]:.2f}, \"\n",
    "                f\"Sharpe={centroid[2]:.4f}\")\n",
    "\n",
    "    # Simply take top 5 medoids by Sharpe ratio\n",
    "    top_medoids = sorted(medoids, key=lambda x: float(x[2]), reverse=True)[:5]\n",
    "    \n",
    "    # Debug print the sorted top medoids\n",
    "    print(\"\\nSELECTED TOP 5 MEDOIDS BY SHARPE RATIO:\")\n",
    "    for idx, medoid in enumerate(top_medoids, 1):\n",
    "        print(f\"Top {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "            f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "    # Find max Sharpe ratio point overall\n",
    "    max_sharpe_idx = np.argmax(data['sharpe_ratio'].values)\n",
    "    max_sharpe_point = data.iloc[max_sharpe_idx][['short_SMA', 'long_SMA', 'sharpe_ratio', 'trades']].values\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n----- CLUSTERING RESULTS -----\")\n",
    "    print(f\"Max Sharpe point: Short SMA={int(max_sharpe_point[0])}, Long SMA={int(max_sharpe_point[1])}, \"\n",
    "        f\"Sharpe={max_sharpe_point[2]:.4f}, Trades={int(max_sharpe_point[3])}\")\n",
    "\n",
    "    print(\"\\nTop 5 Medoids (by Sharpe ratio):\")\n",
    "    for idx, medoid in enumerate(top_medoids, 1):\n",
    "        print(f\"Top {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "            f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "    # Create visualization with clustering results - pass the original labels and valid_clusters\n",
    "    create_cluster_visualization(X_filtered, medoids, top_medoids, centroids, max_sharpe_point, \n",
    "                            labels=labels, valid_clusters=valid_clusters)\n",
    "\n",
    "    return X_filtered, medoids, top_medoids, centroids, max_sharpe_point\n",
    "\n",
    "def create_cluster_visualization(X_filtered_full, medoids, top_medoids, centroids, max_sharpe_point, labels=None, valid_clusters=None, analysis_type=\"kmeans\"):\n",
    "    \"\"\"\n",
    "    Create visualization of clustering results with both unfiltered and filtered heatmaps\n",
    "    \"\"\"\n",
    "    print(\"Creating cluster visualization...\")\n",
    "    \n",
    "    # Load the full dataset for the unfiltered heatmap\n",
    "    data = pd.read_csv('sma_all_results.txt')\n",
    "    \n",
    "    # First plot: Unfiltered heatmap (only short_SMA < long_SMA)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create a pivot table for the unfiltered heatmap\n",
    "    unfiltered_heatmap = data.pivot_table(\n",
    "        index='long_SMA',\n",
    "        columns='short_SMA',\n",
    "        values='sharpe_ratio',\n",
    "        fill_value=np.nan\n",
    "    )\n",
    "    \n",
    "    # Create mask for invalid combinations (where short_SMA >= long_SMA)\n",
    "    unfiltered_mask = np.zeros_like(unfiltered_heatmap, dtype=bool)\n",
    "    for i, long_sma in enumerate(unfiltered_heatmap.index):\n",
    "        for j, short_sma in enumerate(unfiltered_heatmap.columns):\n",
    "            if short_sma >= long_sma:\n",
    "                unfiltered_mask[i, j] = True\n",
    "    \n",
    "    # Plot the unfiltered heatmap\n",
    "    ax = sns.heatmap(\n",
    "        unfiltered_heatmap,\n",
    "        mask=unfiltered_mask,\n",
    "        cmap='coolwarm',\n",
    "        annot=False,\n",
    "        fmt='.4f',\n",
    "        linewidths=0,\n",
    "        cbar_kws={'label': 'Sharpe Ratio'}\n",
    "    )\n",
    "    \n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Plot max Sharpe point on unfiltered heatmap\n",
    "    try:\n",
    "        best_x_pos = np.where(unfiltered_heatmap.columns == max_sharpe_point[0])[0][0] + 0.5\n",
    "        best_y_pos = np.where(unfiltered_heatmap.index == max_sharpe_point[1])[0][0] + 0.5\n",
    "        plt.scatter(best_x_pos, best_y_pos, marker='*', color='lime', s=200,\n",
    "                    edgecolor='black', zorder=5)\n",
    "    except IndexError:\n",
    "        print(f\"Warning: Max Sharpe point at ({max_sharpe_point[0]}, {max_sharpe_point[1]}) not found in unfiltered heatmap coordinates\")\n",
    "    \n",
    "    # Create custom legend for unfiltered heatmap\n",
    "    max_sharpe_handle = mlines.Line2D([], [], color='lime', marker='*', linestyle='None',\n",
    "                                    markersize=15, markeredgecolor='black', label='Max Sharpe')\n",
    "    \n",
    "    plt.legend(handles=[max_sharpe_handle], loc='lower right')\n",
    "    plt.title(f'Unfiltered {analysis_type} Heatmap (Only short_SMA < long_SMA)', pad=20)\n",
    "    plt.xlabel('Short SMA (days)', fontsize=12)\n",
    "    plt.ylabel('Long SMA (days)', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Second plot: Filtered heatmap with clustering results\n",
    "    print(\"Creating filtered heatmap with clustering results...\")\n",
    "    \n",
    "    # Create filtered dataframe from the filtered points that meet trade requirements\n",
    "    filtered_df = pd.DataFrame(X_filtered_full, columns=['short_SMA', 'long_SMA', 'sharpe_ratio', 'trades'])\n",
    "    \n",
    "    # Create a pivot table for the filtered heatmap\n",
    "    heatmap_data = filtered_df.pivot_table(\n",
    "        index='long_SMA',\n",
    "        columns='short_SMA',\n",
    "        values='sharpe_ratio',\n",
    "        fill_value=np.nan\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create mask for invalid combinations and NaN values\n",
    "    mask = np.zeros_like(heatmap_data, dtype=bool)\n",
    "    for i, long_sma in enumerate(heatmap_data.index):\n",
    "        for j, short_sma in enumerate(heatmap_data.columns):\n",
    "            if short_sma >= long_sma or np.isnan(heatmap_data.iloc[i, j]):\n",
    "                mask[i, j] = True\n",
    "    \n",
    "    # Plot the filtered heatmap\n",
    "    ax = sns.heatmap(\n",
    "        heatmap_data,\n",
    "        mask=mask,\n",
    "        cmap='coolwarm',\n",
    "        annot=False,\n",
    "        fmt='.4f',\n",
    "        linewidths=0,\n",
    "        cbar_kws={'label': 'Sharpe Ratio'}\n",
    "    )\n",
    "    \n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Plot max Sharpe point\n",
    "    try:\n",
    "        best_x_pos = np.where(heatmap_data.columns == max_sharpe_point[0])[0][0] + 0.5\n",
    "        best_y_pos = np.where(heatmap_data.index == max_sharpe_point[1])[0][0] + 0.5\n",
    "        plt.scatter(best_x_pos, best_y_pos, marker='*', color='lime', s=200,\n",
    "                    edgecolor='black', zorder=5)\n",
    "    except IndexError:\n",
    "        print(f\"Warning: Max Sharpe point at ({max_sharpe_point[0]}, {max_sharpe_point[1]}) not found in filtered heatmap coordinates\")\n",
    "    \n",
    "    # Plot medoids\n",
    "    if medoids:\n",
    "        for medoid in medoids:\n",
    "            try:\n",
    "                x_pos = np.where(heatmap_data.columns == medoid[0])[0][0] + 0.5\n",
    "                y_pos = np.where(heatmap_data.index == medoid[1])[0][0] + 0.5\n",
    "                plt.scatter(x_pos, y_pos, marker='s', color='black', s=75, zorder=4)\n",
    "            except IndexError:\n",
    "                print(f\"Warning: Medoid at ({medoid[0]}, {medoid[1]}) not found in filtered heatmap coordinates\")\n",
    "    \n",
    "    # Plot top medoids\n",
    "    if top_medoids:\n",
    "        for medoid in top_medoids:\n",
    "            try:\n",
    "                x_pos = np.where(heatmap_data.columns == medoid[0])[0][0] + 0.5\n",
    "                y_pos = np.where(heatmap_data.index == medoid[1])[0][0] + 0.5\n",
    "                plt.scatter(x_pos, y_pos, marker='D', color='purple', s=100, zorder=5)\n",
    "            except IndexError:\n",
    "                print(f\"Warning: Top medoid at ({medoid[0]}, {medoid[1]}) not found in filtered heatmap coordinates\")\n",
    "    \n",
    "    # Plot centroids\n",
    "    print(f\"Plotting centroids from valid clusters...\")\n",
    "    centroids_plotted = 0\n",
    "    \n",
    "    # If valid_clusters is not provided, assume all centroids are valid\n",
    "    plot_centroids = centroids\n",
    "    if valid_clusters is not None and labels is not None:\n",
    "        plot_centroids = [centroids[i] for i in range(len(centroids)) if i in valid_clusters]\n",
    "        print(f\"Filtering centroids to only include valid clusters: {valid_clusters}\")\n",
    "    \n",
    "    for i, centroid in enumerate(plot_centroids):\n",
    "        short_sma = centroid[0]\n",
    "        long_sma = centroid[1]\n",
    "        \n",
    "        print(f\"Centroid {i}: raw values = ({short_sma}, {long_sma})\")\n",
    "        \n",
    "        # First try exact values\n",
    "        try:\n",
    "            if (short_sma in heatmap_data.columns) and (long_sma in heatmap_data.index) and (short_sma < long_sma):\n",
    "                x_pos = np.where(heatmap_data.columns == short_sma)[0][0] + 0.5\n",
    "                y_pos = np.where(heatmap_data.index == long_sma)[0][0] + 0.5\n",
    "                plt.scatter(x_pos, y_pos, marker='o', color='blue', s=75, zorder=4)\n",
    "                centroids_plotted += 1\n",
    "                continue\n",
    "        except (IndexError, TypeError):\n",
    "            pass\n",
    "        \n",
    "        # Try rounded values\n",
    "        try:\n",
    "            short_sma_rounded = int(round(short_sma))\n",
    "            long_sma_rounded = int(round(long_sma))\n",
    "            \n",
    "            print(f\"  Rounded: ({short_sma_rounded}, {long_sma_rounded})\")\n",
    "            \n",
    "            if (short_sma_rounded in heatmap_data.columns) and (long_sma_rounded in heatmap_data.index) and (\n",
    "                    short_sma_rounded < long_sma_rounded):\n",
    "                x_pos = np.where(heatmap_data.columns == short_sma_rounded)[0][0] + 0.5\n",
    "                y_pos = np.where(heatmap_data.index == long_sma_rounded)[0][0] + 0.5\n",
    "                plt.scatter(x_pos, y_pos, marker='o', color='blue', s=75, zorder=4)\n",
    "                centroids_plotted += 1\n",
    "                continue\n",
    "        except (IndexError, TypeError):\n",
    "            pass\n",
    "        \n",
    "        # Try finding nearest valid point\n",
    "        try:\n",
    "            short_options = np.array(heatmap_data.columns)\n",
    "            long_options = np.array(heatmap_data.index)\n",
    "            \n",
    "            short_idx = np.argmin(np.abs(short_options - short_sma))\n",
    "            short_nearest = short_options[short_idx]\n",
    "            \n",
    "            long_idx = np.argmin(np.abs(long_options - long_sma))\n",
    "            long_nearest = long_options[long_idx]\n",
    "            \n",
    "            print(f\"  Nearest: ({short_nearest}, {long_nearest})\")\n",
    "            \n",
    "            if short_nearest < long_nearest:\n",
    "                x_pos = np.where(heatmap_data.columns == short_nearest)[0][0] + 0.5\n",
    "                y_pos = np.where(heatmap_data.index == long_nearest)[0][0] + 0.5\n",
    "                plt.scatter(x_pos, y_pos, marker='o', color='blue', s=75, zorder=4, alpha=0.7)\n",
    "                centroids_plotted += 1\n",
    "            else:\n",
    "                print(f\"  Invalid nearest parameters (short >= long): {short_nearest} >= {long_nearest}\")\n",
    "        except (IndexError, TypeError) as e:\n",
    "            print(f\"  Error finding nearest point: {e}\")\n",
    "    \n",
    "    print(f\"Successfully plotted {centroids_plotted} out of {len(plot_centroids)} centroids\")\n",
    "    \n",
    "    # Create custom legend\n",
    "    max_sharpe_handle = mlines.Line2D([], [], color='lime', marker='*', linestyle='None',\n",
    "                                    markersize=15, markeredgecolor='black', label='Max Sharpe')\n",
    "    # Use different legend labels based on analysis type\n",
    "    if analysis_type == \"hierarchical\":\n",
    "        cluster_handle = mlines.Line2D([], [], color='black', marker='s', linestyle='None',\n",
    "                                    markersize=10, label='Clusters')\n",
    "        top_cluster_handle = mlines.Line2D([], [], color='purple', marker='D', linestyle='None',\n",
    "                                        markersize=10, label='Top 5 Clusters')\n",
    "    else:\n",
    "        cluster_handle = mlines.Line2D([], [], color='black', marker='s', linestyle='None',\n",
    "                                    markersize=10, label='Medoids')\n",
    "        top_cluster_handle = mlines.Line2D([], [], color='purple', marker='D', linestyle='None',\n",
    "                                        markersize=10, label='Top 5 Medoids')\n",
    "    centroid_handle = mlines.Line2D([], [], color='blue', marker='o', linestyle='None',\n",
    "                                    markersize=10, label='Centroids')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(handles=[max_sharpe_handle, cluster_handle, top_cluster_handle, centroid_handle],\n",
    "            loc='lower right')\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.title(f'Filtered {analysis_type} Cluster Analysis', pad=20)\n",
    "    plt.xlabel('Short SMA (days)', fontsize=12)\n",
    "    plt.ylabel('Long SMA (days)', fontsize=12)\n",
    "    \n",
    "    # Rotate tick labels\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def bimonthly_out_of_sample_comparison(data, best_short_sma, best_long_sma, top_medoids, min_sharpe=0.2, \n",
    "                                big_point_value=1, slippage=0, capital=TRADING_CAPITAL, atr_period=ATR_PERIOD,\n",
    "                                analysis_type=\"kmeans\"):  # Add analysis_type parameter\n",
    "    \"\"\"\n",
    "    Compare the performance of the best Sharpe ratio strategy against a portfolio of top medoids\n",
    "    on a bimonthly basis using out-of-sample data\n",
    "    \"\"\"\n",
    "    print(f\"\\n----- BIMONTHLY OUT-OF-SAMPLE COMPARISON -----\")\n",
    "    print(f\"Best Sharpe: ({best_short_sma}/{best_long_sma})\")\n",
    "    print(f\"Using ATR-based position sizing (Capital: ${capital:,}, ATR Period: {atr_period})\")\n",
    "    \n",
    "    # Handle the case where top_medoids is None\n",
    "    if top_medoids is None:\n",
    "        print(\"No medoids provided. Comparison cannot be performed.\")\n",
    "        return None\n",
    "    \n",
    "    # The top_medoids are already sorted by Sharpe ratio in cluster_analysis\n",
    "    # Just print them for verification\n",
    "    print(\"\\nUSING TOP MEDOIDS (BY SHARPE RATIO):\")\n",
    "    for i, medoid in enumerate(top_medoids, 1):\n",
    "        print(f\"Medoid {i}: ({int(medoid[0])}/{int(medoid[1])}) - Sharpe: {float(medoid[2]):.4f}, Trades: {int(medoid[3])}\")\n",
    "    \n",
    "    # Take at most 3 medoids and filter by minimum Sharpe\n",
    "    filtered_medoids = []\n",
    "    for i, m in enumerate(top_medoids[:3]):\n",
    "        # Check if we can access the required elements\n",
    "        try:\n",
    "            # Extract Sharpe ratio and check if it meets the threshold\n",
    "            short_sma = m[0]\n",
    "            long_sma = m[1]\n",
    "            sharpe = float(m[2])  # Convert to float to handle numpy types\n",
    "            trades = m[3]\n",
    "            \n",
    "            if sharpe >= min_sharpe:\n",
    "                filtered_medoids.append(m)\n",
    "                print(f\"Selected medoid {i+1} with Sharpe {sharpe:.4f}\")\n",
    "        except (IndexError, TypeError) as e:\n",
    "            print(f\"Error processing medoid: {e}\")\n",
    "    \n",
    "    if not filtered_medoids:\n",
    "        print(f\"No medoids have a Sharpe ratio >= {min_sharpe}. Comparison cannot be performed.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Creating portfolio of {len(filtered_medoids)} medoids with Sharpe ratio >= {min_sharpe}:\")\n",
    "    for i, medoid in enumerate(filtered_medoids, 1):\n",
    "        print(f\"Final Medoid {i}: ({int(medoid[0])}/{int(medoid[1])}) - Sharpe: {float(medoid[2]):.4f}\")\n",
    "    \n",
    "    # Create strategies\n",
    "    strategies = {\n",
    "        'Best': {'short_sma': best_short_sma, 'long_sma': best_long_sma}\n",
    "    }\n",
    "    \n",
    "    # Add filtered medoids\n",
    "    for i, medoid in enumerate(filtered_medoids, 1):\n",
    "        strategies[f'Medoid_{i}'] = {'short_sma': int(medoid[0]), 'long_sma': int(medoid[1])}\n",
    "    \n",
    "    # Apply each strategy to the data\n",
    "    for name, params in strategies.items():\n",
    "        strategy = SMAStrategy(\n",
    "            short_sma=params['short_sma'],\n",
    "            long_sma=params['long_sma'],\n",
    "            big_point_value=big_point_value,\n",
    "            slippage=slippage,\n",
    "            capital=capital,\n",
    "            atr_period=atr_period\n",
    "        )\n",
    "        \n",
    "        # Apply the strategy\n",
    "        data = strategy.apply_strategy(\n",
    "            data.copy(),\n",
    "            strategy_name=name\n",
    "        )\n",
    "    \n",
    "    # Get the out-of-sample split date\n",
    "    split_index = int(len(data) * TRAIN_TEST_SPLIT)\n",
    "    split_date = data.index[split_index]\n",
    "    print(f\"Out-of-sample period starts on: {split_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Get out-of-sample data\n",
    "    oos_data = data.iloc[split_index:].copy()\n",
    "    \n",
    "    # Add a year and bimonthly period columns for grouping (each year has 6 bimonthly periods)\n",
    "    oos_data['year'] = oos_data.index.year.astype(int)\n",
    "    oos_data['bimonthly'] = ((oos_data.index.month - 1) // 2 + 1).astype(int)\n",
    "    \n",
    "    # Create simplified period labels with just the start month (YYYY-MM)\n",
    "    oos_data['period_label'] = oos_data.apply(\n",
    "        lambda row: f\"{int(row['year'])}-{int((row['bimonthly'] - 1) * 2 + 1):02d}\",\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create a DataFrame to store bimonthly Sharpe ratios\n",
    "    bimonthly_sharpe = []\n",
    "    \n",
    "    # Group by year and bimonthly period, calculate Sharpe ratio for each period\n",
    "    for period_label, group in oos_data.groupby('period_label'):\n",
    "        # Skip periods with too few trading days\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "            \n",
    "        # Create a bimonthly result entry\n",
    "        year, start_month = period_label.split('-')\n",
    "        year = int(year)\n",
    "        start_month = int(start_month)\n",
    "        \n",
    "        bimonthly_result = {\n",
    "            'period_label': period_label,\n",
    "            'date': pd.Timestamp(year=year, month=start_month, day=15),  # Middle of first month in period\n",
    "            'trading_days': len(group),\n",
    "        }\n",
    "        \n",
    "        # Calculate Sharpe ratio for each strategy in this period\n",
    "        for name in strategies.keys():\n",
    "            # Get returns for this strategy in this period\n",
    "            returns = group[f'Daily_PnL_{name}']\n",
    "            \n",
    "            # Calculate Sharpe ratio (annualized)\n",
    "            if len(returns) > 1 and returns.std() > 0:\n",
    "                sharpe = returns.mean() / returns.std() * np.sqrt(252)\n",
    "            else:\n",
    "                sharpe = 0\n",
    "                \n",
    "            bimonthly_result[f'{name}_sharpe'] = sharpe\n",
    "            bimonthly_result[f'{name}_return'] = returns.sum()  # Total P&L for the period\n",
    "        \n",
    "        # Calculate the normalized average of medoid Sharpe ratios\n",
    "        medoid_sharpes = [bimonthly_result[f'Medoid_{i}_sharpe'] for i in range(1, len(filtered_medoids) + 1)]\n",
    "        bimonthly_result['Avg_Medoid_sharpe'] = sum(medoid_sharpes) / len(filtered_medoids)\n",
    "        \n",
    "        # Calculate the normalized average of medoid returns\n",
    "        medoid_returns = [bimonthly_result[f'Medoid_{i}_return'] for i in range(1, len(filtered_medoids) + 1)]\n",
    "        bimonthly_result['Avg_Medoid_return'] = sum(medoid_returns) / len(filtered_medoids)\n",
    "        \n",
    "        bimonthly_sharpe.append(bimonthly_result)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    bimonthly_sharpe_df = pd.DataFrame(bimonthly_sharpe)\n",
    "    \n",
    "    # Sort the DataFrame by date for proper chronological display\n",
    "    if not bimonthly_sharpe_df.empty:\n",
    "        bimonthly_sharpe_df = bimonthly_sharpe_df.sort_values('date')\n",
    "    else:\n",
    "        print(f\"WARNING: No bimonthly periods found for {SYMBOL}. Cannot create chart.\")\n",
    "        return None\n",
    "    \n",
    "    # Add rounded values to dataframe for calculations\n",
    "    bimonthly_sharpe_df['Best_sharpe_rounded'] = np.round(bimonthly_sharpe_df['Best_sharpe'], 2)\n",
    "    bimonthly_sharpe_df['Avg_Medoid_sharpe_rounded'] = np.round(bimonthly_sharpe_df['Avg_Medoid_sharpe'], 2)\n",
    "    \n",
    "    # Print detailed comparison of Sharpe ratios\n",
    "    print(\"\\nDetailed Sharpe ratio comparison by period:\")\n",
    "    print(f\"{'Period':<12} | {'Best Sharpe':>12} | {'Medoid Portfolio':>16} | {'Difference':>12} | {'Portfolio Wins':<14}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for idx, row in bimonthly_sharpe_df.iterrows():\n",
    "        period = row['period_label']\n",
    "        best_sharpe = row['Best_sharpe']\n",
    "        avg_medoid_sharpe = row['Avg_Medoid_sharpe']\n",
    "        best_rounded = row['Best_sharpe_rounded']\n",
    "        avg_medoid_rounded = row['Avg_Medoid_sharpe_rounded']\n",
    "        \n",
    "        diff = avg_medoid_sharpe - best_sharpe\n",
    "        portfolio_wins = avg_medoid_sharpe > best_sharpe\n",
    "        \n",
    "        print(f\"{period:<12} | {best_sharpe:12.6f} | {avg_medoid_sharpe:16.6f} | {diff:12.6f} | {portfolio_wins!s:<14}\")\n",
    "    \n",
    "    # Calculate win rate using raw values\n",
    "    portfolio_wins = sum(bimonthly_sharpe_df['Avg_Medoid_sharpe'] > bimonthly_sharpe_df['Best_sharpe'])\n",
    "    total_periods = len(bimonthly_sharpe_df)\n",
    "    win_percentage = (portfolio_wins / total_periods) * 100 if total_periods > 0 else 0\n",
    "    \n",
    "    # Calculate win rate using rounded values (for alternative comparison)\n",
    "    rounded_wins = sum(bimonthly_sharpe_df['Avg_Medoid_sharpe_rounded'] > bimonthly_sharpe_df['Best_sharpe_rounded'])\n",
    "    rounded_win_percentage = (rounded_wins / total_periods) * 100 if total_periods > 0 else 0\n",
    "    \n",
    "    print(f\"\\nBimonthly periods analyzed: {total_periods}\")\n",
    "    print(f\"Medoid Portfolio Wins: {portfolio_wins} of {total_periods} periods ({win_percentage:.2f}%)\")\n",
    "    print(f\"Using rounded values (2 decimal places): {rounded_wins} of {total_periods} periods ({rounded_win_percentage:.2f}%)\")\n",
    "    \n",
    "    # Create a bar plot to compare bimonthly Sharpe ratios\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Set up x-axis dates\n",
    "    x = np.arange(len(bimonthly_sharpe_df))\n",
    "    width = 0.35  # Width of the bars\n",
    "    \n",
    "    # Create bars\n",
    "    plt.bar(x - width/2, bimonthly_sharpe_df['Best_sharpe'], width, \n",
    "        label=f'Best Sharpe ({best_short_sma}/{best_long_sma})', color='blue')\n",
    "    plt.bar(x + width/2, bimonthly_sharpe_df['Avg_Medoid_sharpe'], width, \n",
    "        label=f'Medoid Portfolio ({len(filtered_medoids)} strategies)', color='green')\n",
    "    \n",
    "    # Add a horizontal line at Sharpe = 0\n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Create medoid description for the title\n",
    "    medoid_desc = \", \".join([f\"({int(m[0])}/{int(m[1])})\" for m in filtered_medoids])\n",
    "\n",
    "    # Add analysis type to the title\n",
    "    title_prefix = \"K-means-Medoids\" if analysis_type == \"kmeans\" else \"Hierarchical\"\n",
    "    plt.title(f'{title_prefix} Bimonthly Out-of-Sample Comparison\\nBest Sharpe vs Medoid Portfolio', pad=20)\n",
    "    \n",
    "    # Create legend with both strategies - moved to bottom to avoid overlap with title\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2,\n",
    "            frameon=True, fancybox=True, framealpha=0.9, fontsize=10)\n",
    "    \n",
    "    # Add a text box with rounded win percentage instead of raw - moved to right side\n",
    "    plt.annotate(f'Medoid Portfolio Win Rate: {rounded_win_percentage:.2f}%\\n'\n",
    "                f'({rounded_wins} out of {total_periods} periods)\\n'\n",
    "                f'Portfolio: {len(filtered_medoids)} medoids with Sharpe ≥ {min_sharpe}\\n'\n",
    "                f'ATR-Based Position Sizing (${capital:,}, {atr_period} days)',\n",
    "                xy=(0.7, 0.95), xycoords='axes fraction',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "                fontsize=12)\n",
    "    \n",
    "    # Use period labels for x-axis\n",
    "    plt.xticks(x, [pd.to_datetime(label).strftime('%Y-%m') for label in bimonthly_sharpe_df['period_label']], rotation=45, ha='right')\n",
    "    plt.xlabel('Bimonthly Period (Start Month)', fontsize=12)\n",
    "    plt.ylabel('Sharpe Ratio (Annualized)', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Add extra space at the bottom for the legend\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return bimonthly_sharpe_df\n",
    "\n",
    "def plot_strategy_performance(short_sma, long_sma, top_medoids=None, big_point_value=1, slippage=0, capital=TRADING_CAPITAL, atr_period=ATR_PERIOD, analysis_type=\"kmeans\"):\n",
    "    \"\"\"\n",
    "    Plot strategy performance including price, SMAs, and P&L for multiple strategies\n",
    "    \"\"\"\n",
    "    print(f\"\\n----- PLOTTING STRATEGY PERFORMANCE -----\")\n",
    "    print(f\"Using Short SMA: {short_sma}, Long SMA: {long_sma}\")\n",
    "    print(f\"Trading with ATR-based position sizing (Capital: ${capital:,}, ATR Period: {atr_period})\")\n",
    "    if top_medoids:\n",
    "        print(f\"Including top {len(top_medoids)} medoids\")\n",
    "\n",
    "    # Load data from local file\n",
    "    print(f\"Loading {TICKER} data from local files...\")\n",
    "    data_file = find_futures_file(SYMBOL, DATA_DIR)\n",
    "    if not data_file:\n",
    "        print(f\"Error: No data file found for {TICKER} in {DATA_DIR}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"Found data file: {os.path.basename(data_file)}\")\n",
    "    print(f\"File size: {os.path.getsize(data_file)} bytes\")\n",
    "\n",
    "    # Load the data from local file\n",
    "    all_data = read_ts_ohlcv_dat(data_file)\n",
    "    data_obj = all_data[0]\n",
    "    ohlc_data = data_obj.data.copy()\n",
    "\n",
    "    # Convert the OHLCV data to the format expected by the strategy\n",
    "    data = ohlc_data.rename(columns={\n",
    "        'datetime': 'Date',\n",
    "        'open': 'Open',\n",
    "        'high': 'High',\n",
    "        'low': 'Low',\n",
    "        'close': 'Close',\n",
    "        'volume': 'Volume'\n",
    "    })\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    # Add a warm-up period before the start date\n",
    "    original_start_idx = None\n",
    "    if START_DATE and END_DATE:\n",
    "        # Calculate warm-up period for SMA calculation (longest SMA + buffer)\n",
    "        warm_up_days = max(short_sma, long_sma) * 3  # Use 3x the longest SMA as warm-up\n",
    "        \n",
    "        # Convert dates to datetime\n",
    "        start_date = pd.to_datetime(START_DATE)\n",
    "        end_date = pd.to_datetime(END_DATE)\n",
    "        \n",
    "        # Adjust start date for warm-up\n",
    "        adjusted_start = start_date - pd.Timedelta(days=warm_up_days)\n",
    "        \n",
    "        # Filter with the extended date range\n",
    "        extended_data = data[(data.index >= adjusted_start) & (data.index <= end_date)]\n",
    "        \n",
    "        # Store the index where the actual analysis should start\n",
    "        if not extended_data.empty:\n",
    "            # Find the closest index to our original start date\n",
    "            original_start_idx = extended_data.index.get_indexer([start_date], method='nearest')[0]\n",
    "        \n",
    "        print(f\"Added {warm_up_days} days warm-up period before {START_DATE}\")\n",
    "        data = extended_data\n",
    "\n",
    "    # Create a dictionary to store results for each strategy\n",
    "    strategies = {\n",
    "        'Best': {'short_sma': short_sma, 'long_sma': long_sma}\n",
    "    }\n",
    "\n",
    "    # Add medoids in their original order, including original Sharpe and Trades\n",
    "    if top_medoids:\n",
    "        print(\"\\nUSING THESE MEDOIDS (IN ORIGINAL ORDER):\")\n",
    "        for i, medoid in enumerate(top_medoids, 1):\n",
    "            strategies[f'Medoid {i}'] = {\n",
    "                'short_sma': int(medoid[0]),\n",
    "                'long_sma': int(medoid[1]),\n",
    "                'original_sharpe': float(medoid[2]),  # Store the original Sharpe ratio\n",
    "                'original_trades': int(medoid[3])     # Store the original number of trades\n",
    "            }\n",
    "            print(f\"Medoid {i}: SMA({int(medoid[0])}/{int(medoid[1])}) - Original Sharpe: {float(medoid[2]):.4f}, Trades: {int(medoid[3])}\")\n",
    "\n",
    "    # Apply the proper strategy for each parameter set\n",
    "    for name, params in strategies.items():\n",
    "        # Calculate centered SMAs directly\n",
    "        data[f'SMA_Short_{name}'] = data['Close'].rolling(window=params['short_sma'], center=True).mean()\n",
    "        data[f'SMA_Long_{name}'] = data['Close'].rolling(window=params['long_sma'], center=True).mean()\n",
    "        \n",
    "        # Create a strategy instance for each parameter set\n",
    "        sma_strategy = SMAStrategy(\n",
    "            short_sma=params['short_sma'],\n",
    "            long_sma=params['long_sma'],\n",
    "            big_point_value=big_point_value,\n",
    "            slippage=slippage,\n",
    "            capital=capital,\n",
    "            atr_period=atr_period\n",
    "        )\n",
    "\n",
    "        # Apply the strategy\n",
    "        data = sma_strategy.apply_strategy(\n",
    "            data.copy(),\n",
    "            strategy_name=name\n",
    "        )\n",
    "\n",
    "    # Trim data to the original date range if we added warm-up period\n",
    "    if original_start_idx is not None:\n",
    "        data_for_evaluation = data.iloc[original_start_idx:]\n",
    "        print(f\"Trimmed warm-up period. Original data length: {len(data)}, Evaluation data length: {len(data_for_evaluation)}\")\n",
    "    else:\n",
    "        data_for_evaluation = data\n",
    "\n",
    "    # Calculate split index for in-sample/out-of-sample using the trimmed data\n",
    "    split_index = int(len(data_for_evaluation) * TRAIN_TEST_SPLIT)\n",
    "    split_date = data_for_evaluation.index[split_index]\n",
    "\n",
    "    # Add analysis type to the title\n",
    "    title_prefix = \"K-means-Medoids\" if analysis_type == \"kmeans\" else \"Hierarchical\"\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), height_ratios=[1, 1.5])\n",
    "    fig.suptitle(f'{title_prefix} Strategy Performance Analysis - {SYMBOL}', fontsize=16)\n",
    "    \n",
    "    # Plot price and SMA (first subplot)\n",
    "    ax1.plot(data_for_evaluation.index, data_for_evaluation['Close'], label=f'{SYMBOL} Price', color='black', alpha=0.5)\n",
    "    \n",
    "    # Use the centered SMAs for plotting\n",
    "    for name, params in strategies.items():\n",
    "        if name == 'Best':\n",
    "            ax1.plot(data_for_evaluation.index, data_for_evaluation[f'SMA_Short_{name}'], \n",
    "                    label=f'Short SMA ({params[\"short_sma\"]})', color='orange')\n",
    "            ax1.plot(data_for_evaluation.index, data_for_evaluation[f'SMA_Long_{name}'], \n",
    "                    label=f'Long SMA ({params[\"long_sma\"]})', color='blue')\n",
    "    \n",
    "    # Mark the train/test split\n",
    "    ax1.axvline(x=split_date, color='black', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.set_title(f'{SYMBOL} Price and SMA Indicators')\n",
    "    ax1.set_ylabel('Price')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot cumulative P&L (second subplot)\n",
    "    colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink']\n",
    "    \n",
    "    for i, (name, params) in enumerate(strategies.items()):\n",
    "        color = colors[i % len(colors)]\n",
    "\n",
    "        # Plot full period P&L\n",
    "        ax2.plot(data_for_evaluation.index, data_for_evaluation[f'Cumulative_PnL_{name}'],\n",
    "                label=f'{name} ({params[\"short_sma\"]}/{params[\"long_sma\"]})', color=color)\n",
    "\n",
    "        # Plot out-of-sample portion with thicker line\n",
    "        ax2.plot(data_for_evaluation.index[split_index:], data_for_evaluation[f'Cumulative_PnL_{name}'].iloc[split_index:],\n",
    "                color=color, linewidth=2.5, alpha=0.7)\n",
    "\n",
    "    ax2.axvline(x=split_date, color='black', linestyle='--',\n",
    "                label=f'Train/Test Split ({int(TRAIN_TEST_SPLIT * 100)}%/{int((1 - TRAIN_TEST_SPLIT) * 100)}%)')\n",
    "    ax2.axhline(y=0.0, color='gray', linestyle='-', alpha=0.5, label='Break-even')\n",
    "    ax2.legend(loc='upper left')\n",
    "    ax2.set_title('Strategy Cumulative P&L')\n",
    "    ax2.set_ylabel('P&L ($)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return data\n",
    "\n",
    "def compute_hierarchical_medoids(X, labels, valid_clusters):\n",
    "    \"\"\"\n",
    "    Compute medoids for each valid cluster from hierarchical clustering\n",
    "    A medoid is the data point in a cluster that has the minimum average distance to all other points in the cluster\n",
    "\n",
    "    Parameters:\n",
    "    X: numpy array of shape (n_samples, n_features) - Original data points (not scaled)\n",
    "    labels: numpy array of shape (n_samples,) - Cluster labels for each data point\n",
    "    valid_clusters: set - Set of valid cluster IDs\n",
    "\n",
    "    Returns:\n",
    "    list of tuples - Each tuple contains (short_SMA, long_SMA, sharpe_ratio, trades) for each medoid\n",
    "    \"\"\"\n",
    "    medoids = []\n",
    "    for cluster_id in valid_clusters:\n",
    "        cluster_indices = np.where(labels == cluster_id)[0]\n",
    "        cluster_points = X[cluster_indices]\n",
    "\n",
    "        if len(cluster_points) == 0:\n",
    "            continue\n",
    "\n",
    "        if len(cluster_points) == 1:\n",
    "            medoids.append(tuple(cluster_points[0]))\n",
    "            continue\n",
    "\n",
    "        distances = cdist(cluster_points, cluster_points, metric='euclidean')\n",
    "        total_distances = np.sum(distances, axis=1)\n",
    "        medoid_idx = np.argmin(total_distances)\n",
    "        medoid = cluster_points[medoid_idx]\n",
    "        medoids.append(tuple(medoid))\n",
    "\n",
    "        print(f\"Cluster {cluster_id} medoid: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "            f\"Sharpe={medoid[2]:.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "    return medoids\n",
    "\n",
    "def create_dendrogram(X_scaled, method='ward', figsize=(12, 8), color_threshold=None, truncate_mode=None,\n",
    "                    p=10, save_path=None):\n",
    "    \"\"\"\n",
    "    Create and display a dendrogram for hierarchical clustering\n",
    "    \"\"\"\n",
    "    if len(X_scaled) > 1000:\n",
    "        np.random.seed(42)\n",
    "        sample_indices = np.random.choice(len(X_scaled), size=1000, replace=False)\n",
    "        X_sample = X_scaled[sample_indices]\n",
    "        print(f\"Using a random sample of 1000 points for dendrogram creation (out of {len(X_scaled)} total points)\")\n",
    "    else:\n",
    "        X_sample = X_scaled\n",
    "\n",
    "    dist_matrix = pdist(X_sample, metric='euclidean')\n",
    "    Z = shc.linkage(dist_matrix, method=method)\n",
    "\n",
    "    print(f\"\\nDendrogram using {method} linkage method:\")\n",
    "    print(f\"Number of data points: {len(X_sample)}\")\n",
    "    print(f\"Cophenetic correlation: {shc.cophenet(Z, dist_matrix)[0]:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.gca().set_facecolor('white')\n",
    "    plt.title(f'Hierarchical Clustering Dendrogram ({method} linkage)', fontsize=14)\n",
    "\n",
    "    dendrogram = shc.dendrogram(\n",
    "        Z,\n",
    "        truncate_mode='lastp',\n",
    "        p=30,\n",
    "        leaf_rotation=90.,\n",
    "        leaf_font_size=10.,\n",
    "        show_contracted=True,\n",
    "        color_threshold=color_threshold\n",
    "    )\n",
    "\n",
    "    plt.xlabel('Sample Index or Cluster Size', fontsize=12)\n",
    "    plt.ylabel('Distance', fontsize=12)\n",
    "\n",
    "    if color_threshold is not None:\n",
    "        plt.axhline(y=color_threshold, color='crimson', linestyle='--',\n",
    "                    label=f'Threshold: {color_threshold:.2f}')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return Z\n",
    "\n",
    "def hierarchical_cluster_analysis(file_path='sma_all_results.txt', min_trades=MIN_TRADES, max_trades=MAX_TRADES,\n",
    "                            min_elements_per_cluster=MIN_ELEMENTS_PER_CLUSTER):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering analysis on SMA optimization results\n",
    "    \"\"\"\n",
    "    print(f\"\\n----- HIERARCHICAL CLUSTER ANALYSIS -----\")\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    X = df[['short_SMA', 'long_SMA', 'sharpe_ratio', 'trades']].values\n",
    "\n",
    "    X_filtered = X[(X[:, 0] < X[:, 1]) &\n",
    "                (X[:, 3] >= min_trades) &\n",
    "                (X[:, 3] <= max_trades)]\n",
    "\n",
    "    if len(X_filtered) == 0:\n",
    "        print(f\"No data points meet the criteria after filtering!\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    print(f\"Filtered data to {len(X_filtered)} points with {min_trades}-{max_trades} trades\")\n",
    "\n",
    "    short_sma_values = X_filtered[:, 0]\n",
    "    long_sma_values = X_filtered[:, 1]\n",
    "    sharpe_values = X_filtered[:, 2]\n",
    "    trades_values = X_filtered[:, 3]\n",
    "\n",
    "    print(f\"Short SMA range: {short_sma_values.min()} to {short_sma_values.max()}\")\n",
    "    print(f\"Long SMA range: {long_sma_values.min()} to {long_sma_values.max()}\")\n",
    "    print(f\"Sharpe ratio range: {sharpe_values.min():.4f} to {sharpe_values.max():.4f}\")\n",
    "    print(f\"Trades range: {trades_values.min()} to {trades_values.max()}\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_filtered)\n",
    "\n",
    "    print(\"\\nScaled data information:\")\n",
    "    scaled_short = X_scaled[:, 0]\n",
    "    scaled_long = X_scaled[:, 1]\n",
    "    scaled_sharpe = X_scaled[:, 2]\n",
    "    scaled_trades = X_scaled[:, 3]\n",
    "\n",
    "    print(f\"Scaled Short SMA range: {scaled_short.min():.4f} to {scaled_short.max():.4f}\")\n",
    "    print(f\"Scaled Long SMA range: {scaled_long.min():.4f} to {scaled_long.max():.4f}\")\n",
    "    print(f\"Scaled Sharpe ratio range: {scaled_sharpe.min():.4f} to {scaled_sharpe.max():.4f}\")\n",
    "    print(f\"Scaled Trades range: {scaled_trades.min():.4f} to {scaled_trades.max():.4f}\")\n",
    "\n",
    "    print(\"\\nCreating dendrogram to visualize hierarchical structure...\")\n",
    "    linkage_method = 'ward'\n",
    "    Z = create_dendrogram(X_scaled, method=linkage_method, figsize=(12, 8))\n",
    "\n",
    "    print(f\"Using default number of clusters based on dendrogram: {DEFAULT_NUM_CLUSTERS}\")\n",
    "    k = DEFAULT_NUM_CLUSTERS\n",
    "\n",
    "    print(f\"Performing hierarchical clustering with {k} clusters using {linkage_method} linkage...\")\n",
    "    hierarchical = AgglomerativeClustering(\n",
    "        n_clusters=k,\n",
    "        linkage=linkage_method\n",
    "    )\n",
    "\n",
    "    labels = hierarchical.fit_predict(X_scaled)\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    cluster_sizes = dict(zip(unique_labels, counts))\n",
    "\n",
    "    print(\"\\nCluster sizes:\")\n",
    "    for cluster_id, size in cluster_sizes.items():\n",
    "        print(f\"Cluster {cluster_id}: {size} elements\")\n",
    "\n",
    "    valid_clusters = {i for i, count in cluster_sizes.items() if count >= min_elements_per_cluster}\n",
    "    if not valid_clusters:\n",
    "        print(f\"No clusters have at least {min_elements_per_cluster} elements! Reducing threshold to 1...\")\n",
    "        valid_clusters = set(unique_labels)\n",
    "    else:\n",
    "        print(f\"Using {len(valid_clusters)} clusters with at least {min_elements_per_cluster} elements\")\n",
    "\n",
    "    filtered_indices = np.array([i in valid_clusters for i in labels])\n",
    "    X_valid = X_filtered[filtered_indices]\n",
    "    labels_valid = labels[filtered_indices]\n",
    "\n",
    "    print(\"Computing medoids for each cluster...\")\n",
    "    medoids = compute_hierarchical_medoids(X_valid, labels_valid, valid_clusters)\n",
    "\n",
    "    max_sharpe_idx = np.argmax(df['sharpe_ratio'].values)\n",
    "    max_sharpe_point = df.iloc[max_sharpe_idx][['short_SMA', 'long_SMA', 'sharpe_ratio', 'trades']].values\n",
    "\n",
    "    medoids_sorted = sorted(medoids, key=lambda x: float(x[2]), reverse=True)\n",
    "    top_medoids = medoids_sorted[:5]\n",
    "\n",
    "    print(\"\\nDEBUG - SORTED MEDOIDS BY SHARPE RATIO:\")\n",
    "    for idx, medoid in enumerate(medoids_sorted[:5], 1):\n",
    "        print(f\"Sorted Medoid {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "            f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "    print(\"\\n----- HIERARCHICAL CLUSTERING RESULTS -----\")\n",
    "    print(f\"Max Sharpe point: Short SMA={int(max_sharpe_point[0])}, Long SMA={int(max_sharpe_point[1])}, \"\n",
    "        f\"Sharpe={max_sharpe_point[2]:.4f}, Trades={int(max_sharpe_point[3])}\")\n",
    "\n",
    "    print(\"\\nTop 5 Medoids (by Sharpe ratio):\")\n",
    "    for idx, medoid in enumerate(top_medoids, 1):\n",
    "        print(f\"Top {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "            f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "    # Create visualization with clustering results\n",
    "    create_hierarchical_cluster_visualization(X_filtered, medoids, top_medoids, max_sharpe_point, labels, analysis_type=\"hierarchical\")\n",
    "\n",
    "    clustering_results = []\n",
    "    for idx, medoid in enumerate(medoids_sorted):\n",
    "        clustering_results.append({\n",
    "            'Rank': idx + 1,\n",
    "            'Short_SMA': int(medoid[0]),\n",
    "            'Long_SMA': int(medoid[1]),\n",
    "            'Sharpe': medoid[2],\n",
    "            'Trades': int(medoid[3])\n",
    "        })\n",
    "    \n",
    "\n",
    "    return X_filtered, medoids, top_medoids, max_sharpe_point, labels\n",
    "\n",
    "def create_hierarchical_cluster_visualization(X, medoids, top_medoids, max_sharpe_point, labels, analysis_type=\"hierarchical\"):\n",
    "    \"\"\"Create visualization of hierarchical clustering results with heatmap plot\"\"\"\n",
    "    print(\"Creating hierarchical cluster visualization...\")\n",
    "    \n",
    "    # First filter by valid clusters\n",
    "    valid_cluster_ids = set([labels[np.where((X[:, 0] == medoid[0]) & (X[:, 1] == medoid[1]))[0][0]] for medoid in medoids])\n",
    "    print(f\"Using {len(valid_cluster_ids)} valid clusters with medoids\")\n",
    "    \n",
    "    # Filter by valid clusters\n",
    "    valid_indices = np.array([label in valid_cluster_ids for label in labels])\n",
    "    X_valid = X[valid_indices]\n",
    "    labels_valid = labels[valid_indices]\n",
    "    \n",
    "    # Additional filtering: ensure short_SMA < long_SMA and trades within range\n",
    "    X_filtered = X_valid[\n",
    "        (X_valid[:, 0] < X_valid[:, 1]) &  # short_SMA < long_SMA\n",
    "        (X_valid[:, 3] >= MIN_TRADES) &     # trades >= min_trades\n",
    "        (X_valid[:, 3] <= MAX_TRADES)       # trades <= max_trades\n",
    "    ]\n",
    "    \n",
    "    print(f\"Filtering to {len(X_filtered)} points from valid clusters\")\n",
    "    \n",
    "    # Create DataFrame for heatmap\n",
    "    filtered_df = pd.DataFrame(X_filtered, columns=['short_SMA', 'long_SMA', 'sharpe_ratio', 'trades'])\n",
    "    \n",
    "    # Load the full dataset to create complete grid\n",
    "    full_data = pd.read_csv('sma_all_results.txt')\n",
    "    \n",
    "    # Create a complete pivot table with all possible combinations\n",
    "    heatmap_data = pd.pivot_table(\n",
    "        full_data,\n",
    "        index='long_SMA',\n",
    "        columns='short_SMA',\n",
    "        values='sharpe_ratio',\n",
    "        fill_value=np.nan\n",
    "    )\n",
    "    \n",
    "    # Now overlay our filtered data\n",
    "    filtered_pivot = pd.pivot_table(\n",
    "        filtered_df,\n",
    "        index='long_SMA',\n",
    "        columns='short_SMA',\n",
    "        values='sharpe_ratio',\n",
    "        fill_value=np.nan\n",
    "    )\n",
    "    \n",
    "    # Update heatmap_data with filtered values\n",
    "    for long_sma in filtered_pivot.index:\n",
    "        for short_sma in filtered_pivot.columns:\n",
    "            if not np.isnan(filtered_pivot.loc[long_sma, short_sma]):\n",
    "                heatmap_data.loc[long_sma, short_sma] = filtered_pivot.loc[long_sma, short_sma]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create mask for invalid combinations and NaN values\n",
    "    mask = np.zeros_like(heatmap_data, dtype=bool)\n",
    "    for i, long_sma in enumerate(heatmap_data.index):\n",
    "        for j, short_sma in enumerate(heatmap_data.columns):\n",
    "            # Only mask if short_SMA >= long_SMA (invalid combination)\n",
    "            if short_sma >= long_sma:\n",
    "                mask[i, j] = True\n",
    "\n",
    "    # Plot heatmap\n",
    "    ax = sns.heatmap(\n",
    "        heatmap_data,\n",
    "        mask=mask,\n",
    "        cmap='coolwarm',\n",
    "        annot=False,\n",
    "        fmt='.4f',\n",
    "        linewidths=0,\n",
    "        cbar_kws={'label': 'Sharpe Ratio'}\n",
    "    )\n",
    "    \n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Plot points on the heatmap\n",
    "    for medoid in medoids:\n",
    "        try:\n",
    "            x_pos = np.where(heatmap_data.columns == medoid[0])[0][0] + 0.5\n",
    "            y_pos = np.where(heatmap_data.index == medoid[1])[0][0] + 0.5\n",
    "            plt.scatter(x_pos, y_pos, marker='s', color='black', s=75, zorder=4)\n",
    "        except IndexError:\n",
    "            print(f\"Warning: Medoid at ({medoid[0]}, {medoid[1]}) not found in heatmap coordinates\")\n",
    "\n",
    "    for medoid in top_medoids:\n",
    "        try:\n",
    "            x_pos = np.where(heatmap_data.columns == medoid[0])[0][0] + 0.5\n",
    "            y_pos = np.where(heatmap_data.index == medoid[1])[0][0] + 0.5\n",
    "            plt.scatter(x_pos, y_pos, marker='D', color='purple', s=100, zorder=5)\n",
    "        except IndexError:\n",
    "            print(f\"Warning: Top medoid at ({medoid[0]}, {medoid[1]}) not found in heatmap coordinates\")\n",
    "\n",
    "    try:\n",
    "        best_x_pos = np.where(heatmap_data.columns == max_sharpe_point[0])[0][0] + 0.5\n",
    "        best_y_pos = np.where(heatmap_data.index == max_sharpe_point[1])[0][0] + 0.5\n",
    "        plt.scatter(best_x_pos, best_y_pos, marker='*', color='lime', s=200,\n",
    "                edgecolor='black', zorder=5)\n",
    "    except IndexError:\n",
    "        print(f\"Warning: Max Sharpe point at ({max_sharpe_point[0]}, {max_sharpe_point[1]}) not found in heatmap coordinates\")\n",
    "\n",
    "    # Create custom legend\n",
    "    max_sharpe_handle = mlines.Line2D([], [], color='lime', marker='*', linestyle='None',\n",
    "                                    markersize=15, markeredgecolor='black', label='Max Sharpe')\n",
    "    cluster_handle = mlines.Line2D([], [], color='black', marker='s', linestyle='None',\n",
    "                                markersize=10, label='Clusters')\n",
    "    top_cluster_handle = mlines.Line2D([], [], color='purple', marker='D', linestyle='None',\n",
    "                                    markersize=10, label='Top 5 Clusters')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(handles=[max_sharpe_handle, cluster_handle, top_cluster_handle],\n",
    "            loc='lower right')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title('Hierarchical Clustering Analysis (Sharpe Ratio)', fontsize=14)\n",
    "    plt.xlabel('Short SMA (days)', fontsize=12)\n",
    "    plt.ylabel('Long SMA (days)', fontsize=12)\n",
    "\n",
    "    # Rotate tick labels\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def save_to_excel(symbol, win_percentage, medoids, best_short_sma, best_long_sma, is_kmeans=True):\n",
    "    \"\"\"Save clustering results to Excel file\"\"\"\n",
    "    try:\n",
    "        # Path to the Excel file\n",
    "        excel_file = r\"C:\\Users\\Admin\\Documents\\darbas\\Results.xlsx\"\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(excel_file):\n",
    "            print(f\"Excel file not found at: {excel_file}\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Updating Excel file with {'K-means' if is_kmeans else 'Hierarchical'} results for {symbol}...\")\n",
    "        \n",
    "        # Load the workbook\n",
    "        wb = openpyxl.load_workbook(excel_file)\n",
    "        \n",
    "        # Get the active sheet\n",
    "        sheet = wb.active\n",
    "        \n",
    "        # Find the row with the ticker symbol or the first empty row\n",
    "        row = 3  # Start from row 3 (assuming rows 1-2 have headers)\n",
    "        ticker_row = None\n",
    "        \n",
    "        while True:\n",
    "            cell_value = sheet.cell(row=row, column=1).value\n",
    "            if cell_value == symbol:\n",
    "                # Found the ticker symbol\n",
    "                ticker_row = row\n",
    "                break\n",
    "            elif cell_value is None:\n",
    "                # Found an empty row\n",
    "                ticker_row = row\n",
    "                # Write the ticker symbol in column A\n",
    "                sheet.cell(row=ticker_row, column=1).value = symbol\n",
    "                break\n",
    "            row += 1\n",
    "        \n",
    "        # Round the win percentage to one decimal place\n",
    "        rounded_win_percentage = round(win_percentage, 1)\n",
    "        \n",
    "        # Write the win percentage in column B (K-means) or C (Hierarchical)\n",
    "        column = 2 if is_kmeans else 3\n",
    "        sheet.cell(row=ticker_row, column=column).value = rounded_win_percentage\n",
    "        \n",
    "        # Write the medoid parameters in the respective cluster columns\n",
    "        # K-means: E=5, F=6, G=7 for Cluster1, Cluster2, Cluster3\n",
    "        # Hierarchical: I=9, J=10, K=11 for Cluster1, Cluster2, Cluster3\n",
    "        start_column = 5 if is_kmeans else 9\n",
    "        \n",
    "        for i, medoid in enumerate(medoids):\n",
    "            if i >= 3:  # Only use up to 3 clusters\n",
    "                break\n",
    "                \n",
    "            # Calculate column index\n",
    "            column_idx = start_column + i\n",
    "            \n",
    "            # Format as \"short/long\" (e.g., \"5/20\")\n",
    "            param_value = f\"{int(medoid[0])}/{int(medoid[1])}\"\n",
    "            \n",
    "            # Write to Excel\n",
    "            sheet.cell(row=ticker_row, column=column_idx).value = param_value\n",
    "        \n",
    "        # Write the best Sharpe parameters in column M (13)\n",
    "        best_sharpe_params = f\"{best_short_sma}/{best_long_sma}\"\n",
    "        sheet.cell(row=ticker_row, column=13).value = best_sharpe_params\n",
    "        \n",
    "        # Save the workbook\n",
    "        wb.save(excel_file)\n",
    "        \n",
    "        print(f\"Excel file updated successfully. Added {symbol} with {'K-means' if is_kmeans else 'Hierarchical'} win rate {rounded_win_percentage}% in row {ticker_row}\")\n",
    "        print(f\"Added best Sharpe parameters {best_sharpe_params} in column M\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error updating Excel file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    import importlib\n",
    "\n",
    "    def get_slippage_from_excel(symbol, data_dir):\n",
    "        excel_path = os.path.join(data_dir, \"sessions_slippages.xlsx\")\n",
    "        \n",
    "        if not os.path.exists(excel_path):\n",
    "            raise FileNotFoundError(f\"Slippage Excel file not found at {excel_path}\")\n",
    "        \n",
    "        lookup_symbol = symbol.replace('=F', '')\n",
    "        df = pd.read_excel(excel_path)\n",
    "        \n",
    "        if df.shape[1] < 4:\n",
    "            raise ValueError(f\"Excel file has fewer than 4 columns: {df.columns.tolist()}\")\n",
    "            \n",
    "        df['SymbolUpper'] = df.iloc[:, 1].astype(str).str.upper()\n",
    "        lookup_symbol_upper = lookup_symbol.upper()\n",
    "        \n",
    "        matching_rows = df[df['SymbolUpper'] == lookup_symbol_upper]\n",
    "        \n",
    "        if matching_rows.empty:\n",
    "            raise ValueError(f\"Symbol '{lookup_symbol}' not found in column B of Excel file\")\n",
    "            \n",
    "        slippage_value = matching_rows.iloc[0, 3]\n",
    "        \n",
    "        if pd.isna(slippage_value) or not isinstance(slippage_value, (int, float)):\n",
    "            raise ValueError(f\"Invalid slippage value for symbol '{lookup_symbol}': {slippage_value}\")\n",
    "            \n",
    "        print(f\"Found slippage for {lookup_symbol} in column D: {slippage_value}\")\n",
    "        return slippage_value\n",
    "\n",
    "    # List of all underlyings to process\n",
    "    underlyings = [\n",
    "        \"AD\", \"BO\", \"BP\", \"BRN\", \"C\", \"CC\", \"CD\", \"CL\", \"CT\", \"DX\", \n",
    "        \"EC\", \"EMD\", \"ES\", \"FC\", \"FDAX\", \"FESX\", \"FGBL\", \"FGBM\", \"FGBX\", \n",
    "        \"FV\", \"GC\", \"HG\", \"HO\", \"JY\", \"KC\", \"KW\", \"LC\", \"LH\", \"LJ\", \n",
    "        \"LZ\", \"MME\", \"MP1\", \"NE1\", \"NG\", \"NK\", \"NQ\", \"PA\", \"PL\", \n",
    "        \"RB\", \"RTY\", \"S\", \"SB\", \"SF\", \"SI\", \"SM\", \"TEN\", \"TY\", \n",
    "        \"TU\", \"UB\", \"ULS\", \"US\", \"VX\", \"W\", \"WBS\"\n",
    "    ]\n",
    "\n",
    "    # Process each underlying\n",
    "    for current_ticker in underlyings:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing {current_ticker}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "        try:\n",
    "            # Override the TICKER from input.py\n",
    "            input.TICKER = current_ticker\n",
    "            TICKER = current_ticker  # Update local TICKER variable\n",
    "\n",
    "            # Start overall execution timer\n",
    "            overall_start_time = time.time()\n",
    "            \n",
    "            # Setup paths using relative directories\n",
    "            WORKING_DIR = \".\"\n",
    "            DATA_DIR = os.path.join(WORKING_DIR, \"data\")\n",
    "\n",
    "            # Define SYMBOL based on TICKER\n",
    "            SYMBOL = TICKER.replace('=F', '')\n",
    "\n",
    "            # Load the futures data file\n",
    "            print(\"Loading data file...\")\n",
    "            load_start_time = time.time()\n",
    "            \n",
    "            data_files = glob.glob(os.path.join(DATA_DIR, f\"*@{SYMBOL}*.dat\"))\n",
    "            if not data_files:\n",
    "                data_files = glob.glob(os.path.join(DATA_DIR, f\"*_{SYMBOL}_*.dat\"))\n",
    "            \n",
    "            if not data_files:\n",
    "                raise FileNotFoundError(f\"No data file found for {SYMBOL} in {DATA_DIR}\")\n",
    "            \n",
    "            all_data = read_ts_ohlcv_dat(data_files[0])\n",
    "            load_end_time = time.time()\n",
    "            load_time = load_end_time - load_start_time\n",
    "            print(f\"Data loaded successfully in {load_time:.2f} seconds! Number of items: {len(all_data)}\")\n",
    "            \n",
    "            # Extract metadata and OHLCV data\n",
    "            data_obj = all_data[0]\n",
    "            tick_size = data_obj.big_point_value * data_obj.tick_size\n",
    "            big_point_value = data_obj.big_point_value\n",
    "            ohlc_data = data_obj.data.copy()\n",
    "\n",
    "            # Fetch slippage value from Excel\n",
    "            slippage_value = get_slippage_from_excel(TICKER, DATA_DIR)\n",
    "            slippage = slippage_value\n",
    "            print(f\"Using slippage from Excel column D: {slippage}\")\n",
    "\n",
    "            # Save parameters\n",
    "            parameters = {\n",
    "                \"big_point_value\": big_point_value,\n",
    "                \"slippage\": slippage,\n",
    "                \"capital\": TRADING_CAPITAL,\n",
    "                \"atr_period\": ATR_PERIOD\n",
    "            }\n",
    "            with open(\"parameters.json\", \"w\") as file:\n",
    "                json.dump(parameters, file)\n",
    "            \n",
    "            # Print information about the data\n",
    "            print(f\"\\nSymbol: {data_obj.symbol}\")\n",
    "            print(f\"Description: {data_obj.description}\")\n",
    "            print(f\"Exchange: {data_obj.exchange}\")\n",
    "            print(f\"Interval: {data_obj.interval_type} {data_obj.interval_span}\")\n",
    "            print(f\"Tick size: {tick_size}\")\n",
    "            print(f\"Big point value: {big_point_value}\")\n",
    "            print(f\"Data shape: {ohlc_data.shape}\")\n",
    "            print(f\"Date range: {ohlc_data['datetime'].min()} to {ohlc_data['datetime'].max()}\")\n",
    "            \n",
    "            # Convert the OHLCV data to the expected format\n",
    "            data = ohlc_data.rename(columns={\n",
    "                'datetime': 'Date',\n",
    "                'open': 'Open',\n",
    "                'high': 'High',\n",
    "                'low': 'Low',\n",
    "                'close': 'Close',\n",
    "                'volume': 'Volume'\n",
    "            })\n",
    "            \n",
    "            data.set_index('Date', inplace=True)\n",
    "            \n",
    "            # Add warm-up period for SMA calculation\n",
    "            original_start_idx = None\n",
    "            \n",
    "            # Filter data to match the date range\n",
    "            if START_DATE and END_DATE:\n",
    "                warm_up_days = SMA_MAX + ATR_PERIOD + 50\n",
    "                \n",
    "                start_date = pd.to_datetime(START_DATE)\n",
    "                end_date = pd.to_datetime(END_DATE)\n",
    "                \n",
    "                adjusted_start = start_date - pd.Timedelta(days=warm_up_days)\n",
    "                \n",
    "                data = data[(data.index >= adjusted_start) & \n",
    "                            (data.index <= end_date)]\n",
    "                \n",
    "                if data.empty:\n",
    "                    raise ValueError(f\"No data available for the specified date range: {START_DATE} to {END_DATE}\")\n",
    "                    \n",
    "                original_start_idx = data.index.get_indexer([start_date], method='nearest')[0]\n",
    "                \n",
    "                print(f\"Loaded extended data with {warm_up_days} days warm-up period\")\n",
    "                print(f\"Original date range: {START_DATE} to {END_DATE}\")\n",
    "                print(f\"Adjusted date range: {adjusted_start.strftime('%Y-%m-%d')} to {END_DATE}\")\n",
    "                print(f\"Original start index: {original_start_idx}\")\n",
    "            \n",
    "            # Define the range of SMA periods to test\n",
    "            sma_range = range(SMA_MIN, SMA_MAX + 1, SMA_STEP)\n",
    "            \n",
    "            print(f\"Optimizing SMA parameters using range from {SMA_MIN} to {SMA_MAX} with step {SMA_STEP}...\")\n",
    "            print(f\"Trading with big point value from data: {big_point_value}\")\n",
    "            print(f\"Using capital allocation: ${TRADING_CAPITAL:,} with ATR period: {ATR_PERIOD}\")\n",
    "            \n",
    "            # Initialize the ATR-based strategy\n",
    "            strategy = SMAStrategy(\n",
    "                short_sma=0,\n",
    "                long_sma=0,\n",
    "                big_point_value=big_point_value,\n",
    "                slippage=slippage,\n",
    "                capital=TRADING_CAPITAL,\n",
    "                atr_period=ATR_PERIOD\n",
    "            )\n",
    "            \n",
    "            # Start optimization process\n",
    "            print(\"\\nStarting optimization process...\")\n",
    "            optimization_start_time = time.time()\n",
    "            \n",
    "            best_sma, best_sharpe, best_trades, all_results = strategy.optimize(\n",
    "                data.copy(),\n",
    "                sma_range,\n",
    "                train_test_split=TRAIN_TEST_SPLIT,\n",
    "                results_file='sma_all_results.txt',\n",
    "                warm_up_idx=original_start_idx\n",
    "            )\n",
    "            \n",
    "            optimization_end_time = time.time()\n",
    "            optimization_time = optimization_end_time - optimization_start_time\n",
    "            print(f\"\\nOptimization completed in {optimization_time:.2f} seconds ({optimization_time/60:.2f} minutes)\")\n",
    "            \n",
    "            print(f\"Optimal SMA parameters: Short = {best_sma[0]} days, Long = {best_sma[1]} days\")\n",
    "            print(f\"In-sample Sharpe ratio = {best_sharpe:.4f}\")\n",
    "            print(f\"Number of trades with optimal parameters = {best_trades}\")\n",
    "            print(f\"Optimization results saved to 'sma_all_results.txt' for further analysis\")\n",
    "\n",
    "            # Apply the best strategy parameters\n",
    "            print(\"\\nApplying best strategy parameters...\")\n",
    "            strategy.short_sma = best_sma[0]\n",
    "            strategy.long_sma = best_sma[1]\n",
    "            \n",
    "            data = strategy.apply_strategy(data.copy())\n",
    "            data_for_evaluation = data.copy()\n",
    "            \n",
    "            if original_start_idx is not None:\n",
    "                print(\"Trimming warm-up period for final evaluation and visualization...\")\n",
    "                data_for_evaluation = data.iloc[original_start_idx:]\n",
    "                print(f\"Original data length: {len(data)}, Evaluation data length: {len(data_for_evaluation)}\")\n",
    "            else:\n",
    "                data_for_evaluation = data\n",
    "            \n",
    "            # Create visualization plots\n",
    "            plt.figure(figsize=(14, 16))\n",
    "            \n",
    "            # Plot price and SMAs\n",
    "            plt.subplot(3, 1, 1)\n",
    "            plt.plot(data_for_evaluation.index, data_for_evaluation['Close'], label=f'{data_obj.symbol} Price', color='blue')\n",
    "            plt.plot(data_for_evaluation.index, data_for_evaluation['SMA_Short_Strategy'], label=f'{best_sma[0]}-day SMA', color='orange')\n",
    "            plt.plot(data_for_evaluation.index, data_for_evaluation['SMA_Long_Strategy'], label=f'{best_sma[1]}-day SMA', color='red')\n",
    "            \n",
    "            long_entries = (data_for_evaluation['Position_Dir_Strategy'] == 1) & data_for_evaluation['Position_Change_Strategy']\n",
    "            short_entries = (data_for_evaluation['Position_Dir_Strategy'] == -1) & data_for_evaluation['Position_Change_Strategy']\n",
    "            \n",
    "            plt.scatter(data_for_evaluation.index[long_entries], data_for_evaluation.loc[long_entries, 'Close'], \n",
    "                        color='green', marker='^', s=50, label='Long Entry')\n",
    "            plt.scatter(data_for_evaluation.index[short_entries], data_for_evaluation.loc[short_entries, 'Close'], \n",
    "                        color='red', marker='v', s=50, label='Short Entry')\n",
    "            \n",
    "            plt.legend()\n",
    "            plt.title(f'{data_obj.symbol} with Optimized SMA Strategy ({best_sma[0]}, {best_sma[1]})')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot position size and ATR\n",
    "            ax1 = plt.subplot(3, 1, 2)\n",
    "            ax2 = ax1.twinx()\n",
    "            \n",
    "            ax1.plot(data_for_evaluation.index, data_for_evaluation['Position_Size_Strategy'], \n",
    "                     label='Position Size (# Contracts)', color='purple')\n",
    "            ax1.set_ylabel('Position Size (# Contracts)', color='purple')\n",
    "            ax1.tick_params(axis='y', colors='purple')\n",
    "            \n",
    "            ax2.plot(data_for_evaluation.index, data_for_evaluation['ATR_Strategy'], \n",
    "                     label=f'ATR ({ATR_PERIOD}-day)', color='orange')\n",
    "            ax2.set_ylabel(f'ATR ({ATR_PERIOD}-day)', color='orange')\n",
    "            ax2.tick_params(axis='y', colors='orange')\n",
    "            \n",
    "            lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "            ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "            \n",
    "            plt.title(f'Position Sizing Based on {ATR_PERIOD}-day ATR')\n",
    "            ax1.grid(True)\n",
    "            \n",
    "            # Plot performance\n",
    "            plt.subplot(3, 1, 3)\n",
    "            \n",
    "            strategy_pnl_cumulative = data_for_evaluation['Cumulative_PnL_Strategy'] - data_for_evaluation['Cumulative_PnL_Strategy'].iloc[0]\n",
    "            \n",
    "            plt.plot(data_for_evaluation.index, strategy_pnl_cumulative, \n",
    "                     label='Strategy P&L (full period)', color='green')\n",
    "            \n",
    "            split_index = int(len(data_for_evaluation) * TRAIN_TEST_SPLIT)\n",
    "            \n",
    "            plt.plot(data_for_evaluation.index[split_index:], strategy_pnl_cumulative.iloc[split_index:],\n",
    "                    label=f'Strategy P&L (last {int((1 - TRAIN_TEST_SPLIT) * 100)}% out-of-sample)', color='purple')\n",
    "            \n",
    "            plt.axvline(x=data_for_evaluation.index[split_index], color='black', linestyle='--',\n",
    "                        label=f'Train/Test Split ({int(TRAIN_TEST_SPLIT * 100)}%/{int((1 - TRAIN_TEST_SPLIT) * 100)}%)')\n",
    "            plt.axhline(y=0.0, color='gray', linestyle='-', label='Break-even')\n",
    "            \n",
    "            plt.legend()\n",
    "            plt.title('Strategy Performance (Dollar P&L)')\n",
    "            plt.ylabel('P&L ($)')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(\"\\nStarting analysis of optimization results...\")\n",
    "\n",
    "            # Run the basic analysis first to get best parameters\n",
    "            data, best_short, best_long, best_sharpe, best_trades = analyze_sma_results()\n",
    "\n",
    "            if data is None:\n",
    "                print(\"Error: Failed to load or analyze SMA results data.\")\n",
    "                continue\n",
    "\n",
    "            print(\"\\nProceeding with K-means cluster analysis...\")\n",
    "            \n",
    "            # Run the K-means cluster analysis to get medoids\n",
    "            X_filtered, medoids, top_medoids, centroids, max_sharpe_point = cluster_analysis()\n",
    "\n",
    "            if X_filtered is None or medoids is None:\n",
    "                print(\"Error: K-means cluster analysis failed.\")\n",
    "                continue\n",
    "\n",
    "            # Re-sort top_medoids to ensure they're in the right order\n",
    "            if top_medoids is not None:\n",
    "                print(\"Re-sorting K-means top medoids by Sharpe ratio...\")\n",
    "                top_medoids = sorted(top_medoids, key=lambda x: float(x[2]), reverse=True)\n",
    "                for idx, medoid in enumerate(top_medoids, 1):\n",
    "                    print(f\"Verified K-means Medoid {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "                        f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "            print(\"\\nPlotting K-means strategy performance...\")\n",
    "\n",
    "            # Plot strategy performance with K-means results\n",
    "            market_data = plot_strategy_performance(\n",
    "                best_short, best_long, top_medoids, \n",
    "                big_point_value=big_point_value,\n",
    "                slippage=slippage,\n",
    "                capital=TRADING_CAPITAL,\n",
    "                atr_period=ATR_PERIOD,\n",
    "                analysis_type=\"kmeans\"\n",
    "            )\n",
    "            \n",
    "            # Run the bimonthly out-of-sample comparison for K-means\n",
    "            if top_medoids and len(top_medoids) > 0:\n",
    "                print(\"\\nPerforming bimonthly out-of-sample comparison with K-means clustering...\")\n",
    "                bimonthly_sharpe_df = bimonthly_out_of_sample_comparison(\n",
    "                    market_data, \n",
    "                    best_short, \n",
    "                    best_long, \n",
    "                    top_medoids,\n",
    "                    big_point_value=big_point_value,\n",
    "                    slippage=slippage,\n",
    "                    capital=TRADING_CAPITAL,\n",
    "                    atr_period=ATR_PERIOD,\n",
    "                    analysis_type=\"kmeans\"\n",
    "                )\n",
    "                \n",
    "                # Calculate win percentage from bimonthly comparison\n",
    "                if bimonthly_sharpe_df is not None:\n",
    "                    total_periods = len(bimonthly_sharpe_df)\n",
    "                    if total_periods > 0:\n",
    "                        rounded_wins = sum(bimonthly_sharpe_df['Avg_Medoid_sharpe_rounded'] > bimonthly_sharpe_df['Best_sharpe_rounded'])\n",
    "                        win_percentage = (rounded_wins / total_periods) * 100\n",
    "                        \n",
    "                        # Save K-means results to Excel\n",
    "                        save_to_excel(SYMBOL, win_percentage, top_medoids, best_short, best_long, is_kmeans=True)\n",
    "            else:\n",
    "                print(\"No K-means top medoids found. Cannot run bimonthly comparison.\")\n",
    "\n",
    "            print(\"\\nProceeding with hierarchical cluster analysis...\")\n",
    "            \n",
    "            # Run the hierarchical cluster analysis to get medoids\n",
    "            X_filtered_h, medoids_h, top_medoids_h, max_sharpe_point_h, labels_h = hierarchical_cluster_analysis()\n",
    "\n",
    "            if X_filtered_h is None or medoids_h is None:\n",
    "                print(\"Error: Hierarchical cluster analysis failed.\")\n",
    "                continue\n",
    "\n",
    "            # Re-sort top_medoids to ensure they're in the right order\n",
    "            if top_medoids_h is not None:\n",
    "                print(\"Re-sorting hierarchical top medoids by Sharpe ratio...\")\n",
    "                top_medoids_h = sorted(top_medoids_h, key=lambda x: float(x[2]), reverse=True)\n",
    "                for idx, medoid in enumerate(top_medoids_h, 1):\n",
    "                    print(f\"Verified Hierarchical Medoid {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "                        f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "            print(\"\\nPlotting hierarchical strategy performance...\")\n",
    "\n",
    "            # Plot strategy performance with hierarchical results\n",
    "            market_data_h = plot_strategy_performance(\n",
    "                best_short, best_long, top_medoids_h, \n",
    "                big_point_value=big_point_value,\n",
    "                slippage=slippage,\n",
    "                capital=TRADING_CAPITAL,\n",
    "                atr_period=ATR_PERIOD,\n",
    "                analysis_type=\"hierarchical\"\n",
    "            )\n",
    "            \n",
    "            # Run the bimonthly out-of-sample comparison for hierarchical\n",
    "            if top_medoids_h and len(top_medoids_h) > 0:\n",
    "                print(\"\\nPerforming bimonthly out-of-sample comparison with hierarchical clustering...\")\n",
    "                bimonthly_sharpe_df_h = bimonthly_out_of_sample_comparison(\n",
    "                    market_data_h, \n",
    "                    best_short, \n",
    "                    best_long, \n",
    "                    top_medoids_h,\n",
    "                    big_point_value=big_point_value,\n",
    "                    slippage=slippage,\n",
    "                    capital=TRADING_CAPITAL,\n",
    "                    atr_period=ATR_PERIOD,\n",
    "                    analysis_type=\"hierarchical\"\n",
    "                )\n",
    "                \n",
    "                # Calculate win percentage from bimonthly comparison\n",
    "                if bimonthly_sharpe_df_h is not None:\n",
    "                    total_periods = len(bimonthly_sharpe_df_h)\n",
    "                    if total_periods > 0:\n",
    "                        rounded_wins = sum(bimonthly_sharpe_df_h['Avg_Medoid_sharpe_rounded'] > bimonthly_sharpe_df_h['Best_sharpe_rounded'])\n",
    "                        win_percentage = (rounded_wins / total_periods) * 100\n",
    "                        \n",
    "                        # Save hierarchical results to Excel\n",
    "                        save_to_excel(SYMBOL, win_percentage, top_medoids_h, best_short, best_long, is_kmeans=False)\n",
    "            else:\n",
    "                print(\"No hierarchical top medoids found. Cannot run bimonthly comparison.\")\n",
    "\n",
    "            print(f\"\\nCompleted processing {current_ticker}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {current_ticker}: {str(e)}\")\n",
    "            print(\"Continuing with next underlying...\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\nAll underlyings processed!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
