{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6e6a12",
   "metadata": {},
   "source": [
    "# SMA Strategy Analysis System\n",
    "This notebook contains all the code organized in cells for running the complete SMA strategy analysis.\n",
    "Run each cell in sequence to perform the full analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "underlyings = [\n",
    "    \"AD\", \"BO\", \"BP\", \"BRN\", \"C\", \"CC\", \"CD\", \"CL\", \"CT\", \"DX\", \n",
    "    \"EC\", \"EMD\", \"ES\", \"FC\", \"FDAX\", \"FESX\", \"FGBL\", \"FGBM\", \"FGBX\", \n",
    "    \"FV\", \"GC\", \"HG\", \"HO\", \"JY\", \"KC\", \"KW\", \"LC\", \"LH\", \"LJ\", \n",
    "    \"LZ\", \"MME\", \"MP1\", \"NE1\", \"NG\", \"NK\", \"NQ\", \"PA\", \"PL\", \n",
    "    \"RB\", \"RTY\", \"S\", \"SB\", \"SF\", \"SI\", \"SM\", \"TEN\", \"TY\", \n",
    "    \"TU\", \"UB\", \"ULS\", \"US\", \"VX\", \"W\", \"WBS\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88331633",
   "metadata": {},
   "source": [
    "## 1. Configuration (input.py)\n",
    "Paste the contents of input.py here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe535a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Configuration file for SMA optimization strategy using genetic algorithm\n",
    "# Ticker symbol for the asset will be set dynamically\n",
    "TICKER = None  # This will be set before each run\n",
    "\n",
    "# Date range for analysis\n",
    "START_DATE = '2014-01-01'\n",
    "END_DATE = '2025-01-01'\n",
    "\n",
    "# SMA range parameters (for genetic algorithm bounds)\n",
    "SMA_MIN = 10\n",
    "SMA_MAX = 300\n",
    "SMA_STEP = 5  # Kept for compatibility with original code, not used in GA\n",
    "\n",
    "# Data splitting ratio (percentage of data used for in-sample testing)\n",
    "TRAIN_TEST_SPLIT = 0.7\n",
    "\n",
    "# Results output file\n",
    "RESULTS_FILE = 'sma_all_results.txt'\n",
    "\n",
    "# ATR-based position sizing parameters\n",
    "ATR_PERIOD = 30          # Period for ATR calculation (days)\n",
    "TRADING_CAPITAL = 6000   # Capital allocation for position sizing\n",
    "\n",
    "# Genetic Algorithm parameters\n",
    "POPULATION_SIZE = 2000    # Number of individuals in population\n",
    "NUM_GENERATIONS = 20     # Number of generations to evolve\n",
    "HALL_OF_FAME_SIZE = 100000   #didelis skaicius kad visus paimtu\n",
    "CROSSOVER_PROB = 0.5     # Probability of crossover\n",
    "MUTATION_PROB = 0.3      # Probability of mutation\n",
    "RANDOM_SEED = 42         # Random seed for reproducibility\n",
    "\n",
    "# Clustering parameters (kept for compatibility)\n",
    "MIN_TRADES = 10              # Minimum number of trades to consider in clustering\n",
    "MAX_TRADES = 2000            # Maximum number of trades to consider in clustering\n",
    "MIN_ELEMENTS_PER_CLUSTER = 20  # Minimum elements required for a valid cluster\n",
    "DEFAULT_NUM_CLUSTERS = 50     # Default number of clusters if not specified by user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24cd464",
   "metadata": {},
   "source": [
    "## 2. Data Reading Utilities (read_ts.py)\n",
    "Paste the contents of read_ts.py here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb4705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from multiprocessing import Pool\n",
    "import struct\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "class mdata:\n",
    "    tick_size = None\n",
    "    big_point_value = None\n",
    "    country = None\n",
    "    exchange = None\n",
    "    symbol = None\n",
    "    description = None\n",
    "    interval_type = None\n",
    "    interval_span = None\n",
    "    time_zone = None\n",
    "    session = None\n",
    "    data = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str((self.symbol, self.interval_type, self.interval_span, self.data.shape))\n",
    "\n",
    "def read_ts_ohlcv_dat_one(fname) -> mdata:\n",
    "    try:\n",
    "        f = open(fname, \"rb\")\n",
    "        d = read_ts_ohlcv_dat_one_stream(f)\n",
    "    finally:\n",
    "        f.close()\n",
    "    return d\n",
    "\n",
    "def read_ts_ohlcv_dat_one_stream(byte_stream) -> mdata:\n",
    "    def read_string(f):\n",
    "        sz = struct.unpack('i', f.read(4))[0]\n",
    "        s = f.read(sz).decode('ascii')\n",
    "        return s\n",
    "    d = mdata()\n",
    "    try:\n",
    "        # f = open(fname, \"rb\")\n",
    "        f = byte_stream\n",
    "        (ones, type_format) = struct.unpack('ii', f.read(8))\n",
    "        if (ones != 1111111111):\n",
    "            print(\"format not supported, must be 1111111111\")\n",
    "            return None\n",
    "        if (type_format != 3):\n",
    "            print(\"type_format not supported, must be 3\")\n",
    "            return None\n",
    "        d.tick_size = struct.unpack('d', f.read(8))[0]\n",
    "        d.big_point_value = struct.unpack('d', f.read(8))[0]\n",
    "        d.country = read_string(f)\n",
    "        d.exchange = read_string(f)\n",
    "        d.symbol = read_string(f)\n",
    "        d.description = read_string(f)\n",
    "        d.interval_type = read_string(f)\n",
    "        d.interval_span = struct.unpack('i', f.read(4))[0]\n",
    "        d.time_zone = read_string(f)\n",
    "        d.session = read_string(f)\n",
    "\n",
    "        dt = np.dtype([('date', 'f8'), ('open', 'f8'), ('high', 'f8'), ('low', 'f8'), ('close', 'f8'), ('volume', 'f8')])\n",
    "        #hist2 = np.fromfile(f, dtype=dt)\n",
    "        z = f.read()\n",
    "        #hist2 = np.frombuffer(z, dtype=dt)\n",
    "        #hist2 = pd.DataFrame.from_records(hist2)\n",
    "        data = pd.DataFrame.from_records(np.frombuffer(z, dtype=dt))\n",
    "    finally:\n",
    "        #f.close() #no need to close - -will be closed outside\n",
    "        pass\n",
    "\n",
    "    arr = ((data['date']-25569)*24*60*60).round(0).astype(np.int64)*1000000000\n",
    "    #25569 - difference between datenum('2088-02-25') - datenum('2018-02-23')\n",
    "    # .round(0) - to remove millisecond error\n",
    "    # *1000000000 - to convert to native type of nanoseconds\n",
    "    z2 = pd.to_datetime(arr)\n",
    "    data.insert(0, 'datetime', z2)\n",
    "    del data['date']\n",
    "    d.data = data\n",
    "    return d\n",
    "\n",
    "def read_ts_ohlcv_dat(fnames) -> List[mdata]:\n",
    "    r = []\n",
    "    for name in glob.glob(fnames):\n",
    "        #print('loading ', name)\n",
    "        z = read_ts_ohlcv_dat_one(name)\n",
    "        r.append(z)\n",
    "    #print('done')\n",
    "    return r\n",
    "\n",
    "class spnl:\n",
    "    data = None\n",
    "    fname = None\n",
    "    symbol = None\n",
    "    dir = None\n",
    "    strat = None\n",
    "    session = None\n",
    "    trade_count = None\n",
    "    bar_size = None\n",
    "    param = None\n",
    "    symbol_root = None\n",
    "    sidb = None\n",
    "    date = None\n",
    "    pnl = None\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "def read_ts_pnl_dat_one(fname)->spnl:\n",
    "    \"\"\"\n",
    "\n",
    "    :param fname: string with file name or bytes with hist2\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if type(fname) is bytes:\n",
    "        f = fname\n",
    "        (ones, type_format) = struct.unpack('ii', f[:8])\n",
    "        dt = np.dtype([('date', 'f4'), ('pnl', 'f4'), ('pos', 'f4'), ('trade', 'f4')])\n",
    "        data = np.frombuffer(f[8:], dtype=dt)\n",
    "        additional_info = ['a','a','a','a','a','a']\n",
    "        sida = 'a_a_a_a_a_a'\n",
    "    elif type(fname) is str:\n",
    "        try:\n",
    "            f = open(fname, \"rb\")\n",
    "            (ones, type_format) = struct.unpack('ii', f.read(8))\n",
    "            dt = np.dtype([('date', 'f4'), ('pnl', 'f4'), ('pos', 'f4'), ('trade', 'f4')])\n",
    "            data = np.fromfile(f, dtype=dt)\n",
    "            #hist2 = pd.DataFrame.from_records(hist2)\n",
    "            additional_info = os.path.basename(fname).split('_')\n",
    "            sida = fname.split('\\\\')[-1].replace('.dat','')\n",
    "        finally:\n",
    "            f.close()\n",
    "    else:\n",
    "        f = fname\n",
    "        fname = os.path.basename(f.name)\n",
    "        (ones, type_format) = struct.unpack('ii', f.read(8))\n",
    "        dt = np.dtype([('date', 'f4'), ('pnl', 'f4'), ('pos', 'f4'), ('trade', 'f4')])\n",
    "        data = np.frombuffer(f.read(), dtype=dt)\n",
    "        additional_info = os.path.basename(fname).split('_')\n",
    "        sida = fname.split('\\\\')[-1].replace('.dat','')\n",
    "\n",
    "\n",
    "\n",
    "    #arr = np.floor(hist2['date'] - 25569).astype(np.int64) * 24 * 60 * 60 * 1000000000\n",
    "    #z2 = pd.to_datetime(arr)\n",
    "    #hist2.insert(0, 'datetime', z2)\n",
    "    #del hist2['date']\n",
    "\n",
    "\n",
    "    d = spnl()\n",
    "\n",
    "    #d.date = pd.to_datetime(np.floor(hist2['date'] - 25569).astype(np.int64) * 24 * 60 * 60 * 1000000000).values\n",
    "    #hist2['date'] =\n",
    "    d.date = np.array(np.floor(data['date'] - 25569).astype(np.int64) * 24 * 60 * 60 * 1000000000, dtype='datetime64[ns]')\n",
    "    d.pnl = data['pnl']\n",
    "\n",
    "    #d.hist2 = hist2\n",
    "    #d.date = hist2.datetime\n",
    "    #d.pnl = hist2.pnl\n",
    "    d.fname = fname\n",
    "    d.symbol = additional_info[0]\n",
    "    d.dir = additional_info[1]\n",
    "    d.strat = additional_info[2]\n",
    "    d.session = additional_info[3]\n",
    "    d.trade_count = data['trade'].max()\n",
    "    d.bar_size = additional_info[4]\n",
    "    d.param = additional_info[5]\n",
    "    d.symbol_root = additional_info[0]\n",
    "    d.sida = sida\n",
    "    k = d.sida.split('_')\n",
    "    d.sidb = '_'.join(k[0:1] + k[2:-1])\n",
    "    d.pos = data['pos']\n",
    "    d.trade = data['trade']\n",
    "\n",
    "    return d\n",
    "########################################################################################################################\n",
    "def read_ts_pnl_dat_one_with_entry_price(fname)->spnl:\n",
    "    \"\"\"\n",
    "\n",
    "    :param fname: string with file name or bytes with hist2\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if type(fname) is bytes:\n",
    "        f = fname\n",
    "        (ones, type_format) = struct.unpack('ii', f[:8])\n",
    "        dt = np.dtype([('date', 'f8'), ('pnl', 'f8'), ('pos', 'f8'), ('trade', 'f8'), ('entry_price', 'f8'), ('empty', 'f8')])\n",
    "        data = np.frombuffer(f[8:], dtype=dt)\n",
    "        additional_info = ['a','a','a','a','a','a']\n",
    "        sida = 'a_a_a_a_a_a'\n",
    "    elif type(fname) is str:\n",
    "        try:\n",
    "            f = open(fname, \"rb\")\n",
    "            (ones, type_format) = struct.unpack('ii', f.read(8))\n",
    "            dt = np.dtype([('date', 'f8'), ('pnl', 'f8'), ('pos', 'f8'), ('trade', 'f8'), ('entry_price', 'f8'), ('empty', 'f8')])\n",
    "            data = np.fromfile(f, dtype=dt)\n",
    "            #hist2 = pd.DataFrame.from_records(hist2)\n",
    "            additional_info = os.path.basename(fname).split('_')\n",
    "            sida = fname.split('\\\\')[-1].replace('.dat','')\n",
    "        finally:\n",
    "            f.close()\n",
    "    else:\n",
    "        f = fname\n",
    "        fname = os.path.basename(f.name)\n",
    "        (ones, type_format) = struct.unpack('ii', f.read(8))\n",
    "        dt = np.dtype([('date', 'f4'), ('pnl', 'f4'), ('pos', 'f4'), ('trade', 'f4')])\n",
    "        data = np.frombuffer(f.read(), dtype=dt)\n",
    "        additional_info = os.path.basename(fname).split('_')\n",
    "        sida = fname.split('\\\\')[-1].replace('.dat','')\n",
    "\n",
    "\n",
    "\n",
    "    #arr = np.floor(hist2['date'] - 25569).astype(np.int64) * 24 * 60 * 60 * 1000000000\n",
    "    #z2 = pd.to_datetime(arr)\n",
    "    #hist2.insert(0, 'datetime', z2)\n",
    "    #del hist2['date']\n",
    "\n",
    "\n",
    "    d = spnl()\n",
    "\n",
    "    #d.date = pd.to_datetime(np.floor(hist2['date'] - 25569).astype(np.int64) * 24 * 60 * 60 * 1000000000).values\n",
    "    #hist2['date'] =\n",
    "    d.date = np.array(np.floor(data['date'] - 25569).astype(np.int64) * 24 * 60 * 60 * 1000000000, dtype='datetime64[ns]')\n",
    "    d.pnl = data['pnl']\n",
    "\n",
    "    #d.hist2 = hist2\n",
    "    #d.date = hist2.datetime\n",
    "    #d.pnl = hist2.pnl\n",
    "    d.fname = fname\n",
    "    d.symbol = additional_info[0]\n",
    "    d.dir = additional_info[1]\n",
    "    d.strat = additional_info[2]\n",
    "    d.session = additional_info[3]\n",
    "    d.trade_count = data['trade'].max()\n",
    "    d.bar_size = additional_info[4]\n",
    "    d.param = additional_info[5]\n",
    "    d.symbol_root = additional_info[0]\n",
    "    d.sida = sida\n",
    "    k = d.sida.split('_')\n",
    "    d.sidb = '_'.join(k[0:1] + k[2:-1])\n",
    "    d.pos = data['pos']\n",
    "    d.trade = data['trade']\n",
    "    d.entry_price = data['entry_price']\n",
    "\n",
    "    return d\n",
    "########################################################################################################################\n",
    "def read_ts_pnl_dat(fnames):\n",
    "    print('loading ', fnames)\n",
    "    r = []\n",
    "    counter = 0\n",
    "    for name in glob.glob(fnames):\n",
    "        #print('loading ', name)\n",
    "        try:\n",
    "            z = read_ts_pnl_dat_one(name)\n",
    "            r.append(z)\n",
    "            # print('done')\n",
    "        except:\n",
    "            print(\"cannot read :\", name)\n",
    "        else:\n",
    "            counter += 1\n",
    "    print(f\"loaded files: {counter}\")\n",
    "    return r\n",
    "########################################################################################################################\n",
    "def read_ts_pnl_dat_multicore(fnames):\n",
    "    #print(f'load started from {fnames}', end =\" \")\n",
    "\n",
    "    import threading\n",
    "    import time\n",
    "\n",
    "    files = glob.glob(fnames)\n",
    "\n",
    "    #print(f', files: {len(files)}', end =\" \")\n",
    "\n",
    "    time_start = time.time()\n",
    "\n",
    "    def read_one_file(one_file, results, index):\n",
    "        results[index] = read_ts_pnl_dat_one(one_file)\n",
    "\n",
    "    thread_list = [None] * len(files)\n",
    "    results = [None] * len(files)\n",
    "    for i in range(len(files)):\n",
    "        thread_list[i] = threading.Thread(target=read_one_file, args=(files[i], results, i))\n",
    "        thread_list[i].start()\n",
    "\n",
    "    for o in thread_list:\n",
    "        o.join()\n",
    "\n",
    "    #print(f\"done. elapsed: {(time.time() - time_start):7} seconds\")\n",
    "    return results\n",
    "###############################################################\n",
    "def read_ts_pnl_dat_multicore_with_entry_price(fnames):\n",
    "    #print(f'load started from {fnames}', end =\" \")\n",
    "\n",
    "    import threading\n",
    "    import time\n",
    "\n",
    "    files = glob.glob(fnames)\n",
    "\n",
    "    #print(f', files: {len(files)}', end =\" \")\n",
    "\n",
    "    time_start = time.time()\n",
    "\n",
    "    def read_one_file(one_file, results, index):\n",
    "        results[index] = read_ts_pnl_dat_one_with_entry_price(one_file)\n",
    "\n",
    "    thread_list = [None] * len(files)\n",
    "    results = [None] * len(files)\n",
    "    for i in range(len(files)):\n",
    "        thread_list[i] = threading.Thread(target=read_one_file, args=(files[i], results, i))\n",
    "        thread_list[i].start()\n",
    "\n",
    "    for o in thread_list:\n",
    "        o.join()\n",
    "\n",
    "    #print(f\"done. elapsed: {(time.time() - time_start):7} seconds\")\n",
    "    return results\n",
    "###############################################################\n",
    "def read_ts_pnl_dat_multicore_old(fnames, number_of_threads=16):\n",
    "    \"\"\"\n",
    "    plase use this form: if __name__ == '__main__':\n",
    "    :param fnames:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('load started from ' + fnames)\n",
    "    files = glob.glob(fnames)\n",
    "    ilgis = len(files)\n",
    "    chunksize = ilgis // number_of_threads\n",
    "    agents = number_of_threads\n",
    "    pool = Pool(agents)\n",
    "    r = pool.map(read_ts_pnl_dat_one, files, chunksize)\n",
    "    pool.close()\n",
    "    print('done')\n",
    "    return r\n",
    "###############################################################\n",
    "def read_ts_pnl_csv_one(fname: str) -> pd.DataFrame:\n",
    "    a = pd.read_csv(fname, delimiter=',', index_col=0, parse_dates=True, dayfirst=False, header=None,\n",
    "                    names=['pnl', 'c', 'd', 'e'])\n",
    "    a.index = np.array(a.index, dtype='datetime64[D]')\n",
    "    a.index.name = 'date'\n",
    "    a.fname = fname\n",
    "    a.sida = fname.split('\\\\')[-1].replace('.CSV', '')\n",
    "    k = a.sida.split('_')\n",
    "    a.sidb = '_'.join(k[0:1] + k[2:-1])\n",
    "    a.symbol = k[0]\n",
    "    return a\n",
    "########################################################################################################################\n",
    "def read_ts_pnl_csv(fnames: \"\") -> List[pd.DataFrame]: # type: ignore\n",
    "    r = []\n",
    "    for name in glob.glob(fnames):\n",
    "        print('loading ', name)\n",
    "        z = read_ts_pnl_csv_one(name)\n",
    "        r.append(z)\n",
    "    print('done')\n",
    "    return r\n",
    "########################################################################################################################\n",
    "def allign_numpy(d) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    all_date = np.unique(np.concatenate([t.date for t in d]))\n",
    "    all_date = np.array(all_date, dtype='datetime64[D]')\n",
    "    all_pnl_matrix = np.zeros((len(d), len(all_date)), dtype='float32')\n",
    "    i = 0\n",
    "    for q in d:\n",
    "        idx = np.in1d(all_date, q.date, assume_unique=True)\n",
    "        all_pnl_matrix[i, idx] = q.pnl\n",
    "        i = i + 1\n",
    "\n",
    "    return all_date, all_pnl_matrix\n",
    "########################################################################################################################\n",
    "def allign_pandas(d) -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    all_date = np.unique(np.concatenate([t.date for t in d]))\n",
    "    all_date = np.array(all_date, dtype='datetime64[D]')\n",
    "    df1 = pd.DataFrame(0.0, index=all_date, columns=[a.sidb for a in d])\n",
    "    for q in d:\n",
    "        df1.iloc[np.in1d(all_date, q.date, assume_unique=True), df1.columns.get_loc(q.sidb)] = q.pnl\n",
    "\n",
    "    return all_date, df1\n",
    "########################################################################################################################\n",
    "def mavg_old_and_slow(d,p):\n",
    "    r = np.zeros(len(d))\n",
    "    r[p - 1] = d[:p].sum()\n",
    "    for i in range(p, len(d)):\n",
    "        r[i] = r[i - 1] - d[i - p] + d[i]\n",
    "    return r / p\n",
    "########################################################################################################################\n",
    "def mavg(d, period):\n",
    "    return np.concatenate([np.zeros((period-1)), np.convolve(d, np.ones((period,))/period, mode='valid')])\n",
    "########################################################################################################################\n",
    "def emavg(d : np.ndarray,period: int) -> np.ndarray:\n",
    "    if period == 1:\n",
    "        return d\n",
    "    r = np.zeros(len(d))\n",
    "    alpha = 2 / (period + 1)  # weight for ema\n",
    "    # first exponential average is a first price\n",
    "    r[0] = d[1]\n",
    "    for j in range(1, len(d)):\n",
    "        r[j] = r[j - 1] + alpha * (d[j] - r[j - 1])\n",
    "    return r\n",
    "########################################################################################################################\n",
    "def smooth(data_ : np.ndarray, period : int) -> np.ndarray:\n",
    "    #r = np.zeros(len(data_))\n",
    "    r = data = data_\n",
    "    #hist2: np.ndarray = data_\n",
    "    for i in range(0,period):\n",
    "        r[0] = data[0]\n",
    "        r[-1] = data[-1]\n",
    "        r[1:-1] = (data[:-2] + data[1:-1] + data[2:])/3\n",
    "        data = r\n",
    "    return data\n",
    "########################################################################################################################\n",
    "def remove_bad_is(data, oos_date):\n",
    "\n",
    "    if len(data) < 10000:\n",
    "        print(f\"too few hist2: {len(data)} - will not delete\")\n",
    "        return data\n",
    "\n",
    "    bad_res = []\n",
    "    for i in range(len(data)):\n",
    "        a1 = data[i]\n",
    "        idx_oos_date = np.argmax(a1.date >= oos_date)\n",
    "        if a1.pnl[:idx_oos_date].sum() <= 0:\n",
    "            bad_res.append(i)\n",
    "\n",
    "    print(f\"bad ones: {len(bad_res)} will be deleted\")\n",
    "\n",
    "    del_percent = len(bad_res) / len(data) * 100\n",
    "    if del_percent > 90:\n",
    "        print(f\"too many deletes: {del_percent}% - will not delete\")\n",
    "        return data\n",
    "\n",
    "    #delete\n",
    "    import os\n",
    "    for i in bad_res:\n",
    "        try:\n",
    "            os.remove(data[i].fname)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    #remove from list\n",
    "    a3 = np.array(data)\n",
    "    a4 = np.delete(a3, bad_res)\n",
    "    return a4\n",
    "########################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47da4654",
   "metadata": {},
   "source": [
    "## 3. SMA Strategy Implementation (SMA_Strategy.py)\n",
    "Paste the contents of SMA_Strategy.py here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from read_ts import mavg  # Add import for mavg function\n",
    "\n",
    "class SMAStrategy:\n",
    "    \"\"\"\n",
    "    Vectorized SMA (Simple Moving Average) trading strategy with ATR-based position sizing\n",
    "    \n",
    "    Features:\n",
    "    - Basic SMA crossover entry signals (long when short SMA crosses above long SMA, short when it crosses below)\n",
    "    - Pure long/short positions based on crossover direction\n",
    "    - Position size based on ATR volatility\n",
    "    - Slippage modeling for realistic execution\n",
    "    - P&L calculated in absolute dollar terms using big point value\n",
    "    - Fully vectorized implementation for efficiency\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, short_sma, long_sma, big_point_value, slippage=0, capital=6000, atr_period=50):\n",
    "        \"\"\"\n",
    "        Initialize the SMA strategy with specific parameters\n",
    "\n",
    "        Parameters:\n",
    "        short_sma (int): Short SMA period in days\n",
    "        long_sma (int): Long SMA period in days\n",
    "        big_point_value (int): Contract big point value for calculating dollar P&L\n",
    "        slippage (float): Slippage in price units added/subtracted from execution price\n",
    "        capital (float): The capital allocation for position sizing\n",
    "        atr_period (int): Period for ATR calculation for position sizing\n",
    "        \"\"\"\n",
    "        self.short_sma = short_sma\n",
    "        self.long_sma = long_sma\n",
    "        self.big_point_value = big_point_value\n",
    "        self.slippage = slippage\n",
    "        self.capital = capital\n",
    "        self.atr_period = atr_period\n",
    "\n",
    "    def calculate_atr(self, data, period=None):\n",
    "        \"\"\"\n",
    "        Calculate Average True Range (ATR)\n",
    "        \n",
    "        Parameters:\n",
    "        data: DataFrame with OHLC data\n",
    "        period: ATR calculation period, defaults to self.atr_period if None\n",
    "        \n",
    "        Returns:\n",
    "        Series: ATR values\n",
    "        \"\"\"\n",
    "        if period is None:\n",
    "            period = self.atr_period\n",
    "            \n",
    "        # Calculate True Range\n",
    "        # True High is the maximum of today's high and yesterday's close\n",
    "        high = data['High'].copy()\n",
    "        prev_close = data['Close'].shift(1)\n",
    "        true_high = pd.DataFrame({'high': high, 'prev_close': prev_close}).max(axis=1)\n",
    "        \n",
    "        # True Low is the minimum of today's low and yesterday's close\n",
    "        low = data['Low'].copy()\n",
    "        true_low = pd.DataFrame({'low': low, 'prev_close': prev_close}).min(axis=1)\n",
    "        \n",
    "        # True Range = True High - True Low\n",
    "        true_range = true_high - true_low\n",
    "        \n",
    "        # ATR is the moving average of True Range\n",
    "        atr = true_range.rolling(window=period).mean()\n",
    "        \n",
    "        return atr\n",
    "\n",
    "    def apply_strategy(self, data, strategy_name=\"Strategy\"):\n",
    "        \"\"\"\n",
    "        Apply the simple SMA crossover strategy to the price data using vectorized operations\n",
    "        with ATR-based position sizing\n",
    "        \"\"\"\n",
    "        # Create a copy of the DataFrame to avoid modifying the original\n",
    "        sim_data = data.copy()\n",
    "        \n",
    "        # Calculate SMAs using the efficient mavg function from read_ts\n",
    "        sim_data[f'SMA_Short_{strategy_name}'] = mavg(sim_data['Close'].values, self.short_sma)\n",
    "        sim_data[f'SMA_Long_{strategy_name}'] = mavg(sim_data['Close'].values, self.long_sma)\n",
    "        \n",
    "        # Calculate ATR for position sizing\n",
    "        sim_data[f'ATR_{strategy_name}'] = self.calculate_atr(sim_data, self.atr_period)\n",
    "        \n",
    "        # Calculate position size based on ATR\n",
    "        sim_data[f'Position_Size_{strategy_name}'] = np.round(\n",
    "            self.capital / (sim_data[f'ATR_{strategy_name}'] * self.big_point_value) + 0.5\n",
    "        )\n",
    "        \n",
    "        # Determine position direction\n",
    "        sim_data[f'Position_Dir_{strategy_name}'] = np.where(\n",
    "            sim_data[f'SMA_Short_{strategy_name}'] > sim_data[f'SMA_Long_{strategy_name}'], 1, -1\n",
    "        )\n",
    "        \n",
    "        # Fill NaN values at the beginning\n",
    "        sim_data[f'Position_Dir_{strategy_name}'] = sim_data[f'Position_Dir_{strategy_name}'].fillna(0)\n",
    "        \n",
    "        # Identify position changes\n",
    "        sim_data[f'Position_Change_{strategy_name}'] = sim_data[f'Position_Dir_{strategy_name}'].diff() != 0\n",
    "        \n",
    "        # Calculate P&L\n",
    "        market_pnl = sim_data['Close'].diff() * self.big_point_value\n",
    "        sim_data[f'Market_PnL_{strategy_name}'] = market_pnl\n",
    "        \n",
    "        # Strategy P&L\n",
    "        sim_data[f'Daily_PnL_{strategy_name}'] = (\n",
    "            market_pnl * \n",
    "            sim_data[f'Position_Dir_{strategy_name}'].shift(1) * \n",
    "            sim_data[f'Position_Size_{strategy_name}'].shift(1)\n",
    "        )\n",
    "        \n",
    "        # Apply slippage at position changes\n",
    "        position_changed = sim_data[f'Position_Change_{strategy_name}']\n",
    "        sim_data.loc[position_changed, f'Daily_PnL_{strategy_name}'] -= (\n",
    "            self.slippage * sim_data[f'Position_Size_{strategy_name}'][position_changed]\n",
    "        )\n",
    "        \n",
    "        # Replace NaN values in first row\n",
    "        sim_data[f'Daily_PnL_{strategy_name}'] = sim_data[f'Daily_PnL_{strategy_name}'].fillna(0)\n",
    "        \n",
    "        # Calculate cumulative P&L\n",
    "        sim_data[f'Cumulative_PnL_{strategy_name}'] = sim_data[f'Daily_PnL_{strategy_name}'].cumsum()\n",
    "        \n",
    "        return sim_data\n",
    "\n",
    "    def optimize(self, data, sma_range, train_test_split=0.7, results_file=None, warm_up_idx=None):\n",
    "        \"\"\"\n",
    "        Find the optimal SMA parameters and record all simulations using vectorized operations\n",
    "\n",
    "        Parameters:\n",
    "        data: DataFrame with market data\n",
    "        sma_range: Range of SMA periods to test\n",
    "        train_test_split: Portion of data to use for in-sample testing\n",
    "        results_file: Path to save simulation results\n",
    "        warm_up_idx: Index to trim warm-up period (if provided)\n",
    "\n",
    "        Returns:\n",
    "        best_sma_params: Tuple with (short_sma, long_sma)\n",
    "        best_sharpe: Best Sharpe ratio found\n",
    "        best_trades: Number of trades with best parameters\n",
    "        all_results: List of tuples with all simulation results\n",
    "        \"\"\"\n",
    "        # Initialize variables to track the best performance\n",
    "        best_sharpe = -np.inf  # Start with negative infinity to ensure any valid Sharpe ratio will be better\n",
    "        best_sma = (0, 0)  # Tuple to store the best (short_sma, long_sma) combination\n",
    "        best_trades = 0  # Number of trades with the best parameters\n",
    "        best_sim_data = None  # Store the simulation data for the best parameters\n",
    "\n",
    "        # Create a list to store all simulation results\n",
    "        all_results = []\n",
    "\n",
    "        # Create the output file for all simulations and write the header\n",
    "        if results_file:\n",
    "            with open(results_file, 'w') as f:\n",
    "                f.write(\"short_SMA,long_SMA,trades,sharpe_ratio\\n\")\n",
    "\n",
    "        # Count total combinations to test\n",
    "        total_combinations = sum(1 for a, b in [(s, l) for s in sma_range for l in sma_range] if a < b)\n",
    "        completed = 0\n",
    "\n",
    "        print(f\"Running {total_combinations} simulations...\")\n",
    "\n",
    "        # Track individual simulation times\n",
    "        total_sim_time = 0\n",
    "        \n",
    "        # Iterate through all possible combinations of short and long SMA periods\n",
    "        for short_sma in sma_range:\n",
    "            for long_sma in sma_range:\n",
    "                # Skip invalid combinations where short SMA is not actually shorter than long SMA\n",
    "                if short_sma >= long_sma:\n",
    "                    continue\n",
    "\n",
    "                # Start timing this simulation\n",
    "                sim_start_time = time.time()\n",
    "                \n",
    "                # Save original parameters\n",
    "                orig_short_sma = self.short_sma\n",
    "                orig_long_sma = self.long_sma\n",
    "\n",
    "                # Set new parameters for this simulation\n",
    "                self.short_sma = short_sma\n",
    "                self.long_sma = long_sma\n",
    "\n",
    "                # Apply strategy to the data\n",
    "                sim_data = self.apply_strategy(data.copy(), strategy_name=\"Sim\")\n",
    "\n",
    "                # Trim warm-up period if provided\n",
    "                if warm_up_idx is not None:\n",
    "                    # Create an explicit copy to avoid the SettingWithCopyWarning\n",
    "                    sim_data_eval = sim_data.iloc[warm_up_idx:].copy()\n",
    "                    # Recalculate the split index based on the trimmed data length\n",
    "                    split_index = int(len(sim_data_eval) * train_test_split)\n",
    "                else:\n",
    "                    # Since we're already working with a copy, no need to make another\n",
    "                    sim_data_eval = sim_data\n",
    "                    # Calculate split index for in-sample/out-of-sample\n",
    "                    split_index = int(len(sim_data_eval) * train_test_split)\n",
    "\n",
    "                # Count trades by identifying position changes\n",
    "                trade_entries = sim_data_eval['Position_Change_Sim']\n",
    "                trade_count = trade_entries.sum()\n",
    "\n",
    "                # Calculate daily returns - using loc to avoid SettingWithCopyWarning\n",
    "                sim_data_eval.loc[:, 'Daily_Returns'] = sim_data_eval['Daily_PnL_Sim']\n",
    "                \n",
    "                # Calculate Sharpe ratio using only in-sample data (on dollar returns)\n",
    "                in_sample_returns = sim_data_eval['Daily_Returns'].iloc[:split_index]\n",
    "\n",
    "                # Skip if there are no returns or all returns are 0\n",
    "                if len(in_sample_returns.dropna()) == 0 or in_sample_returns.std() == 0:\n",
    "                    sharpe_ratio = 0\n",
    "                else:\n",
    "                    sharpe_ratio = in_sample_returns.mean() / in_sample_returns.std() * np.sqrt(252)  # Annualized\n",
    "\n",
    "                # Append the results to our file\n",
    "                if results_file:\n",
    "                    with open(results_file, 'a') as f:\n",
    "                        f.write(f\"{short_sma},{long_sma},{trade_count},{sharpe_ratio:.6f}\\n\")\n",
    "\n",
    "                # Store the results\n",
    "                result = (short_sma, long_sma, trade_count, sharpe_ratio)\n",
    "                all_results.append(result)\n",
    "\n",
    "                # Update best parameters if current combination performs better\n",
    "                if sharpe_ratio > best_sharpe:\n",
    "                    best_sharpe = sharpe_ratio\n",
    "                    best_sma = (short_sma, long_sma)\n",
    "                    best_trades = trade_count\n",
    "                    best_sim_data = sim_data_eval.copy()  # Store for verification\n",
    "\n",
    "                # Restore original parameters\n",
    "                self.short_sma = orig_short_sma\n",
    "                self.long_sma = orig_long_sma\n",
    "\n",
    "                # End timing for this simulation\n",
    "                sim_end_time = time.time()\n",
    "                sim_time = sim_end_time - sim_start_time\n",
    "                total_sim_time += sim_time\n",
    "                \n",
    "                # Update progress\n",
    "                completed += 1\n",
    "                if completed % 100 == 0 or completed == total_combinations:\n",
    "                    avg_sim_time = total_sim_time / completed\n",
    "                    est_remaining = avg_sim_time * (total_combinations - completed)\n",
    "                    print(\n",
    "                        f\"Progress: {completed}/{total_combinations} simulations completed ({(completed / total_combinations * 100):.1f}%)\"\n",
    "                        f\" - Avg sim time: {avg_sim_time:.4f}s - Est. remaining: {est_remaining:.1f}s\")\n",
    "\n",
    "        # Verify the calculation (for debugging)\n",
    "        if best_sim_data is not None:\n",
    "            print(\"\\n--- OPTIMIZATION SHARPE VERIFICATION ---\")\n",
    "            \n",
    "            # Calculate metrics on the best sim data\n",
    "            verify_split_idx = int(len(best_sim_data) * train_test_split)\n",
    "            verify_returns = best_sim_data['Daily_Returns'].iloc[:verify_split_idx]\n",
    "            \n",
    "            if verify_returns.std() > 0:\n",
    "                verify_sharpe = verify_returns.mean() / verify_returns.std() * np.sqrt(252)\n",
    "                print(f\"Optimization best Sharpe = {best_sharpe:.6f}\")\n",
    "                print(f\"Verification Sharpe = {verify_sharpe:.6f}\")\n",
    "                print(f\"Data points used: {len(best_sim_data)}\")\n",
    "                print(f\"In-sample data points: {len(verify_returns)}\")\n",
    "            else:\n",
    "                print(\"Cannot verify Sharpe (std = 0)\")\n",
    "\n",
    "        # Return the optimal SMA parameters, corresponding Sharpe ratio, and all results\n",
    "        return best_sma, best_sharpe, best_trades, all_results\n",
    "\n",
    "    def calculate_performance_metrics(self, data, strategy_name=\"Strategy\", train_test_split=0.7):\n",
    "        \"\"\"\n",
    "        Calculate detailed performance metrics for the strategy\n",
    "\n",
    "        Parameters:\n",
    "        data: DataFrame with strategy results\n",
    "        strategy_name: Name suffix for the strategy columns\n",
    "        train_test_split: Portion of data used for in-sample testing\n",
    "\n",
    "        Returns:\n",
    "        dict: Dictionary with performance metrics\n",
    "        \"\"\"\n",
    "        # Calculate split index for in-sample/out-of-sample\n",
    "        split_index = int(len(data) * train_test_split)\n",
    "        split_date = data.index[split_index]\n",
    "\n",
    "        # Extract daily P&L data\n",
    "        daily_pnl = data[f'Daily_PnL_{strategy_name}']\n",
    "        \n",
    "        # Split returns for performance comparison\n",
    "        returns_in_sample = daily_pnl.iloc[:split_index]\n",
    "        returns_out_sample = daily_pnl.iloc[split_index:]\n",
    "\n",
    "        # Calculate separate Sharpe ratios on dollar P&L (annualized)\n",
    "        sharpe_in_sample = returns_in_sample.mean() / returns_in_sample.std() * np.sqrt(\n",
    "            252) if returns_in_sample.std() > 0 else 0\n",
    "        sharpe_out_sample = returns_out_sample.mean() / returns_out_sample.std() * np.sqrt(\n",
    "            252) if returns_out_sample.std() > 0 else 0\n",
    "        sharpe_full = daily_pnl.mean() / daily_pnl.std() * np.sqrt(252) if daily_pnl.std() > 0 else 0\n",
    "\n",
    "        # Print detailed Sharpe ratio calculation info for verification\n",
    "        print(\"\\n--- PERFORMANCE METRICS SHARPE VERIFICATION ---\")\n",
    "        print(f\"In-sample Sharpe = {sharpe_in_sample:.6f}\")\n",
    "        print(f\"Data points used: {len(data)}\")\n",
    "        print(f\"In-sample data points: {len(returns_in_sample)}\")\n",
    "        print(f\"Mean: {returns_in_sample.mean():.6f}, Std: {returns_in_sample.std():.6f}\")\n",
    "\n",
    "        # Calculate trade counts using position changes\n",
    "        position_changes = data[f'Position_Change_{strategy_name}']\n",
    "        \n",
    "        total_trades = position_changes.sum()\n",
    "        in_sample_trades = position_changes.iloc[:split_index].sum()\n",
    "        out_sample_trades = position_changes.iloc[split_index:].sum()\n",
    "\n",
    "        # Calculate max drawdown in dollar terms\n",
    "        pnl_series = data[f'Cumulative_PnL_{strategy_name}']\n",
    "        # Use loc to avoid SettingWithCopyWarning\n",
    "        data.loc[:, 'Peak'] = pnl_series.cummax()\n",
    "        data.loc[:, 'Drawdown_Dollars'] = pnl_series - data['Peak']\n",
    "        max_drawdown_dollars = data['Drawdown_Dollars'].min()\n",
    "\n",
    "        # Calculate profit/loss metrics\n",
    "        total_pnl = data[f'Cumulative_PnL_{strategy_name}'].iloc[-1]\n",
    "\n",
    "        # Calculate in-sample and out-of-sample P&L\n",
    "        in_sample_pnl = data[f'Daily_PnL_{strategy_name}'].iloc[:split_index].sum()\n",
    "        out_sample_pnl = data[f'Daily_PnL_{strategy_name}'].iloc[split_index:].sum()\n",
    "\n",
    "        # Calculate average position size\n",
    "        avg_position_size = data[f'Position_Size_{strategy_name}'].mean()\n",
    "        max_position_size = data[f'Position_Size_{strategy_name}'].max()\n",
    "\n",
    "        # Assemble the results in a dictionary\n",
    "        metrics = {\n",
    "            'split_date': split_date,\n",
    "            'total_pnl': total_pnl,\n",
    "            'sharpe_full': sharpe_full,\n",
    "            'sharpe_in_sample': sharpe_in_sample,\n",
    "            'sharpe_out_sample': sharpe_out_sample,\n",
    "            'max_drawdown_dollars': max_drawdown_dollars,\n",
    "            'total_trades': total_trades,\n",
    "            'in_sample_trades': in_sample_trades,\n",
    "            'out_sample_trades': out_sample_trades,\n",
    "            'in_sample_pnl': in_sample_pnl,\n",
    "            'out_sample_pnl': out_sample_pnl,\n",
    "            'avg_position_size': avg_position_size,\n",
    "            'max_position_size': max_position_size\n",
    "        }\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab2629",
   "metadata": {},
   "source": [
    "## 4. Data Gathering Implementation (data_gather.py)\n",
    "Paste the contents of data_gather.py here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37ef77",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Import the read_ts module for data loading\n",
    "from read_ts import read_ts_ohlcv_dat, read_ts_pnl_dat_multicore\n",
    "\n",
    "# Import configuration\n",
    "from input import *\n",
    "from SMA_Strategy import SMAStrategy\n",
    "\n",
    "\n",
    "def data_gather_main():\n",
    "    # Start overall execution timer\n",
    "    overall_start_time = time.time()\n",
    "    \n",
    "    # Setup paths using relative directories\n",
    "    WORKING_DIR = \".\"  # Current directory\n",
    "    DATA_DIR = os.path.join(WORKING_DIR, \"data\")\n",
    "\n",
    "    # Define SYMBOL based on TICKER\n",
    "    SYMBOL = TICKER.replace('=F', '')  # This strips the '=F' part if present in the ticker symbol\n",
    "\n",
    "    # Create the output directory for each symbol\n",
    "    output_dir = os.path.join(WORKING_DIR, 'output', SYMBOL)  # Symbol-specific folder\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "    # Function to save plots in the created folder\n",
    "    def save_plot(plot_name):\n",
    "        plt.savefig(os.path.join(output_dir, plot_name))  # Save plot to the symbol-specific folder\n",
    "        plt.show()  # Display the plot\n",
    "        plt.close()  # Close the plot to free up memory\n",
    "\n",
    "    # Function to save parameters to a JSON file\n",
    "    def save_parameters():\n",
    "        \"\"\"Save the big_point_value and dynamic_slippage to a JSON file.\"\"\"\n",
    "        parameters = {\n",
    "            \"big_point_value\": big_point_value,\n",
    "            \"slippage\": slippage,\n",
    "            \"capital\": TRADING_CAPITAL,\n",
    "            \"atr_period\": ATR_PERIOD\n",
    "        }\n",
    "\n",
    "        with open(\"parameters.json\", \"w\") as file:\n",
    "            json.dump(parameters, file)\n",
    "\n",
    "    # Function to get slippage value from an Excel file - STRICT VERSION\n",
    "    def get_slippage_from_excel(symbol, data_dir):\n",
    "        \"\"\"\n",
    "        Get the slippage value for a specific symbol from the Excel file\n",
    "        \n",
    "        Parameters:\n",
    "        symbol: str - The trading symbol to look up (without '=F')\n",
    "        data_dir: str - Directory containing the Excel file\n",
    "        \n",
    "        Returns:\n",
    "        float - Slippage value for the symbol\n",
    "        \"\"\"\n",
    "        excel_path = os.path.join(data_dir, \"sessions_slippages.xlsx\")\n",
    "        \n",
    "        # No fallback - if file doesn't exist, crash\n",
    "        if not os.path.exists(excel_path):\n",
    "            raise FileNotFoundError(f\"Slippage Excel file not found at {excel_path}\")\n",
    "        \n",
    "        # Remove '=F' suffix if present for lookup\n",
    "        lookup_symbol = symbol.replace('=F', '')\n",
    "        \n",
    "        # Read the Excel file - will throw exception if any issue\n",
    "        df = pd.read_excel(excel_path)\n",
    "        \n",
    "        # Print the Excel contents for debugging\n",
    "        print(\"\\nContents of sessions_slippages.xlsx:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Check if we have at least 4 columns (to access column D)\n",
    "        if df.shape[1] < 4:\n",
    "            raise ValueError(f\"Excel file has fewer than 4 columns: {df.columns.tolist()}\")\n",
    "            \n",
    "        # Print column names for debugging\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Use direct column access - Column B (index 1) for symbol, Column D (index 3) for slippage\n",
    "        # First convert to uppercase for case-insensitive comparison\n",
    "        df['SymbolUpper'] = df.iloc[:, 1].astype(str).str.upper()\n",
    "        lookup_symbol_upper = lookup_symbol.upper()\n",
    "        \n",
    "        # Find the matching row\n",
    "        matching_rows = df[df['SymbolUpper'] == lookup_symbol_upper]\n",
    "        \n",
    "        if matching_rows.empty:\n",
    "            raise ValueError(f\"Symbol '{lookup_symbol}' not found in column B of Excel file\")\n",
    "            \n",
    "        # Get the slippage value from column D (index 3)\n",
    "        slippage_value = matching_rows.iloc[0, 3]\n",
    "        \n",
    "        # Validate the slippage value is numeric\n",
    "        if pd.isna(slippage_value) or not isinstance(slippage_value, (int, float)):\n",
    "            raise ValueError(f\"Invalid slippage value for symbol '{lookup_symbol}': {slippage_value}\")\n",
    "            \n",
    "        print(f\"Found slippage for {lookup_symbol} in column D: {slippage_value}\")\n",
    "        return slippage_value\n",
    "\n",
    "    # Function to find the data file for the specified futures symbol\n",
    "    def find_futures_file(symbol, data_dir):\n",
    "        \"\"\"Find a data file for the specified futures symbol\"\"\"\n",
    "        # First try a pattern that specifically looks for @SYMBOL\n",
    "        pattern = f\"*@{symbol}_*.dat\"\n",
    "        files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            # Try a pattern that looks for the symbol with an underscore or at boundary\n",
    "            pattern = f\"*_@{symbol}_*.dat\"\n",
    "            files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            # Try a more specific boundary pattern for the symbol\n",
    "            pattern = f\"*_{symbol}_*.dat\"\n",
    "            files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            # Last resort: less specific but better than nothing\n",
    "            pattern = f\"*@{symbol}*.dat\"\n",
    "            files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        # No fallback - if no file found, crash\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No data file found for {symbol} in {data_dir}\")\n",
    "            \n",
    "        return files[0]  # Return the first matching file\n",
    "\n",
    "    # Get symbol from the TICKER variable (remove '=F' if it exists)\n",
    "    SYMBOL = TICKER.replace('=F', '')\n",
    "\n",
    "    # Load the futures data file using multicore processing for better performance\n",
    "    print(\"Loading data file...\")\n",
    "    load_start_time = time.time()\n",
    "    \n",
    "    # First find all matching files\n",
    "    data_files = glob.glob(os.path.join(DATA_DIR, f\"*@{SYMBOL}*.dat\"))\n",
    "    if not data_files:\n",
    "        data_files = glob.glob(os.path.join(DATA_DIR, f\"*_{SYMBOL}_*.dat\"))\n",
    "    \n",
    "    if not data_files:\n",
    "        raise FileNotFoundError(f\"No data file found for {SYMBOL} in {DATA_DIR}\")\n",
    "    \n",
    "    # Load data using multicore processing\n",
    "    all_data = read_ts_ohlcv_dat(data_files[0])  # Use first matching file\n",
    "    load_end_time = time.time()\n",
    "    load_time = load_end_time - load_start_time\n",
    "    print(f\"Data loaded successfully in {load_time:.2f} seconds! Number of items: {len(all_data)}\")\n",
    "    \n",
    "    # Extract metadata and OHLCV data from the first data object\n",
    "    data_obj = all_data[0]\n",
    "    tick_size = data_obj.big_point_value * data_obj.tick_size\n",
    "    \n",
    "    # Get big point value from data\n",
    "    big_point_value = data_obj.big_point_value\n",
    "    \n",
    "    # Get OHLCV data\n",
    "    ohlc_data = data_obj.data.copy()  # Make a copy to avoid modifying original data\n",
    "\n",
    "    # Fetch slippage value from Excel - NO FALLBACK\n",
    "    slippage_value = get_slippage_from_excel(TICKER, DATA_DIR)\n",
    "    slippage = slippage_value\n",
    "    print(f\"Using slippage from Excel column D: {slippage}\")\n",
    "\n",
    "    # Save the parameters to a JSON file\n",
    "    save_parameters()\n",
    "    \n",
    "    # Start timing data preparation\n",
    "    prep_start_time = time.time()\n",
    "    \n",
    "    # Print information about the data\n",
    "    print(f\"\\nSymbol: {data_obj.symbol}\")\n",
    "    print(f\"Description: {data_obj.description}\")\n",
    "    print(f\"Exchange: {data_obj.exchange}\")\n",
    "    print(f\"Interval: {data_obj.interval_type} {data_obj.interval_span}\")\n",
    "    print(f\"Tick size: {tick_size}\")\n",
    "    print(f\"Big point value: {big_point_value}\")\n",
    "    print(f\"Data shape: {ohlc_data.shape}\")\n",
    "    print(f\"Date range: {ohlc_data['datetime'].min()} to {ohlc_data['datetime'].max()}\")\n",
    "    \n",
    "    # Display the first few rows of data\n",
    "    print(\"\\nFirst few rows of OHLCV data:\")\n",
    "    print(ohlc_data.head())\n",
    "    \n",
    "    # Convert the OHLCV data to the format expected by the SMA strategy\n",
    "    # First, rename columns to match what yfinance provides\n",
    "    data = ohlc_data.rename(columns={\n",
    "        'datetime': 'Date',\n",
    "        'open': 'Open',\n",
    "        'high': 'High',\n",
    "        'low': 'Low',\n",
    "        'close': 'Close',\n",
    "        'volume': 'Volume'\n",
    "    })\n",
    "    \n",
    "    # Set the datetime column as the index\n",
    "    data.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Add warm-up period for SMA calculation\n",
    "    original_start_idx = None\n",
    "    \n",
    "    # Filter data to match the date range if specified in input.py\n",
    "    if START_DATE and END_DATE:\n",
    "        # Calculate warm-up period (longest SMA + buffer for ATR calculation)\n",
    "        warm_up_days = SMA_MAX + ATR_PERIOD + 50  # Add buffer days for safety\n",
    "        \n",
    "        # Convert dates to datetime\n",
    "        start_date = pd.to_datetime(START_DATE)\n",
    "        end_date = pd.to_datetime(END_DATE)\n",
    "        \n",
    "        # Adjust start date for warm-up\n",
    "        adjusted_start = start_date - pd.Timedelta(days=warm_up_days)\n",
    "        \n",
    "        # Load more data for warm-up\n",
    "        data = data[(data.index >= adjusted_start) & \n",
    "                    (data.index <= end_date)]\n",
    "        \n",
    "        # Store the original start date index for later use\n",
    "        if data.empty:\n",
    "            raise ValueError(f\"No data available for the specified date range: {START_DATE} to {END_DATE}\")\n",
    "            \n",
    "        # Find the closest index to our original start date\n",
    "        original_start_idx = data.index.get_indexer([start_date], method='nearest')[0]\n",
    "        \n",
    "        print(f\"Loaded extended data with {warm_up_days} days warm-up period\")\n",
    "        print(f\"Original date range: {START_DATE} to {END_DATE}\")\n",
    "        print(f\"Adjusted date range: {adjusted_start.strftime('%Y-%m-%d')} to {END_DATE}\")\n",
    "        print(f\"Original start index: {original_start_idx}\")\n",
    "    \n",
    "    prep_end_time = time.time()\n",
    "    prep_time = prep_end_time - prep_start_time\n",
    "    print(f\"Data preparation completed in {prep_time:.2f} seconds\")\n",
    "    \n",
    "    # Define the range of SMA periods to test\n",
    "    sma_range = range(SMA_MIN, SMA_MAX + 1, SMA_STEP)\n",
    "    \n",
    "    print(f\"Optimizing SMA parameters using range from {SMA_MIN} to {SMA_MAX} with step {SMA_STEP}...\")\n",
    "    print(f\"Trading with big point value from data: {big_point_value}\")\n",
    "    print(f\"Using capital allocation: ${TRADING_CAPITAL:,} with ATR period: {ATR_PERIOD}\")\n",
    "    \n",
    "    # Initialize the ATR-based strategy using the big point value from the data\n",
    "    strategy = SMAStrategy(\n",
    "        short_sma=0,  # Will be set during optimization\n",
    "        long_sma=0,  # Will be set during optimization\n",
    "        big_point_value=big_point_value,  # Use the big point value from data\n",
    "        slippage=slippage,  # Use dynamically calculated slippage\n",
    "        capital=TRADING_CAPITAL,  # Capital allocation for position sizing\n",
    "        atr_period=ATR_PERIOD  # ATR period for position sizing\n",
    "    )\n",
    "    \n",
    "    # Start timing the optimization process\n",
    "    print(\"\\nStarting optimization process...\")\n",
    "    optimization_start_time = time.time()\n",
    "    \n",
    "    # Run the optimization function to find the best SMA parameters\n",
    "    # Save results file for data_analysis.py to use\n",
    "    best_sma, best_sharpe, best_trades, all_results = strategy.optimize(\n",
    "        data.copy(),\n",
    "        sma_range,\n",
    "        train_test_split=TRAIN_TEST_SPLIT,\n",
    "        results_file='sma_all_results.txt',  # Relative path to current directory\n",
    "        warm_up_idx=original_start_idx  # Pass the warm-up index to ensure consistent Sharpe calculation\n",
    "    )\n",
    "    \n",
    "    # Calculate and display the time taken for optimization\n",
    "    optimization_end_time = time.time()\n",
    "    optimization_time = optimization_end_time - optimization_start_time\n",
    "    print(f\"\\nOptimization completed in {optimization_time:.2f} seconds ({optimization_time/60:.2f} minutes)\")\n",
    "    \n",
    "    print(f\"Optimal SMA parameters: Short = {best_sma[0]} days, Long = {best_sma[1]} days\")\n",
    "    print(f\"In-sample Sharpe ratio = {best_sharpe:.4f}\")\n",
    "    print(f\"Number of trades with optimal parameters = {best_trades}\")\n",
    "    print(f\"Optimization results saved to 'sma_all_results.txt' for further analysis\")\n",
    "    \n",
    "    # Update strategy with the best parameters\n",
    "    strategy.short_sma = best_sma[0]\n",
    "    strategy.long_sma = best_sma[1]\n",
    "    \n",
    "    # Apply the best SMA parameters found from optimization to the dataset\n",
    "    print(\"\\nApplying best strategy parameters...\")\n",
    "    apply_start_time = time.time()\n",
    "    \n",
    "    # Apply strategy to the full dataset including warm-up period\n",
    "    data = strategy.apply_strategy(data.copy())\n",
    "    \n",
    "    # Store the data for evaluation (will be trimmed later)\n",
    "    data_for_evaluation = data.copy()\n",
    "    \n",
    "    apply_end_time = time.time()\n",
    "    apply_time = apply_end_time - apply_start_time\n",
    "    print(f\"Strategy application completed in {apply_time:.2f} seconds\")\n",
    "    \n",
    "    # Start timing the visualization process\n",
    "    viz_start_time = time.time()\n",
    "    \n",
    "    # Trim data back to the original date range for evaluation\n",
    "    if original_start_idx is not None:\n",
    "        print(\"Trimming warm-up period for final evaluation and visualization...\")\n",
    "        data_for_evaluation = data.iloc[original_start_idx:]\n",
    "        print(f\"Original data length: {len(data)}, Evaluation data length: {len(data_for_evaluation)}\")\n",
    "    else:\n",
    "        data_for_evaluation = data\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    plt.figure(figsize=(14, 16))\n",
    "    \n",
    "    # Plot price and SMAs\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(data_for_evaluation.index, data_for_evaluation['Close'], label=f'{data_obj.symbol} Price', color='blue')\n",
    "    plt.plot(data_for_evaluation.index, data_for_evaluation['SMA_Short_Strategy'], label=f'{best_sma[0]}-day SMA', color='orange')\n",
    "    plt.plot(data_for_evaluation.index, data_for_evaluation['SMA_Long_Strategy'], label=f'{best_sma[1]}-day SMA', color='red')\n",
    "    \n",
    "    # Plot position changes (using vectorized identification of changes)\n",
    "    long_entries = (data_for_evaluation['Position_Dir_Strategy'] == 1) & data_for_evaluation['Position_Change_Strategy']\n",
    "    short_entries = (data_for_evaluation['Position_Dir_Strategy'] == -1) & data_for_evaluation['Position_Change_Strategy']\n",
    "    \n",
    "    # Plot the entries\n",
    "    plt.scatter(data_for_evaluation.index[long_entries], data_for_evaluation.loc[long_entries, 'Close'], \n",
    "                color='green', marker='^', s=50, label='Long Entry')\n",
    "    plt.scatter(data_for_evaluation.index[short_entries], data_for_evaluation.loc[short_entries, 'Close'], \n",
    "                color='red', marker='v', s=50, label='Short Entry')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f'{data_obj.symbol} with Optimized SMA Strategy ({best_sma[0]}, {best_sma[1]})')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot position size based on ATR with dual y-axes\n",
    "    ax1 = plt.subplot(3, 1, 2)\n",
    "    ax2 = ax1.twinx()  # Create a second y-axis that shares the same x-axis\n",
    "    \n",
    "    # Plot position size on the left y-axis\n",
    "    ax1.plot(data_for_evaluation.index, data_for_evaluation['Position_Size_Strategy'], \n",
    "             label='Position Size (# Contracts)', color='purple')\n",
    "    ax1.set_ylabel('Position Size (# Contracts)', color='purple')\n",
    "    ax1.tick_params(axis='y', colors='purple')\n",
    "    \n",
    "    # Plot ATR on the right y-axis\n",
    "    ax2.plot(data_for_evaluation.index, data_for_evaluation['ATR_Strategy'], \n",
    "             label=f'ATR ({ATR_PERIOD}-day)', color='orange')\n",
    "    ax2.set_ylabel(f'ATR ({ATR_PERIOD}-day)', color='orange')\n",
    "    ax2.tick_params(axis='y', colors='orange')\n",
    "    \n",
    "    # Add legends for both axes\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    plt.title(f'Position Sizing Based on {ATR_PERIOD}-day ATR')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot the performance (P&L)\n",
    "    plt.subplot(3, 1, 3)\n",
    "    \n",
    "    # Reset P&L to zero at start of evaluation period for cleaner visualization\n",
    "    strategy_pnl_cumulative = data_for_evaluation['Cumulative_PnL_Strategy'] - data_for_evaluation['Cumulative_PnL_Strategy'].iloc[0]\n",
    "    \n",
    "    # Plot the cumulative P&L of the strategy (removed market P&L)\n",
    "    plt.plot(data_for_evaluation.index, strategy_pnl_cumulative, \n",
    "             label='Strategy P&L (full period)', color='green')\n",
    "    \n",
    "    # Calculate split index based on trimmed data\n",
    "    split_index = int(len(data_for_evaluation) * TRAIN_TEST_SPLIT)\n",
    "    \n",
    "    # Highlight out-of-sample period\n",
    "    plt.plot(data_for_evaluation.index[split_index:], strategy_pnl_cumulative.iloc[split_index:],\n",
    "            label=f'Strategy P&L (last {int((1 - TRAIN_TEST_SPLIT) * 100)}% out-of-sample)', color='purple')\n",
    "    \n",
    "    # Add split line and zero line\n",
    "    plt.axvline(x=data_for_evaluation.index[split_index], color='black', linestyle='--',\n",
    "                label=f'Train/Test Split ({int(TRAIN_TEST_SPLIT * 100)}%/{int((1 - TRAIN_TEST_SPLIT) * 100)}%)')\n",
    "    plt.axhline(y=0.0, color='gray', linestyle='-', label='Break-even')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('Strategy Performance (Dollar P&L)')\n",
    "    plt.ylabel('P&L ($)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_plot('Optimized_Strategy_Plot.png')\n",
    "    \n",
    "    viz_end_time = time.time()\n",
    "    viz_time = viz_end_time - viz_start_time\n",
    "    print(f\"Visualization completed in {viz_time:.2f} seconds\")\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    metrics_start_time = time.time()\n",
    "    metrics = strategy.calculate_performance_metrics(\n",
    "        data_for_evaluation,  # Use the trimmed data for metrics\n",
    "        strategy_name=\"Strategy\",\n",
    "        train_test_split=TRAIN_TEST_SPLIT\n",
    "    )\n",
    "    metrics_end_time = time.time()\n",
    "    metrics_time = metrics_end_time - metrics_start_time\n",
    "    print(f\"Performance metrics calculation completed in {metrics_time:.2f} seconds\")\n",
    "    \n",
    "    # Calculate market performance for comparison (for reporting only, not plotting)\n",
    "    market_cumulative_pnl = data_for_evaluation['Market_PnL_Strategy'].cumsum().iloc[-1]\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n--- PERFORMANCE SUMMARY OF ATR-BASED SMA STRATEGY ---\")\n",
    "    print(f\"Symbol: {data_obj.symbol}\")\n",
    "    print(f\"Big Point Value (from data): {big_point_value}\")\n",
    "    print(f\"ATR Period for Position Sizing: {ATR_PERIOD} days\")\n",
    "    print(f\"Capital Allocation: ${TRADING_CAPITAL:,}\")\n",
    "    print(f\"Average Position Size: {metrics['avg_position_size']:.2f} contracts\")\n",
    "    print(f\"Maximum Position Size: {metrics['max_position_size']:.0f} contracts\")\n",
    "    print(f\"Strategy Total P&L: ${metrics['total_pnl']:,.2f}\")\n",
    "    print(f\"Market Buy & Hold P&L: ${market_cumulative_pnl:,.2f}\")\n",
    "    print(f\"Outperformance: ${(metrics['total_pnl'] - market_cumulative_pnl):,.2f}\")\n",
    "    \n",
    "    # *** SHARPE RATIO VERIFICATION ***\n",
    "    print(\"\\n--- SHARPE RATIO COMPARISON VERIFICATION ---\")\n",
    "    print(f\"Optimization in-sample Sharpe ratio: {best_sharpe:.6f}\")\n",
    "    print(f\"Final in-sample Sharpe ratio: {metrics['sharpe_in_sample']:.6f}\")\n",
    "    print(f\"Difference: {abs(best_sharpe - metrics['sharpe_in_sample']):.6f}\")\n",
    "    if abs(best_sharpe - metrics['sharpe_in_sample']) < 0.001:\n",
    "        print(\" SHARPE RATIOS MATCH (within 0.001 tolerance)\")\n",
    "    else:\n",
    "        print(\" SHARPE RATIOS DO NOT MATCH\")\n",
    "    \n",
    "    print(f\"Sharpe ratio (entire period, annualized): {metrics['sharpe_full']:.4f}\")\n",
    "    print(f\"Sharpe ratio (in-sample, annualized): {metrics['sharpe_in_sample']:.4f}\")\n",
    "    print(f\"Sharpe ratio (out-of-sample, annualized): {metrics['sharpe_out_sample']:.4f}\")\n",
    "    print(f\"Maximum Drawdown: ${abs(metrics['max_drawdown_dollars']):,.2f}\")\n",
    "    print(\"\\n--- TRADE COUNT SUMMARY ---\")\n",
    "    print(f\"In-sample period trades: {metrics['in_sample_trades']}\")\n",
    "    print(f\"Out-of-sample period trades: {metrics['out_sample_trades']}\")\n",
    "    print(f\"Total trades: {metrics['total_trades']}\")\n",
    "    print(f\"In-sample P&L: ${metrics['in_sample_pnl']:,.2f}\")\n",
    "    print(f\"Out-of-sample P&L: ${metrics['out_sample_pnl']:,.2f}\")\n",
    "    \n",
    "    print(f\"\\nBest parameters: Short SMA = {best_sma[0]}, Long SMA = {best_sma[1]}, Sharpe = {best_sharpe:.6f}, Trades = {best_trades}\")\n",
    "    \n",
    "    # Calculate overall execution time\n",
    "    overall_end_time = time.time()\n",
    "    overall_time = overall_end_time - overall_start_time\n",
    "    \n",
    "    # Print timing summary\n",
    "    print(\"\\n--- EXECUTION TIME SUMMARY (Vectorized Implementation) ---\")\n",
    "    print(f\"Data loading time: {load_time:.2f} seconds\")\n",
    "    print(f\"Data preparation time: {prep_time:.2f} seconds\")\n",
    "    print(f\"Optimization time: {optimization_time:.2f} seconds ({optimization_time/60:.2f} minutes)\")\n",
    "    print(f\"Strategy application time: {apply_time:.2f} seconds\")\n",
    "    print(f\"Visualization time: {viz_time:.2f} seconds\")\n",
    "    print(f\"Metrics calculation time: {metrics_time:.2f} seconds\")\n",
    "    print(f\"Total execution time: {overall_time:.2f} seconds ({overall_time/60:.2f} minutes)\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_gather_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e820736",
   "metadata": {},
   "source": [
    "## 5. Data Analysis Implementation (data_analysis.py)\n",
    "Paste the contents of data_analysis.py here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a32c91",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.patches import Circle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import read_ts\n",
    "import openpyxl\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from input import MIN_TRADES, MAX_TRADES, MIN_ELEMENTS_PER_CLUSTER, DEFAULT_NUM_CLUSTERS\n",
    "from input import TICKER, START_DATE, END_DATE, TRAIN_TEST_SPLIT, ATR_PERIOD, TRADING_CAPITAL\n",
    "from SMA_Strategy import SMAStrategy\n",
    "\n",
    "import glob\n",
    "import json\n",
    "def data_analysis_main():\n",
    "    def load_parameters():\n",
    "        try:\n",
    "            with open(\"parameters.json\", \"r\") as file:\n",
    "                parameters = json.load(file)\n",
    "                big_point_value = parameters[\"big_point_value\"]\n",
    "                slippage = parameters[\"slippage\"]\n",
    "                capital = parameters.get(\"capital\", TRADING_CAPITAL)\n",
    "                atr_period = parameters.get(\"atr_period\", ATR_PERIOD)\n",
    "                return big_point_value, slippage, capital, atr_period\n",
    "        except FileNotFoundError:\n",
    "            print(\"Parameters file not found. Ensuring it was saved correctly in data_gather.py.\")\n",
    "            return None, None, TRADING_CAPITAL, ATR_PERIOD\n",
    "\n",
    "    # Load the parameters\n",
    "    big_point_value, slippage, capital, atr_period = load_parameters()\n",
    "\n",
    "    print(f\"Big Point Value: {big_point_value}\")\n",
    "    print(f\"Dynamic Slippage: {slippage}\")\n",
    "    print(f\"Capital for Position Sizing: {capital:,}\")\n",
    "    print(f\"ATR Period: {atr_period}\")\n",
    "\n",
    "    # Setup paths\n",
    "    WORKING_DIR = \".\"  # Current directory\n",
    "    DATA_DIR = os.path.join(WORKING_DIR, \"data\")\n",
    "    SYMBOL = TICKER.replace('=F', '')\n",
    "\n",
    "    # Define the output folder where the plots will be saved\n",
    "    output_dir = os.path.join(WORKING_DIR, 'output', SYMBOL)\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Function to save plots in the created folder\n",
    "    def save_plot(plot_name):\n",
    "        plt.savefig(os.path.join(output_dir, plot_name))  # Save plot to the symbol-specific folder\n",
    "        plt.show()  # Display the plot\n",
    "        plt.close()  # Close the plot to free up memory\n",
    "\n",
    "    # Function to save results to csv in the same output directory\n",
    "    def save_results(data_frame, file_name):\n",
    "        csv_path = os.path.join(output_dir, file_name)\n",
    "        data_frame.to_csv(csv_path, index=False)\n",
    "        print(f\"Results saved to {csv_path}\")\n",
    "\n",
    "    def find_futures_file(symbol, data_dir):\n",
    "        \"\"\"Find a data file for the specified futures symbol\"\"\"\n",
    "        # First try a pattern that specifically looks for @SYMBOL\n",
    "        pattern = f\"*@{symbol}_*.dat\"\n",
    "        files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            # Try a pattern that looks for the symbol with an underscore or at boundary\n",
    "            pattern = f\"*_@{symbol}_*.dat\"\n",
    "            files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            # Try a more specific boundary pattern for the symbol\n",
    "            pattern = f\"*_{symbol}_*.dat\"\n",
    "            files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            # Last resort: less specific but better than nothing\n",
    "            pattern = f\"*@{symbol}*.dat\"\n",
    "            files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        return files[0] if files else None\n",
    "\n",
    "    # Added functions from cluster_visualization.py\n",
    "    def compute_medoids(X, labels, valid_clusters):\n",
    "        \"\"\"Compute medoids for each cluster (point with minimum distance to all other points in cluster)\"\"\"\n",
    "        medoids = []\n",
    "\n",
    "        for cluster_id in valid_clusters:\n",
    "            # Get points in this cluster\n",
    "            cluster_points = X[labels == cluster_id]\n",
    "\n",
    "            if len(cluster_points) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate pairwise distances within cluster\n",
    "            min_total_distance = float('inf')\n",
    "            medoid = None\n",
    "\n",
    "            for i, point1 in enumerate(cluster_points):\n",
    "                total_distance = 0\n",
    "                for j, point2 in enumerate(cluster_points):\n",
    "                    # Calculate Euclidean distance between points\n",
    "                    distance = np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "                    total_distance += distance\n",
    "\n",
    "                if total_distance < min_total_distance:\n",
    "                    min_total_distance = total_distance\n",
    "                    medoid = point1\n",
    "\n",
    "            if medoid is not None:\n",
    "                medoids.append(medoid)\n",
    "\n",
    "        return medoids\n",
    "\n",
    "    def cluster_analysis(file_path='sma_all_results.txt', min_trades=MIN_TRADES, max_trades=MAX_TRADES,\n",
    "                min_elements_per_cluster=MIN_ELEMENTS_PER_CLUSTER):\n",
    "        \"\"\"\n",
    "        Perform clustering analysis on SMA optimization results to find robust parameter regions\n",
    "        Now clusters based on short_SMA, long_SMA, and sharpe_ratio only (not trades)\n",
    "        \"\"\"\n",
    "        print(f\"\\n----- CLUSTER ANALYSIS -----\")\n",
    "        print(f\"Loading data from {file_path}...\")\n",
    "\n",
    "        # Load the data\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Convert data to numpy array for easier processing\n",
    "        X_full = df[['short_SMA', 'long_SMA', 'sharpe_ratio', 'trades']].values\n",
    "\n",
    "        # Filter data by number of trades and ensure short_SMA < long_SMA\n",
    "        X_filtered_full = X_full[(X_full[:, 0] < X_full[:, 1]) &  # short_SMA < long_SMA\n",
    "                    (X_full[:, 3] >= min_trades) &  # trades >= min_trades\n",
    "                    (X_full[:, 3] <= max_trades)]  # trades <= max_trades\n",
    "\n",
    "        if len(X_filtered_full) == 0:\n",
    "            print(\n",
    "                f\"No data points meet the criteria after filtering! Adjust min_trades ({min_trades}) and max_trades ({max_trades}).\")\n",
    "            return None, None, None, None, None\n",
    "\n",
    "        # Create a version with only the 3 dimensions for clustering\n",
    "        X_filtered = X_filtered_full[:, 0:3]  # Only short_SMA, long_SMA, and sharpe_ratio\n",
    "\n",
    "        print(f\"Filtered data to {len(X_filtered)} points with {min_trades}-{max_trades} trades\")\n",
    "\n",
    "        # Extract the fields for better scaling visibility\n",
    "        short_sma_values = X_filtered[:, 0]\n",
    "        long_sma_values = X_filtered[:, 1]\n",
    "        sharpe_values = X_filtered[:, 2]\n",
    "        trades_values = X_filtered_full[:, 3]\n",
    "\n",
    "        print(f\"Short SMA range: {short_sma_values.min()} to {short_sma_values.max()}\")\n",
    "        print(f\"Long SMA range: {long_sma_values.min()} to {long_sma_values.max()}\")\n",
    "        print(f\"Sharpe ratio range: {sharpe_values.min():.4f} to {sharpe_values.max():.4f}\")\n",
    "        print(f\"Trades range: {trades_values.min()} to {trades_values.max()}\")\n",
    "\n",
    "        # Scale the data for clustering - using StandardScaler for each dimension\n",
    "        # This addresses the issue where SMA values have much larger ranges than Sharpe ratio\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_filtered)  # Only scale the 3 dimensions we use for clustering\n",
    "\n",
    "        # Print scaling info for verification\n",
    "        print(\"\\nScaled data information:\")\n",
    "        scaled_short = X_scaled[:, 0]\n",
    "        scaled_long = X_scaled[:, 1]\n",
    "        scaled_sharpe = X_scaled[:, 2]\n",
    "\n",
    "        print(f\"Scaled Short SMA range: {scaled_short.min():.4f} to {scaled_short.max():.4f}\")\n",
    "        print(f\"Scaled Long SMA range: {scaled_long.min():.4f} to {scaled_long.max():.4f}\")\n",
    "        print(f\"Scaled Sharpe ratio range: {scaled_sharpe.min():.4f} to {scaled_sharpe.max():.4f}\")\n",
    "\n",
    "        # Determine number of clusters\n",
    "        print(f\"Using default number of clusters: {DEFAULT_NUM_CLUSTERS}\")\n",
    "        k = DEFAULT_NUM_CLUSTERS\n",
    "\n",
    "        # Apply KMeans clustering\n",
    "        print(f\"Performing KMeans clustering with k={k}...\")\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "\n",
    "        # Get cluster labels\n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        # Count elements per cluster\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        cluster_sizes = dict(zip(unique_labels, counts))\n",
    "\n",
    "        print(\"\\nCluster sizes:\")\n",
    "        for cluster_id, size in cluster_sizes.items():\n",
    "            print(f\"Cluster {cluster_id}: {size} elements\")\n",
    "\n",
    "        # Filter clusters with enough elements\n",
    "        valid_clusters = {i for i, count in cluster_sizes.items() if count >= min_elements_per_cluster}\n",
    "        filtered_indices = np.array([i in valid_clusters for i in labels])\n",
    "\n",
    "        # Filter data to only include points in valid clusters\n",
    "        X_valid = X_filtered_full[filtered_indices]  # Use full data to get trades too\n",
    "        labels_valid = labels[filtered_indices]\n",
    "\n",
    "        # Compute medoids using the existing method that expects 4D data\n",
    "        print(\"Computing medoids...\")\n",
    "        medoids = compute_medoids(X_valid, labels_valid, valid_clusters)\n",
    "\n",
    "        # Compute centroids (only for the 3 dimensions we clustered on)\n",
    "        centroids_scaled = kmeans.cluster_centers_\n",
    "        # Create a version that can be inverse-transformed (matches the original dimension count)\n",
    "        centroids = np.zeros((centroids_scaled.shape[0], 3))\n",
    "        centroids[:, 0:3] = scaler.inverse_transform(centroids_scaled)\n",
    "\n",
    "        # Print raw centroids for debugging\n",
    "        print(\"\\nCluster Centroids (in original space):\")\n",
    "        for i, centroid in enumerate(centroids):\n",
    "            if i in valid_clusters:  # Only show valid clusters\n",
    "                print(f\"Centroid {i}: Short SMA={centroid[0]:.2f}, Long SMA={centroid[1]:.2f}, \"\n",
    "                    f\"Sharpe={centroid[2]:.4f}\")\n",
    "\n",
    "        # Simply take top 5 medoids by Sharpe ratio\n",
    "        top_medoids = sorted(medoids, key=lambda x: float(x[2]), reverse=True)[:5]\n",
    "        \n",
    "        # Debug print the sorted top medoids\n",
    "        print(\"\\nSELECTED TOP 5 MEDOIDS BY SHARPE RATIO:\")\n",
    "        for idx, medoid in enumerate(top_medoids, 1):\n",
    "            print(f\"Top {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "                f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "        # Find max Sharpe ratio point overall\n",
    "        max_sharpe_idx = np.argmax(df['sharpe_ratio'].values)\n",
    "        max_sharpe_point = df.iloc[max_sharpe_idx][['short_SMA', 'long_SMA', 'sharpe_ratio', 'trades']].values\n",
    "\n",
    "        # Print results\n",
    "        print(\"\\n----- CLUSTERING RESULTS -----\")\n",
    "        print(f\"Max Sharpe point: Short SMA={int(max_sharpe_point[0])}, Long SMA={int(max_sharpe_point[1])}, \"\n",
    "            f\"Sharpe={max_sharpe_point[2]:.4f}, Trades={int(max_sharpe_point[3])}\")\n",
    "\n",
    "        print(\"\\nTop 5 Medoids (by Sharpe ratio):\")\n",
    "        for idx, medoid in enumerate(top_medoids, 1):\n",
    "            print(f\"Top {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "                f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "        # Create visualization with clustering results - pass the original labels and valid_clusters\n",
    "        create_cluster_visualization(X_filtered_full, medoids, top_medoids, centroids, max_sharpe_point, \n",
    "                                labels=labels, valid_clusters=valid_clusters)\n",
    "\n",
    "        return X_filtered_full, medoids, top_medoids, centroids, max_sharpe_point\n",
    "\n",
    "    def create_cluster_visualization(X_filtered_full, medoids, top_medoids, centroids, max_sharpe_point, labels=None, valid_clusters=None):\n",
    "        \"\"\"\n",
    "        Create a continuous heatmap visualization with cluster centers overlaid.\n",
    "        Only plots data points and clusters that meet the filtering criteria.\n",
    "        \n",
    "        Parameters:\n",
    "        X_filtered_full: array - Filter-compliant data points with shape (n_samples, 4) containing short_SMA, long_SMA, sharpe_ratio, trades\n",
    "        medoids: list - List of medoids from valid clusters\n",
    "        top_medoids: list - List of top medoids by Sharpe ratio\n",
    "        centroids: array - Centroids of clusters\n",
    "        max_sharpe_point: array - Point with maximum Sharpe ratio\n",
    "        labels: array - Cluster labels for each point in X_filtered_full (optional)\n",
    "        valid_clusters: set - Set of valid cluster IDs that meet the min_elements_per_cluster requirement (optional)\n",
    "        \"\"\"\n",
    "        print(\"Creating cluster visualization...\")\n",
    "\n",
    "        # Load the full dataset, but we'll only use it for creating the heatmap grid\n",
    "        data = pd.read_csv('sma_all_results.txt')\n",
    "        \n",
    "        # Create filtered dataframe from the filtered points that meet trade requirements\n",
    "        # This ensures we only visualize points that meet the trade count requirements\n",
    "        filtered_df = pd.DataFrame(X_filtered_full, columns=['short_SMA', 'long_SMA', 'sharpe_ratio', 'trades'])\n",
    "        \n",
    "        # Create a pivot table for the heatmap using ONLY the filtered data points\n",
    "        heatmap_data = filtered_df.pivot_table(\n",
    "            index='long_SMA',\n",
    "            columns='short_SMA',\n",
    "            values='sharpe_ratio',\n",
    "            fill_value=np.nan  # Use NaN for empty cells\n",
    "        )\n",
    "\n",
    "        # Create the heatmap visualization\n",
    "        plt.figure(figsize=(12, 10))\n",
    "\n",
    "        # Create mask for invalid combinations (where short_SMA >= long_SMA)\n",
    "        # and also for NaN values (which represent filtered-out points)\n",
    "        mask = np.zeros_like(heatmap_data, dtype=bool)\n",
    "        for i, long_sma in enumerate(heatmap_data.index):\n",
    "            for j, short_sma in enumerate(heatmap_data.columns):\n",
    "                if short_sma >= long_sma or np.isnan(heatmap_data.iloc[i, j]):\n",
    "                    mask[i, j] = True\n",
    "\n",
    "        # Plot the base heatmap with ONLY filtered data\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_data,\n",
    "            mask=mask,\n",
    "            cmap='coolwarm',  # Blue to red colormap\n",
    "            annot=False,      # Don't annotate each cell with its value\n",
    "            fmt='.4f',\n",
    "            linewidths=0,\n",
    "            cbar_kws={'label': 'Sharpe Ratio'}\n",
    "        )\n",
    "\n",
    "        # Invert the y-axis so smaller long_SMA values are at the top\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        # Plot max Sharpe point (Green Star)\n",
    "        # Note: We need to convert from data values to plot coordinates\n",
    "        try:\n",
    "            best_x_pos = np.where(heatmap_data.columns == max_sharpe_point[0])[0][0] + 0.5\n",
    "            best_y_pos = np.where(heatmap_data.index == max_sharpe_point[1])[0][0] + 0.5\n",
    "            plt.scatter(best_x_pos, best_y_pos, marker='*', color='lime', s=200,\n",
    "                        edgecolor='black', zorder=5)\n",
    "        except IndexError:\n",
    "            print(f\"Warning: Max Sharpe point at ({max_sharpe_point[0]}, {max_sharpe_point[1]}) not found in heatmap coordinates\")\n",
    "\n",
    "        # Only plot medoids from valid clusters\n",
    "        if medoids:\n",
    "            for medoid in medoids:\n",
    "                try:\n",
    "                    x_pos = np.where(heatmap_data.columns == medoid[0])[0][0] + 0.5\n",
    "                    y_pos = np.where(heatmap_data.index == medoid[1])[0][0] + 0.5\n",
    "                    plt.scatter(x_pos, y_pos, marker='s', color='black', s=75, zorder=4)\n",
    "                except IndexError:\n",
    "                    print(f\"Warning: Medoid at ({medoid[0]}, {medoid[1]}) not found in heatmap coordinates\")\n",
    "\n",
    "        # Plot top 5 medoids (Purple Diamonds)\n",
    "        if top_medoids:\n",
    "            for medoid in top_medoids:\n",
    "                try:\n",
    "                    x_pos = np.where(heatmap_data.columns == medoid[0])[0][0] + 0.5\n",
    "                    y_pos = np.where(heatmap_data.index == medoid[1])[0][0] + 0.5\n",
    "                    plt.scatter(x_pos, y_pos, marker='D', color='purple', s=100, zorder=5)\n",
    "                except IndexError:\n",
    "                    print(f\"Warning: Top medoid at ({medoid[0]}, {medoid[1]}) not found in heatmap coordinates\")\n",
    "\n",
    "        # Only plot centroids from valid clusters\n",
    "        print(f\"Plotting centroids from valid clusters...\")\n",
    "        centroids_plotted = 0\n",
    "\n",
    "        # If valid_clusters is not provided, assume all centroids are valid\n",
    "        plot_centroids = centroids\n",
    "        if valid_clusters is not None and labels is not None:\n",
    "            # Only plot centroids from valid clusters\n",
    "            plot_centroids = [centroids[i] for i in range(len(centroids)) if i in valid_clusters]\n",
    "            print(f\"Filtering centroids to only include valid clusters: {valid_clusters}\")\n",
    "        \n",
    "        for i, centroid in enumerate(plot_centroids):\n",
    "            # Get the actual raw centroid values\n",
    "            short_sma = centroid[0]\n",
    "            long_sma = centroid[1]\n",
    "\n",
    "            print(f\"Centroid {i}: raw values = ({short_sma}, {long_sma})\")\n",
    "\n",
    "            # First try exact values\n",
    "            try:\n",
    "                if (short_sma in heatmap_data.columns) and (long_sma in heatmap_data.index) and (short_sma < long_sma):\n",
    "                    x_pos = np.where(heatmap_data.columns == short_sma)[0][0] + 0.5\n",
    "                    y_pos = np.where(heatmap_data.index == long_sma)[0][0] + 0.5\n",
    "                    plt.scatter(x_pos, y_pos, marker='o', color='blue', s=75, zorder=4)\n",
    "                    centroids_plotted += 1\n",
    "                    continue\n",
    "            except (IndexError, TypeError):\n",
    "                pass\n",
    "\n",
    "            # Try rounded values\n",
    "            try:\n",
    "                short_sma_rounded = int(round(short_sma))\n",
    "                long_sma_rounded = int(round(long_sma))\n",
    "\n",
    "                print(f\"  Rounded: ({short_sma_rounded}, {long_sma_rounded})\")\n",
    "\n",
    "                if (short_sma_rounded in heatmap_data.columns) and (long_sma_rounded in heatmap_data.index) and (\n",
    "                        short_sma_rounded < long_sma_rounded):\n",
    "                    x_pos = np.where(heatmap_data.columns == short_sma_rounded)[0][0] + 0.5\n",
    "                    y_pos = np.where(heatmap_data.index == long_sma_rounded)[0][0] + 0.5\n",
    "                    plt.scatter(x_pos, y_pos, marker='o', color='blue', s=75, zorder=4)\n",
    "                    centroids_plotted += 1\n",
    "                    continue\n",
    "            except (IndexError, TypeError):\n",
    "                pass\n",
    "\n",
    "            # Try finding nearest valid point\n",
    "            try:\n",
    "                # Find nearest valid parameter values\n",
    "                short_options = np.array(heatmap_data.columns)\n",
    "                long_options = np.array(heatmap_data.index)\n",
    "\n",
    "                # Find nearest short SMA\n",
    "                short_idx = np.argmin(np.abs(short_options - short_sma))\n",
    "                short_nearest = short_options[short_idx]\n",
    "\n",
    "                # Find nearest long SMA\n",
    "                long_idx = np.argmin(np.abs(long_options - long_sma))\n",
    "                long_nearest = long_options[long_idx]\n",
    "\n",
    "                print(f\"  Nearest: ({short_nearest}, {long_nearest})\")\n",
    "\n",
    "                # Check if valid\n",
    "                if short_nearest < long_nearest:\n",
    "                    x_pos = np.where(heatmap_data.columns == short_nearest)[0][0] + 0.5\n",
    "                    y_pos = np.where(heatmap_data.index == long_nearest)[0][0] + 0.5\n",
    "                    plt.scatter(x_pos, y_pos, marker='o', color='blue', s=75, zorder=4, alpha=0.7)\n",
    "                    centroids_plotted += 1\n",
    "                else:\n",
    "                    print(f\"  Invalid nearest parameters (short >= long): {short_nearest} >= {long_nearest}\")\n",
    "            except (IndexError, TypeError) as e:\n",
    "                print(f\"  Error finding nearest point: {e}\")\n",
    "\n",
    "        print(f\"Successfully plotted {centroids_plotted} out of {len(plot_centroids)} centroids\")\n",
    "\n",
    "        # Create custom legend\n",
    "        max_sharpe_handle = mlines.Line2D([], [], color='lime', marker='*', linestyle='None',\n",
    "                                        markersize=15, markeredgecolor='black', label='Max Sharpe')\n",
    "        medoid_handle = mlines.Line2D([], [], color='black', marker='s', linestyle='None',\n",
    "                                    markersize=10, label='Medoids')\n",
    "        top_medoid_handle = mlines.Line2D([], [], color='purple', marker='D', linestyle='None',\n",
    "                                        markersize=10, label='Top 5 Medoids')\n",
    "        centroid_handle = mlines.Line2D([], [], color='blue', marker='o', linestyle='None',\n",
    "                                        markersize=10, label='Centroids')\n",
    "\n",
    "        # Add legend\n",
    "        plt.legend(handles=[max_sharpe_handle, medoid_handle, top_medoid_handle, centroid_handle],\n",
    "                loc='best')\n",
    "\n",
    "        # Set labels and title\n",
    "        plt.title('SMA Parameter Clustering Analysis (Sharpe Ratio)', fontsize=14)\n",
    "        plt.xlabel('Short SMA (days)', fontsize=12)\n",
    "        plt.ylabel('Long SMA (days)', fontsize=12)\n",
    "\n",
    "        # Rotate tick labels\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "\n",
    "        # Display plot\n",
    "        plt.tight_layout()\n",
    "        save_plot('Cluster_Analysis.png')\n",
    "\n",
    "    # Load the SMA simulation results\n",
    "    def analyze_sma_results(file_path='sma_all_results.txt'):\n",
    "        print(f\"Loading simulation results from {file_path}...\")\n",
    "\n",
    "        # Load the data from the CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Print basic information about the data\n",
    "        print(f\"Loaded {len(data)} simulation results\")\n",
    "        print(f\"Short SMA range: {data['short_SMA'].min()} to {data['short_SMA'].max()}\")\n",
    "        print(f\"Long SMA range: {data['long_SMA'].min()} to {data['long_SMA'].max()}\")\n",
    "        print(f\"Sharpe ratio range: {data['sharpe_ratio'].min():.4f} to {data['sharpe_ratio'].max():.4f}\")\n",
    "\n",
    "        # Find the best Sharpe ratio\n",
    "        best_idx = data['sharpe_ratio'].idxmax()\n",
    "        best_short_sma = data.loc[best_idx, 'short_SMA']\n",
    "        best_long_sma = data.loc[best_idx, 'long_SMA']\n",
    "        best_sharpe = data.loc[best_idx, 'sharpe_ratio']\n",
    "        best_trades = data.loc[best_idx, 'trades']\n",
    "\n",
    "        print(f\"\\nBest parameters:\")\n",
    "        print(f\"Short SMA: {best_short_sma}\")\n",
    "        print(f\"Long SMA: {best_long_sma}\")\n",
    "        print(f\"Sharpe Ratio: {best_sharpe:.6f}\")\n",
    "        print(f\"Number of Trades: {best_trades}\")\n",
    "\n",
    "        # Create a pivot table for the heatmap\n",
    "        heatmap_data = data.pivot_table(\n",
    "            index='long_SMA',\n",
    "            columns='short_SMA',\n",
    "            values='sharpe_ratio'\n",
    "        )\n",
    "\n",
    "        # Create the heatmap visualization\n",
    "        plt.figure(figsize=(12, 10))\n",
    "\n",
    "        # Create a mask for invalid combinations (where short_SMA >= long_SMA)\n",
    "        mask = np.zeros_like(heatmap_data, dtype=bool)\n",
    "        for i, long_sma in enumerate(heatmap_data.index):\n",
    "            for j, short_sma in enumerate(heatmap_data.columns):\n",
    "                if short_sma >= long_sma:\n",
    "                    mask[i, j] = True\n",
    "\n",
    "        # Plot the heatmap with the mask\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_data,\n",
    "            mask=mask,\n",
    "            cmap='coolwarm',  # Blue to red colormap\n",
    "            annot=False,  # Don't annotate each cell with its value\n",
    "            fmt='.4f',\n",
    "            linewidths=0,\n",
    "            cbar_kws={'label': 'Sharpe Ratio'}\n",
    "        )\n",
    "\n",
    "        # Invert the y-axis so smaller long_SMA values are at the top\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        # Find the position of the best Sharpe ratio in the heatmap\n",
    "        best_y = heatmap_data.index.get_loc(best_long_sma)\n",
    "        best_x = heatmap_data.columns.get_loc(best_short_sma)\n",
    "\n",
    "        # Add a star to mark the best Sharpe ratio\n",
    "        # We need to add 0.5 to center the marker in the cell\n",
    "        ax.add_patch(Circle((best_x + 0.5, best_y + 0.5), 0.4, facecolor='none',\n",
    "                            edgecolor='white', lw=2))\n",
    "        plt.plot(best_x + 0.5, best_y + 0.5, 'w*', markersize=10)\n",
    "\n",
    "        # Set labels and title\n",
    "        plt.title(f'SMA Optimization Heatmap (Best Sharpe: {best_sharpe:.4f} at {best_short_sma}/{best_long_sma})',\n",
    "                fontsize=14)\n",
    "        plt.xlabel('Short SMA (days)', fontsize=12)\n",
    "        plt.ylabel('Long SMA (days)', fontsize=12)\n",
    "\n",
    "        # Rotate tick labels for better readability\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "\n",
    "        # Add a text annotation for the best parameters\n",
    "        plt.annotate(\n",
    "            f'Best: Short={best_short_sma}, Long={best_long_sma}\\nSharpe={best_sharpe:.4f}, Trades={best_trades}',\n",
    "            xy=(best_x + 0.5, best_y + 0.5),\n",
    "            xytext=(best_x + 5, best_y + 5),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color='white'),\n",
    "            color='white',\n",
    "            backgroundcolor='black',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"black\", alpha=0.7)\n",
    "        )\n",
    "\n",
    "        # Display the plot\n",
    "        plt.tight_layout()\n",
    "        save_plot('Heatmap.png')\n",
    "\n",
    "        # Return the data and best parameters\n",
    "        return data, best_short_sma, best_long_sma, best_sharpe, best_trades\n",
    "\n",
    "\n",
    "    def plot_strategy_performance(short_sma, long_sma, top_medoids=None, big_point_value=1, slippage=0, capital=1000000, atr_period=14):\n",
    "        print(f\"\\n----- PLOTTING STRATEGY PERFORMANCE -----\")\n",
    "        print(f\"Using Short SMA: {short_sma}, Long SMA: {long_sma}\")\n",
    "        print(f\"Trading with ATR-based position sizing (Capital: ${capital:,}, ATR Period: {atr_period})\")\n",
    "        if top_medoids:\n",
    "            print(f\"Including top {len(top_medoids)} medoids\")\n",
    "\n",
    "        # Load data from local file\n",
    "        print(f\"Loading {TICKER} data from local files...\")\n",
    "        data_file = find_futures_file(SYMBOL, DATA_DIR)\n",
    "        if not data_file:\n",
    "            print(f\"Error: No data file found for {TICKER} in {DATA_DIR}\")\n",
    "            exit(1)\n",
    "\n",
    "        print(f\"Found data file: {os.path.basename(data_file)}\")\n",
    "        print(f\"File size: {os.path.getsize(data_file)} bytes\")\n",
    "\n",
    "        # Load the data from local file\n",
    "        all_data = read_ts.read_ts_ohlcv_dat(data_file)\n",
    "        data_obj = all_data[0]\n",
    "        ohlc_data = data_obj.data.copy()\n",
    "\n",
    "        # Convert the OHLCV data to the format expected by the strategy\n",
    "        data = ohlc_data.rename(columns={\n",
    "            'datetime': 'Date',\n",
    "            'open': 'Open',\n",
    "            'high': 'High',\n",
    "            'low': 'Low',\n",
    "            'close': 'Close',\n",
    "            'volume': 'Volume'\n",
    "        })\n",
    "        data.set_index('Date', inplace=True)\n",
    "\n",
    "        # Add a warm-up period before the start date\n",
    "        original_start_idx = None\n",
    "        if START_DATE and END_DATE:\n",
    "            # Calculate warm-up period for SMA calculation (longest SMA + buffer)\n",
    "            warm_up_days = max(short_sma, long_sma) * 3  # Use 3x the longest SMA as warm-up\n",
    "            \n",
    "            # Convert dates to datetime\n",
    "            start_date = pd.to_datetime(START_DATE)\n",
    "            end_date = pd.to_datetime(END_DATE)\n",
    "            \n",
    "            # Adjust start date for warm-up\n",
    "            adjusted_start = start_date - pd.Timedelta(days=warm_up_days)\n",
    "            \n",
    "            # Filter with the extended date range\n",
    "            extended_data = data[(data.index >= adjusted_start) & (data.index <= end_date)]\n",
    "            \n",
    "            # Store the index where the actual analysis should start\n",
    "            if not extended_data.empty:\n",
    "                # Find the closest index to our original start date\n",
    "                original_start_idx = extended_data.index.get_indexer([start_date], method='nearest')[0]\n",
    "            \n",
    "            print(f\"Added {warm_up_days} days warm-up period before {START_DATE}\")\n",
    "            data = extended_data\n",
    "        \n",
    "        # Create a dictionary to store results for each strategy\n",
    "        strategies = {\n",
    "            'Best': {'short_sma': short_sma, 'long_sma': long_sma}\n",
    "        }\n",
    "\n",
    "        # Add medoids in their original order, including original Sharpe and Trades\n",
    "        if top_medoids:\n",
    "            print(\"\\nUSING THESE MEDOIDS (IN ORIGINAL ORDER):\")\n",
    "            for i, medoid in enumerate(top_medoids, 1):\n",
    "                strategies[f'Medoid {i}'] = {\n",
    "                    'short_sma': int(medoid[0]),\n",
    "                    'long_sma': int(medoid[1]),\n",
    "                    'original_sharpe': float(medoid[2]),  # Store the original Sharpe ratio\n",
    "                    'original_trades': int(medoid[3])     # Store the original number of trades\n",
    "                }\n",
    "                print(f\"Medoid {i}: SMA({int(medoid[0])}/{int(medoid[1])}) - Original Sharpe: {float(medoid[2]):.4f}, Trades: {int(medoid[3])}\")\n",
    "\n",
    "        # Apply the proper strategy for each parameter set\n",
    "        for name, params in strategies.items():\n",
    "            # Calculate centered SMAs directly\n",
    "            data[f'SMA_Short_{name}'] = data['Close'].rolling(window=params['short_sma'], center=True).mean()\n",
    "            data[f'SMA_Long_{name}'] = data['Close'].rolling(window=params['long_sma'], center=True).mean()\n",
    "            \n",
    "            # Create a strategy instance for each parameter set\n",
    "            sma_strategy = SMAStrategy(\n",
    "                short_sma=params['short_sma'],\n",
    "                long_sma=params['long_sma'],\n",
    "                big_point_value=big_point_value,\n",
    "                slippage=slippage,\n",
    "                capital=capital,\n",
    "                atr_period=atr_period\n",
    "            )\n",
    "\n",
    "            # Apply the strategy\n",
    "            data = sma_strategy.apply_strategy(\n",
    "                data.copy(),\n",
    "                strategy_name=name\n",
    "            )\n",
    "\n",
    "        # Trim data to the original date range if we added warm-up period\n",
    "        if original_start_idx is not None:\n",
    "            data_for_evaluation = data.iloc[original_start_idx:]\n",
    "            print(f\"Trimmed warm-up period. Original data length: {len(data)}, Evaluation data length: {len(data_for_evaluation)}\")\n",
    "        else:\n",
    "            data_for_evaluation = data\n",
    "        \n",
    "        # Calculate split index for in-sample/out-of-sample using the trimmed data\n",
    "        split_index = int(len(data_for_evaluation) * TRAIN_TEST_SPLIT)\n",
    "        split_date = data_for_evaluation.index[split_index]\n",
    "\n",
    "        # Create color palette for strategies\n",
    "        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink']\n",
    "\n",
    "        # Create the performance visualization with just two panels\n",
    "        plt.figure(figsize=(14, 12))\n",
    "\n",
    "        # Plot price and SMA (first subplot)\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(data_for_evaluation.index, data_for_evaluation['Close'], label=f'{SYMBOL} Price', color='black', alpha=0.5)\n",
    "        \n",
    "        # Use the centered SMAs for plotting\n",
    "        for name, params in strategies.items():\n",
    "            if name == 'Best':\n",
    "                plt.plot(data_for_evaluation.index, data_for_evaluation[f'SMA_Short_{name}'], \n",
    "                        label=f'Short SMA ({params[\"short_sma\"]})', color='orange')\n",
    "                plt.plot(data_for_evaluation.index, data_for_evaluation[f'SMA_Long_{name}'], \n",
    "                        label=f'Long SMA ({params[\"long_sma\"]})', color='blue')\n",
    "        \n",
    "        # Mark the train/test split\n",
    "        plt.axvline(x=split_date, color='black', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title(f'{SYMBOL} Price and SMA Indicators')\n",
    "        plt.ylabel('Price')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot cumulative P&L (second subplot)\n",
    "        plt.subplot(2, 1, 2)\n",
    "\n",
    "        for i, (name, params) in enumerate(strategies.items()):\n",
    "            color = colors[i % len(colors)]\n",
    "\n",
    "            # Plot full period P&L\n",
    "            plt.plot(data_for_evaluation.index, data_for_evaluation[f'Cumulative_PnL_{name}'],\n",
    "                    label=f'{name} ({params[\"short_sma\"]}/{params[\"long_sma\"]})', color=color)\n",
    "\n",
    "            # Plot out-of-sample portion with thicker line\n",
    "            plt.plot(data_for_evaluation.index[split_index:], data_for_evaluation[f'Cumulative_PnL_{name}'].iloc[split_index:],\n",
    "                    color=color, linewidth=2.5, alpha=0.7)\n",
    "\n",
    "        plt.axvline(x=split_date, color='black', linestyle='--',\n",
    "                    label=f'Train/Test Split ({int(TRAIN_TEST_SPLIT * 100)}%/{int((1 - TRAIN_TEST_SPLIT) * 100)}%)')\n",
    "        plt.axhline(y=0.0, color='gray', linestyle='-', alpha=0.5, label='Break-even')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title('Strategy Cumulative P&L')\n",
    "        plt.ylabel('P&L ($)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_plot('Multiple_Strategy_Plots.png')\n",
    "\n",
    "        # Create a list to store performance data for saving to file\n",
    "        performance_data = []\n",
    "\n",
    "        # Print detailed performance metrics for all strategies with in-sample and out-of-sample breakdown\n",
    "        print(\"\\n----- PERFORMANCE SUMMARY -----\")\n",
    "\n",
    "        # IN-SAMPLE PERFORMANCE\n",
    "        print(\"\\nIN-SAMPLE PERFORMANCE:\")\n",
    "        header = f\"{'Strategy':<10} | {'Short/Long':<10} | {'P&L':>12} | {'Sharpe':>7} | {'Trades':>6}\"\n",
    "        separator = \"-\" * len(header)\n",
    "        print(separator)\n",
    "        print(header)\n",
    "        print(separator)\n",
    "\n",
    "        # Calculate in-sample metrics using trimmed data\n",
    "        for name, params in strategies.items():\n",
    "            short = params['short_sma']\n",
    "            long = params['long_sma']\n",
    "\n",
    "            # Get in-sample data from trimmed dataset\n",
    "            in_sample = data_for_evaluation.iloc[:split_index]\n",
    "\n",
    "            # Calculate in-sample metrics\n",
    "            in_sample_daily_pnl = in_sample[f'Daily_PnL_{name}']\n",
    "            in_sample_cumulative_pnl = in_sample[f'Daily_PnL_{name}'].sum()\n",
    "            \n",
    "            # Calculate average position size for in-sample\n",
    "            avg_pos_size = in_sample[f'Position_Size_{name}'].mean()\n",
    "\n",
    "            # Use original Sharpe and Trades for medoids, calculate for Best\n",
    "            if name.startswith('Medoid'):\n",
    "                in_sample_sharpe = params['original_sharpe']\n",
    "                in_sample_trades = params['original_trades']\n",
    "            else:\n",
    "                # Calculate Sharpe ratio (annualized) for Best\n",
    "                if in_sample_daily_pnl.std() > 0:\n",
    "                    in_sample_sharpe = in_sample_daily_pnl.mean() / in_sample_daily_pnl.std() * np.sqrt(252)\n",
    "                else:\n",
    "                    in_sample_sharpe = 0\n",
    "                # Count in-sample trades for Best\n",
    "                in_sample_trades = in_sample[f'Position_Change_{name}'].sum()\n",
    "            \n",
    "            # Print in the original order\n",
    "            row = f\"{name:<10} | {short:>4}/{long:<5} | ${in_sample_cumulative_pnl:>10,.2f} | {in_sample_sharpe:>6.3f} | {in_sample_trades:>6}\"\n",
    "            print(row)\n",
    "            \n",
    "            # Store data for later use\n",
    "            performance_data.append({\n",
    "                'Period': 'In-Sample',\n",
    "                'Strategy': name,\n",
    "                'Short_SMA': short,\n",
    "                'Long_SMA': long,\n",
    "                'PnL': in_sample_cumulative_pnl,\n",
    "                'Avg_Position': avg_pos_size,\n",
    "                'Sharpe': in_sample_sharpe,\n",
    "                'Trades': in_sample_trades\n",
    "            })\n",
    "\n",
    "        print(separator)\n",
    "\n",
    "        # OUT-OF-SAMPLE PERFORMANCE\n",
    "        print(\"\\nOUT-OF-SAMPLE PERFORMANCE:\")\n",
    "        print(separator)\n",
    "        print(header)\n",
    "        print(separator)\n",
    "\n",
    "        # Calculate out-of-sample metrics using trimmed data\n",
    "        for name, params in strategies.items():\n",
    "            short = params['short_sma']\n",
    "            long = params['long_sma']\n",
    "\n",
    "            # Get out-of-sample data from trimmed dataset\n",
    "            out_sample = data_for_evaluation.iloc[split_index:]\n",
    "\n",
    "            # Calculate out-of-sample metrics\n",
    "            out_sample_daily_pnl = out_sample[f'Daily_PnL_{name}']\n",
    "            out_sample_cumulative_pnl = out_sample[f'Daily_PnL_{name}'].sum()\n",
    "            \n",
    "            # Calculate average position size for out-of-sample\n",
    "            avg_pos_size = out_sample[f'Position_Size_{name}'].mean()\n",
    "\n",
    "            # Calculate Sharpe ratio (annualized)\n",
    "            if out_sample_daily_pnl.std() > 0:\n",
    "                out_sample_sharpe = out_sample_daily_pnl.mean() / out_sample_daily_pnl.std() * np.sqrt(252)\n",
    "            else:\n",
    "                out_sample_sharpe = 0\n",
    "\n",
    "            # Count out-of-sample trades\n",
    "            out_sample_trades = out_sample[f'Position_Change_{name}'].sum()\n",
    "            \n",
    "            # Print in the original order\n",
    "            row = f\"{name:<10} | {short:>4}/{long:<5} | ${out_sample_cumulative_pnl:>10,.2f} | {out_sample_sharpe:>6.3f} | {out_sample_trades:>6}\"\n",
    "            print(row)\n",
    "            \n",
    "            # Store data for later use\n",
    "            performance_data.append({\n",
    "                'Period': 'Out-of-Sample',\n",
    "                'Strategy': name,\n",
    "                'Short_SMA': short,\n",
    "                'Long_SMA': long,\n",
    "                'PnL': out_sample_cumulative_pnl,\n",
    "                'Avg_Position': avg_pos_size,\n",
    "                'Sharpe': out_sample_sharpe,\n",
    "                'Trades': out_sample_trades\n",
    "            })\n",
    "\n",
    "        print(separator)\n",
    "\n",
    "        # FULL PERIOD PERFORMANCE\n",
    "        print(\"\\nFULL PERIOD PERFORMANCE:\")\n",
    "        print(separator)\n",
    "        print(header)\n",
    "        print(separator)\n",
    "\n",
    "        # Calculate full period metrics using trimmed data\n",
    "        for name, params in strategies.items():\n",
    "            short = params['short_sma']\n",
    "            long = params['long_sma']\n",
    "\n",
    "            # Calculate full period metrics using trimmed data\n",
    "            full_daily_pnl = data_for_evaluation[f'Daily_PnL_{name}']\n",
    "            full_cumulative_pnl = full_daily_pnl.sum()\n",
    "            \n",
    "            # Calculate average position size for full period\n",
    "            avg_pos_size = data_for_evaluation[f'Position_Size_{name}'].mean()\n",
    "            max_pos_size = data_for_evaluation[f'Position_Size_{name}'].max()\n",
    "\n",
    "            # Calculate Sharpe ratio (annualized)\n",
    "            if full_daily_pnl.std() > 0:\n",
    "                full_sharpe = full_daily_pnl.mean() / full_daily_pnl.std() * np.sqrt(252)\n",
    "            else:\n",
    "                full_sharpe = 0\n",
    "\n",
    "            # Count full period trades\n",
    "            full_trades = data_for_evaluation[f'Position_Change_{name}'].sum()\n",
    "            \n",
    "            # Print in the original order\n",
    "            row = f\"{name:<10} | {short:>4}/{long:<5} | ${full_cumulative_pnl:>10,.2f} | {full_sharpe:>6.3f} | {full_trades:>6}\"\n",
    "            print(row)\n",
    "            \n",
    "            # Store data for later use\n",
    "            performance_data.append({\n",
    "                'Period': 'Full',\n",
    "                'Strategy': name,\n",
    "                'Short_SMA': short,\n",
    "                'Long_SMA': long,\n",
    "                'PnL': full_cumulative_pnl,\n",
    "                'Avg_Position': avg_pos_size,\n",
    "                'Max_Position': max_pos_size,\n",
    "                'Sharpe': full_sharpe,\n",
    "                'Trades': full_trades\n",
    "            })\n",
    "            \n",
    "            # Additional metrics for the best strategy\n",
    "            if name == 'Best':\n",
    "                print(f\"\\nAdditional metrics for Best strategy:\")\n",
    "                print(f\"Maximum position size: {max_pos_size:.2f} contracts\")\n",
    "                print(f\"Average ATR value: {data_for_evaluation['ATR_Best'].mean():.4f}\")\n",
    "                \n",
    "                # Calculate drawdown using trimmed data\n",
    "                peak = data_for_evaluation[f'Cumulative_PnL_{name}'].cummax()\n",
    "                drawdown = data_for_evaluation[f'Cumulative_PnL_{name}'] - peak\n",
    "                max_drawdown = drawdown.min()\n",
    "                \n",
    "                print(f\"Maximum drawdown: ${max_drawdown:.2f}\")\n",
    "                \n",
    "                # Calculate win rate using trimmed data\n",
    "                daily_win_rate = (data_for_evaluation[f'Daily_PnL_{name}'] > 0).mean() * 100\n",
    "                print(f\"Daily win rate: {daily_win_rate:.2f}%\")\n",
    "\n",
    "        print(separator)\n",
    "\n",
    "        # Save performance data to CSV in the ticker output directory\n",
    "        performance_df = pd.DataFrame(performance_data)\n",
    "        save_results(performance_df, f\"{SYMBOL}_performance_summary.csv\")\n",
    "\n",
    "        return data_for_evaluation  # Return the trimmed data instead of full data\n",
    "\n",
    "\n",
    "    def bimonthly_out_of_sample_comparison(data, best_short_sma, best_long_sma, top_medoids, min_sharpe=0.2, \n",
    "                                big_point_value=big_point_value, slippage=slippage,\n",
    "                                capital=capital, atr_period=atr_period):\n",
    "        \"\"\"\n",
    "        Compare bimonthly (2-month) performance between the best Sharpe strategy and a portfolio of top medoids\n",
    "        using ATR-based position sizing.\n",
    "        \n",
    "        Parameters:\n",
    "        data: DataFrame with market data\n",
    "        best_short_sma: int - The short SMA period for the best Sharpe strategy\n",
    "        best_long_sma: int - The long SMA period for the best Sharpe strategy\n",
    "        top_medoids: list - List of top medoids, each as (short_sma, long_sma, sharpe, trades)\n",
    "        min_sharpe: float - Minimum Sharpe ratio threshold for medoids to be included\n",
    "        big_point_value: float - Big point value for the futures contract\n",
    "        slippage: float - Slippage value in price units\n",
    "        capital: float - Capital allocation for position sizing\n",
    "        atr_period: int - Period for ATR calculation\n",
    "        \"\"\"\n",
    "        print(f\"\\n----- BIMONTHLY OUT-OF-SAMPLE COMPARISON -----\")\n",
    "        print(f\"Best Sharpe: ({best_short_sma}/{best_long_sma})\")\n",
    "        print(f\"Using ATR-based position sizing (Capital: ${capital:,}, ATR Period: {atr_period})\")\n",
    "        \n",
    "        # Handle the case where top_medoids is None\n",
    "        if top_medoids is None:\n",
    "            print(\"No medoids provided. Comparison cannot be performed.\")\n",
    "            return None\n",
    "        \n",
    "        # The top_medoids are already sorted by Sharpe ratio in cluster_analysis\n",
    "        # Just print them for verification\n",
    "        print(\"\\nUSING TOP MEDOIDS (BY SHARPE RATIO):\")\n",
    "        for i, medoid in enumerate(top_medoids, 1):\n",
    "            print(f\"Medoid {i}: ({int(medoid[0])}/{int(medoid[1])}) - Sharpe: {float(medoid[2]):.4f}, Trades: {int(medoid[3])}\")\n",
    "        \n",
    "        # Take at most 3 medoids and filter by minimum Sharpe\n",
    "        filtered_medoids = []\n",
    "        for i, m in enumerate(top_medoids[:3]):\n",
    "            # Check if we can access the required elements\n",
    "            try:\n",
    "                # Extract Sharpe ratio and check if it meets the threshold\n",
    "                short_sma = m[0]\n",
    "                long_sma = m[1]\n",
    "                sharpe = float(m[2])  # Convert to float to handle numpy types\n",
    "                trades = m[3]\n",
    "                \n",
    "                if sharpe >= min_sharpe:\n",
    "                    filtered_medoids.append(m)\n",
    "                    print(f\"Selected medoid {i+1} with Sharpe {sharpe:.4f}\")\n",
    "            except (IndexError, TypeError) as e:\n",
    "                print(f\"Error processing medoid: {e}\")\n",
    "        \n",
    "        if not filtered_medoids:\n",
    "            print(f\"No medoids have a Sharpe ratio >= {min_sharpe}. Comparison cannot be performed.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Creating portfolio of {len(filtered_medoids)} medoids with Sharpe ratio >= {min_sharpe}:\")\n",
    "        for i, medoid in enumerate(filtered_medoids, 1):\n",
    "            print(f\"Final Medoid {i}: ({int(medoid[0])}/{int(medoid[1])}) - Sharpe: {float(medoid[2]):.4f}\")\n",
    "        \n",
    "        # Download historical data if not already provided\n",
    "        if data is None:\n",
    "            # Load data from local file\n",
    "            print(f\"Loading {TICKER} data from local files...\")\n",
    "            data_file = find_futures_file(SYMBOL, DATA_DIR)\n",
    "            if not data_file:\n",
    "                print(f\"Error: No data file found for {TICKER} in {DATA_DIR}\")\n",
    "                exit(1)\n",
    "            \n",
    "            print(f\"Found data file: {os.path.basename(data_file)}\")\n",
    "            print(f\"File size: {os.path.getsize(data_file)} bytes\")\n",
    "\n",
    "            # Load the data from local file\n",
    "            all_data = read_ts.read_ts_ohlcv_dat(data_file)\n",
    "            data_obj = all_data[0]\n",
    "            ohlc_data = data_obj.data.copy()\n",
    "\n",
    "            # Convert the OHLCV data to the format expected by the strategy\n",
    "            data = ohlc_data.rename(columns={\n",
    "                'datetime': 'Date',\n",
    "                'open': 'Open',\n",
    "                'high': 'High',\n",
    "                'low': 'Low',\n",
    "                'close': 'Close',\n",
    "                'volume': 'Volume'\n",
    "            })\n",
    "            data.set_index('Date', inplace=True)\n",
    "            \n",
    "            # Filter data to match the date range if specified in input.py\n",
    "            if START_DATE and END_DATE:\n",
    "                data = data[(data.index >= pd.to_datetime(START_DATE)) & (data.index <= pd.to_datetime(END_DATE))]\n",
    "                print(f\"Filtered data to date range: {START_DATE} to {END_DATE}\")\n",
    "        \n",
    "        # Create strategies\n",
    "        strategies = {\n",
    "            'Best': {'short_sma': best_short_sma, 'long_sma': best_long_sma}\n",
    "        }\n",
    "        \n",
    "        # Add filtered medoids\n",
    "        for i, medoid in enumerate(filtered_medoids, 1):\n",
    "            strategies[f'Medoid_{i}'] = {'short_sma': int(medoid[0]), 'long_sma': int(medoid[1])}\n",
    "        \n",
    "        # Apply each strategy to the data\n",
    "        for name, params in strategies.items():\n",
    "            strategy = SMAStrategy(\n",
    "                short_sma=params['short_sma'],\n",
    "                long_sma=params['long_sma'],\n",
    "                big_point_value=big_point_value,\n",
    "                slippage=slippage,\n",
    "                capital=capital,\n",
    "                atr_period=atr_period\n",
    "            )\n",
    "            \n",
    "            # Apply the strategy\n",
    "            data = strategy.apply_strategy(\n",
    "                data.copy(),\n",
    "                strategy_name=name\n",
    "            )\n",
    "        \n",
    "        # Get the out-of-sample split date\n",
    "        split_index = int(len(data) * TRAIN_TEST_SPLIT)\n",
    "        split_date = data.index[split_index]\n",
    "        print(f\"Out-of-sample period starts on: {split_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        # Get out-of-sample data\n",
    "        oos_data = data.iloc[split_index:].copy()\n",
    "        \n",
    "        # Add a year and bimonthly period columns for grouping (each year has 6 bimonthly periods)\n",
    "        oos_data['year'] = oos_data.index.year.astype(int)\n",
    "        oos_data['bimonthly'] = ((oos_data.index.month - 1) // 2 + 1).astype(int)\n",
    "        \n",
    "        # Create simplified period labels with just the start month (YYYY-MM)\n",
    "        oos_data['period_label'] = oos_data.apply(\n",
    "            lambda row: f\"{int(row['year'])}-{int((row['bimonthly'] - 1) * 2 + 1):02d}\",\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Create a DataFrame to store bimonthly Sharpe ratios\n",
    "        bimonthly_sharpe = []\n",
    "        \n",
    "        # Group by year and bimonthly period, calculate Sharpe ratio for each period\n",
    "        for period_label, group in oos_data.groupby('period_label'):\n",
    "            # Skip periods with too few trading days\n",
    "            if len(group) < 10:\n",
    "                continue\n",
    "                \n",
    "            # Create a bimonthly result entry\n",
    "            year, start_month = period_label.split('-')\n",
    "            year = int(year)\n",
    "            start_month = int(start_month)\n",
    "            \n",
    "            bimonthly_result = {\n",
    "                'period_label': period_label,\n",
    "                'date': pd.Timestamp(year=year, month=start_month, day=15),  # Middle of first month in period\n",
    "                'trading_days': len(group),\n",
    "            }\n",
    "            \n",
    "            # Calculate Sharpe ratio for each strategy in this period\n",
    "            for name in strategies.keys():\n",
    "                # Get returns for this strategy in this period\n",
    "                returns = group[f'Daily_PnL_{name}']\n",
    "                \n",
    "                # Calculate Sharpe ratio (annualized)\n",
    "                if len(returns) > 1 and returns.std() > 0:\n",
    "                    sharpe = returns.mean() / returns.std() * np.sqrt(252)\n",
    "                else:\n",
    "                    sharpe = 0\n",
    "                    \n",
    "                bimonthly_result[f'{name}_sharpe'] = sharpe\n",
    "                bimonthly_result[f'{name}_return'] = returns.sum()  # Total P&L for the period\n",
    "            \n",
    "            # Calculate the normalized average of medoid Sharpe ratios\n",
    "            medoid_sharpes = [bimonthly_result[f'Medoid_{i}_sharpe'] for i in range(1, len(filtered_medoids) + 1)]\n",
    "            bimonthly_result['Avg_Medoid_sharpe'] = sum(medoid_sharpes) / len(filtered_medoids)\n",
    "            \n",
    "            # Calculate the normalized average of medoid returns\n",
    "            medoid_returns = [bimonthly_result[f'Medoid_{i}_return'] for i in range(1, len(filtered_medoids) + 1)]\n",
    "            bimonthly_result['Avg_Medoid_return'] = sum(medoid_returns) / len(filtered_medoids)\n",
    "            \n",
    "            bimonthly_sharpe.append(bimonthly_result)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        bimonthly_sharpe_df = pd.DataFrame(bimonthly_sharpe)\n",
    "        \n",
    "        # Sort the DataFrame by date for proper chronological display\n",
    "        if not bimonthly_sharpe_df.empty:\n",
    "            bimonthly_sharpe_df = bimonthly_sharpe_df.sort_values('date')\n",
    "        else:\n",
    "            print(f\"WARNING: No bimonthly periods found for {SYMBOL}. Cannot create chart.\")\n",
    "            return None\n",
    "        \n",
    "        # Add rounded values to dataframe for calculations\n",
    "        bimonthly_sharpe_df['Best_sharpe_rounded'] = np.round(bimonthly_sharpe_df['Best_sharpe'], 2)\n",
    "        bimonthly_sharpe_df['Avg_Medoid_sharpe_rounded'] = np.round(bimonthly_sharpe_df['Avg_Medoid_sharpe'], 2)\n",
    "        \n",
    "        # Print detailed comparison of Sharpe ratios\n",
    "        print(\"\\nDetailed Sharpe ratio comparison by period:\")\n",
    "        print(f\"{'Period':<12} | {'Best Sharpe':>12} | {'Medoid Portfolio':>16} | {'Difference':>12} | {'Portfolio Wins':<14}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for idx, row in bimonthly_sharpe_df.iterrows():\n",
    "            period = row['period_label']\n",
    "            best_sharpe = row['Best_sharpe']\n",
    "            avg_medoid_sharpe = row['Avg_Medoid_sharpe']\n",
    "            best_rounded = row['Best_sharpe_rounded']\n",
    "            avg_medoid_rounded = row['Avg_Medoid_sharpe_rounded']\n",
    "            \n",
    "            diff = avg_medoid_sharpe - best_sharpe\n",
    "            portfolio_wins = avg_medoid_sharpe > best_sharpe\n",
    "            \n",
    "            print(f\"{period:<12} | {best_sharpe:12.6f} | {avg_medoid_sharpe:16.6f} | {diff:12.6f} | {portfolio_wins!s:<14}\")\n",
    "        \n",
    "        # Calculate win rate using raw values\n",
    "        portfolio_wins = sum(bimonthly_sharpe_df['Avg_Medoid_sharpe'] > bimonthly_sharpe_df['Best_sharpe'])\n",
    "        total_periods = len(bimonthly_sharpe_df)\n",
    "        win_percentage = (portfolio_wins / total_periods) * 100 if total_periods > 0 else 0\n",
    "        \n",
    "        # Calculate win rate using rounded values (for alternative comparison)\n",
    "        rounded_wins = sum(bimonthly_sharpe_df['Avg_Medoid_sharpe_rounded'] > bimonthly_sharpe_df['Best_sharpe_rounded'])\n",
    "        rounded_win_percentage = (rounded_wins / total_periods) * 100 if total_periods > 0 else 0\n",
    "        \n",
    "        print(f\"\\nBimonthly periods analyzed: {total_periods}\")\n",
    "        print(f\"Medoid Portfolio Wins: {portfolio_wins} of {total_periods} periods ({win_percentage:.2f}%)\")\n",
    "        print(f\"Using rounded values (2 decimal places): {rounded_wins} of {total_periods} periods ({rounded_win_percentage:.2f}%)\")\n",
    "        \n",
    "        # Create a bar plot to compare bimonthly Sharpe ratios\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Set up x-axis dates\n",
    "        x = np.arange(len(bimonthly_sharpe_df))\n",
    "        width = 0.35  # Width of the bars\n",
    "        \n",
    "        # Create bars\n",
    "        plt.bar(x - width/2, bimonthly_sharpe_df['Best_sharpe'], width, \n",
    "            label=f'Best Sharpe ({best_short_sma}/{best_long_sma})', color='blue')\n",
    "        plt.bar(x + width/2, bimonthly_sharpe_df['Avg_Medoid_sharpe'], width, \n",
    "            label=f'Medoid Portfolio ({len(filtered_medoids)} strategies)', color='green')\n",
    "        \n",
    "        # Add a horizontal line at Sharpe = 0\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Create medoid description for the title\n",
    "        medoid_desc = \", \".join([f\"({int(m[0])}/{int(m[1])})\" for m in filtered_medoids])\n",
    "\n",
    "        # Customize the plot - using rounded win percentage instead of raw\n",
    "        plt.title(f'Bimonthly Sharpe Ratio Comparison (Out-of-Sample Period)\\n' + \n",
    "                f'Medoid Portfolio [{medoid_desc}] outperformed {rounded_win_percentage:.2f}% of the time', \n",
    "                fontsize=14)\n",
    "        plt.xlabel('Bimonthly Period (Start Month)', fontsize=12)\n",
    "        plt.ylabel('Sharpe Ratio (Annualized)', fontsize=12)\n",
    "        \n",
    "        # Simplified x-tick labels with just the period start month\n",
    "        plt.xticks(x, bimonthly_sharpe_df['period_label'], rotation=45)\n",
    "        \n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Create legend with both strategies - moved to bottom to avoid overlap with title\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2,\n",
    "                frameon=True, fancybox=True, framealpha=0.9, fontsize=10)\n",
    "        \n",
    "        # Add a text box with rounded win percentage instead of raw - moved to right side\n",
    "        plt.annotate(f'Medoid Portfolio Win Rate: {rounded_win_percentage:.2f}%\\n'\n",
    "                    f'({rounded_wins} out of {total_periods} periods)\\n'\n",
    "                    f'Portfolio: {len(filtered_medoids)} medoids with Sharpe  {min_sharpe}\\n'\n",
    "                    f'ATR-Based Position Sizing (${capital:,}, {atr_period} days)',\n",
    "                    xy=(0.7, 0.95), xycoords='axes fraction',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "                    fontsize=12)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Add extra space at the bottom for the legend\n",
    "        save_plot('Bimonthly_Comparison.png')\n",
    "        \n",
    "        # Save bimonthly comparison to CSV\n",
    "        save_results(bimonthly_sharpe_df, f\"{SYMBOL}_bimonthly_comparison.csv\")\n",
    "        \n",
    "        # Save win percentage and medoid parameters to Excel file\n",
    "        try:\n",
    "            import openpyxl\n",
    "            \n",
    "            # Path to the Excel file\n",
    "            excel_file = r\"C:\\Users\\Admin\\Documents\\darbas\\Results.xlsx\"\n",
    "            \n",
    "            # Check if file exists\n",
    "            if not os.path.exists(excel_file):\n",
    "                print(f\"Excel file not found at: {excel_file}\")\n",
    "                return bimonthly_sharpe_df\n",
    "                \n",
    "            print(f\"Updating Excel file with K-means results for {SYMBOL}...\")\n",
    "            \n",
    "            # Load the workbook\n",
    "            wb = openpyxl.load_workbook(excel_file)\n",
    "            \n",
    "            # Get the active sheet\n",
    "            sheet = wb.active\n",
    "            \n",
    "            # Find the row with the ticker symbol or the first empty row\n",
    "            row = 3  # Start from row 3 (assuming rows 1-2 have headers)\n",
    "            ticker_row = None\n",
    "            \n",
    "            while True:\n",
    "                cell_value = sheet.cell(row=row, column=1).value\n",
    "                if cell_value == SYMBOL:\n",
    "                    # Found the ticker symbol\n",
    "                    ticker_row = row\n",
    "                    break\n",
    "                elif cell_value is None:\n",
    "                    # Found an empty row\n",
    "                    ticker_row = row\n",
    "                    # Write the ticker symbol in column A\n",
    "                    sheet.cell(row=ticker_row, column=1).value = SYMBOL\n",
    "                    break\n",
    "                row += 1\n",
    "            \n",
    "            # Round the win percentage to one decimal place\n",
    "            rounded_win_percentage_1dp = round(rounded_win_percentage, 1)\n",
    "            \n",
    "            # Write the win percentage in column B (K-means)\n",
    "            sheet.cell(row=ticker_row, column=2).value = rounded_win_percentage_1dp\n",
    "            \n",
    "            # Write the medoid parameters in the respective cluster columns\n",
    "            # K-means clusters start at column E (5)\n",
    "            for i, medoid in enumerate(filtered_medoids):\n",
    "                if i >= 3:  # Only use up to 3 clusters\n",
    "                    break\n",
    "                    \n",
    "                # Calculate column index: E=5, F=6, G=7 for Cluster1, Cluster2, Cluster3\n",
    "                column_idx = 5 + i\n",
    "                \n",
    "                # Format as \"short/long\" (e.g., \"5/20\")\n",
    "                param_value = f\"{int(medoid[0])}/{int(medoid[1])}\"\n",
    "                \n",
    "                # Write to Excel\n",
    "                sheet.cell(row=ticker_row, column=column_idx).value = param_value\n",
    "            \n",
    "            # Write the best Sharpe parameters in column M (13)\n",
    "            best_sharpe_params = f\"{best_short_sma}/{best_long_sma}\"\n",
    "            sheet.cell(row=ticker_row, column=13).value = best_sharpe_params\n",
    "            \n",
    "            # Save the workbook\n",
    "            wb.save(excel_file)\n",
    "            \n",
    "            print(f\"Excel file updated successfully. Added {SYMBOL} with K-means win rate {rounded_win_percentage_1dp}% in row {ticker_row}\")\n",
    "            print(f\"Added best Sharpe parameters {best_sharpe_params} in column M\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating Excel file: {e}\")\n",
    "        \n",
    "        # Return the bimonthly Sharpe ratio data\n",
    "        return bimonthly_sharpe_df\n",
    "        # Main execution block\n",
    "    # Main execution block\n",
    "    # Set matplotlib backend explicitly\n",
    "    matplotlib.use('Agg')  # Use non-interactive backend for headless environments\n",
    "\n",
    "    print(\"Starting ATR-based SMA strategy analysis...\")\n",
    "\n",
    "    # Run the basic analysis first\n",
    "    data, best_short, best_long, best_sharpe, best_trades = analyze_sma_results()\n",
    "\n",
    "    if data is None:\n",
    "        print(\"Error: Failed to load or analyze SMA results data.\")\n",
    "        exit(1)\n",
    "\n",
    "    print(\"\\nProceeding with cluster analysis...\")\n",
    "\n",
    "    # Run the cluster analysis to get medoids\n",
    "    X_filtered, medoids, top_medoids, centroids, max_sharpe_point = cluster_analysis()\n",
    "\n",
    "    if X_filtered is None or medoids is None:\n",
    "        print(\"Error: Cluster analysis failed.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Re-sort top_medoids to ensure they're in the right order\n",
    "    if top_medoids is not None:\n",
    "        print(\"Re-sorting top medoids by Sharpe ratio...\")\n",
    "        top_medoids = sorted(top_medoids, key=lambda x: float(x[2]), reverse=True)\n",
    "        for idx, medoid in enumerate(top_medoids, 1):\n",
    "            print(f\"Verified Medoid {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "                f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "    print(\"\\nPlotting strategy performance...\")\n",
    "\n",
    "    # Plot strategy performance with the best parameters AND top medoids using ATR-based position sizing\n",
    "    market_data = plot_strategy_performance(\n",
    "        best_short, best_long, top_medoids, \n",
    "        big_point_value=big_point_value,\n",
    "        slippage=slippage,\n",
    "        capital=capital,\n",
    "        atr_period=atr_period\n",
    "    )\n",
    "    \n",
    "    # Run the bimonthly out-of-sample comparison between best Sharpe and top medoids\n",
    "    if top_medoids and len(top_medoids) > 0:\n",
    "        print(\"\\nPerforming bimonthly out-of-sample comparison...\")\n",
    "        bimonthly_sharpe_df = bimonthly_out_of_sample_comparison(\n",
    "            market_data, \n",
    "            best_short, \n",
    "            best_long, \n",
    "            top_medoids,  # Pass the entire top_medoids list\n",
    "            big_point_value=big_point_value,\n",
    "            slippage=slippage,\n",
    "            capital=capital,\n",
    "            atr_period=atr_period\n",
    "        )\n",
    "    else:\n",
    "        print(\"No top medoids found. Cannot run bimonthly comparison.\")\n",
    "    \n",
    "    print(\"\\nAnalysis complete! All plots and result files have been saved to the output directory.\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Call the main function to execute the script\n",
    "if __name__ == \"__main__\":\n",
    "    data_analysis_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a6930",
   "metadata": {},
   "source": [
    "## 6. Hierarchical Analysis Implementation (data_analysis_hierarchy.py)\n",
    "Paste the contents of data_analysis_hierarchy.py here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5db0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.patches import Circle\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import calendar\n",
    "import os\n",
    "import glob\n",
    "import read_ts\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib\n",
    "import openpyxl\n",
    "\n",
    "from input import MIN_TRADES, MAX_TRADES, MIN_ELEMENTS_PER_CLUSTER, DEFAULT_NUM_CLUSTERS\n",
    "from input import TICKER, START_DATE, END_DATE, TRAIN_TEST_SPLIT, ATR_PERIOD, TRADING_CAPITAL\n",
    "from SMA_Strategy import SMAStrategy\n",
    "\n",
    "import json\n",
    "\n",
    "def data_analysis_hierarchy_main():\n",
    "    def load_parameters():\n",
    "        try:\n",
    "            with open(\"parameters.json\", \"r\") as file:\n",
    "                parameters = json.load(file)\n",
    "                big_point_value = parameters[\"big_point_value\"]\n",
    "                slippage = parameters[\"slippage\"]\n",
    "                capital = parameters.get(\"capital\", TRADING_CAPITAL)\n",
    "                atr_period = parameters.get(\"atr_period\", ATR_PERIOD)\n",
    "                return big_point_value, slippage, capital, atr_period\n",
    "        except FileNotFoundError:\n",
    "            print(\"Parameters file not found. Ensure it was saved correctly in data_gather.py.\")\n",
    "            return None, None, TRADING_CAPITAL, ATR_PERIOD\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key in parameters.json: {e}\")\n",
    "            print(f\"Available keys: {', '.join(parameters.keys())}\")\n",
    "            return None, None, TRADING_CAPITAL, ATR_PERIOD\n",
    "\n",
    "    # Load the parameters\n",
    "    big_point_value, slippage, capital, atr_period = load_parameters()\n",
    "\n",
    "    print(f\"Big Point Value: {big_point_value}\")\n",
    "    print(f\"Dynamic Slippage: {slippage}\")\n",
    "    print(f\"Capital for Position Sizing: {capital:,}\")\n",
    "    print(f\"ATR Period: {atr_period}\")\n",
    "\n",
    "    # Setup paths\n",
    "    WORKING_DIR = \".\"  # Current directory\n",
    "    DATA_DIR = os.path.join(WORKING_DIR, \"data\")\n",
    "    SYMBOL = TICKER.replace('=F', '')\n",
    "\n",
    "    # Define the output folder where the plots will be saved\n",
    "    output_dir = os.path.join(WORKING_DIR, 'output', SYMBOL)\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "    # Function to save plots in the created folder\n",
    "    def save_plot(plot_name):\n",
    "        plt.savefig(os.path.join(output_dir, plot_name))  # Save plot to the symbol-specific folder\n",
    "        plt.show()  # Display the plot\n",
    "        plt.close()  # Close the plot to free up memory\n",
    "\n",
    "    # Function to save results to csv in the same output directory\n",
    "    def save_results(data_frame, file_name):\n",
    "        csv_path = os.path.join(output_dir, file_name)\n",
    "        data_frame.to_csv(csv_path, index=False)\n",
    "        print(f\"Results saved to {csv_path}\")\n",
    "\n",
    "    def find_futures_file(symbol, data_dir):\n",
    "        \"\"\"Find a data file for the specified futures symbol\"\"\"\n",
    "        # First try a pattern that specifically looks for @SYMBOL\n",
    "        pattern = f\"*@{symbol}_*.dat\"\n",
    "        files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            # Try a pattern that looks for the symbol with an underscore or at boundary\n",
    "            pattern = f\"*_@{symbol}_*.dat\"\n",
    "            files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            # Try a more specific boundary pattern for the symbol\n",
    "            pattern = f\"*_{symbol}_*.dat\"\n",
    "            files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        if not files:\n",
    "            # Last resort: less specific but better than nothing\n",
    "            pattern = f\"*@{symbol}*.dat\"\n",
    "            files = glob.glob(os.path.join(data_dir, pattern))\n",
    "        \n",
    "        return files[0] if files else None\n",
    "\n",
    "    # Load the SMA simulation results\n",
    "    def analyze_sma_results(file_path='sma_all_results.txt'):\n",
    "        print(f\"Loading simulation results from {file_path}...\")\n",
    "\n",
    "        # Load the data from the CSV file\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Print basic information about the data\n",
    "        print(f\"Loaded {len(data)} simulation results\")\n",
    "        print(f\"Short SMA range: {data['short_SMA'].min()} to {data['short_SMA'].max()}\")\n",
    "        print(f\"Long SMA range: {data['long_SMA'].min()} to {data['long_SMA'].max()}\")\n",
    "        print(f\"Sharpe ratio range: {data['sharpe_ratio'].min():.4f} to {data['sharpe_ratio'].max():.4f}\")\n",
    "\n",
    "        # Find the best Sharpe ratio\n",
    "        best_idx = data['sharpe_ratio'].idxmax()\n",
    "        best_short_sma = data.loc[best_idx, 'short_SMA']\n",
    "        best_long_sma = data.loc[best_idx, 'long_SMA']\n",
    "        best_sharpe = data.loc[best_idx, 'sharpe_ratio']\n",
    "        best_trades = data.loc[best_idx, 'trades']\n",
    "\n",
    "        print(f\"\\nBest parameters:\")\n",
    "        print(f\"Short SMA: {best_short_sma}\")\n",
    "        print(f\"Long SMA: {best_long_sma}\")\n",
    "        print(f\"Sharpe Ratio: {best_sharpe:.6f}\")\n",
    "        print(f\"Number of Trades: {best_trades}\")\n",
    "\n",
    "        # Create a pivot table for the heatmap\n",
    "        heatmap_data = data.pivot_table(\n",
    "            index='long_SMA',\n",
    "            columns='short_SMA',\n",
    "            values='sharpe_ratio'\n",
    "        )\n",
    "\n",
    "        # Create the heatmap visualization\n",
    "        plt.figure(figsize=(12, 10))\n",
    "\n",
    "        # Create a mask for invalid combinations (where short_SMA >= long_SMA)\n",
    "        mask = np.zeros_like(heatmap_data, dtype=bool)\n",
    "        for i, long_sma in enumerate(heatmap_data.index):\n",
    "            for j, short_sma in enumerate(heatmap_data.columns):\n",
    "                if short_sma >= long_sma:\n",
    "                    mask[i, j] = True\n",
    "\n",
    "        # Plot the heatmap with the mask\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_data,\n",
    "            mask=mask,\n",
    "            cmap='coolwarm',  # Blue to red colormap\n",
    "            annot=False,  # Don't annotate each cell with its value\n",
    "            fmt='.4f',\n",
    "            linewidths=0,\n",
    "            cbar_kws={'label': 'Sharpe Ratio'}\n",
    "        )\n",
    "\n",
    "        # Invert the y-axis so smaller long_SMA values are at the top\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        # Find the position of the best Sharpe ratio in the heatmap\n",
    "        best_y = heatmap_data.index.get_loc(best_long_sma)\n",
    "        best_x = heatmap_data.columns.get_loc(best_short_sma)\n",
    "\n",
    "        # Add a star to mark the best Sharpe ratio\n",
    "        # We need to add 0.5 to center the marker in the cell\n",
    "        ax.add_patch(Circle((best_x + 0.5, best_y + 0.5), 0.4, facecolor='none',\n",
    "                            edgecolor='white', lw=2))\n",
    "        plt.plot(best_x + 0.5, best_y + 0.5, 'w*', markersize=10)\n",
    "\n",
    "        # Set labels and title\n",
    "        plt.title(f'SMA Optimization Heatmap (Best Sharpe: {best_sharpe:.4f} at {best_short_sma}/{best_long_sma})',\n",
    "                fontsize=14)\n",
    "        plt.xlabel('Short SMA (days)', fontsize=12)\n",
    "        plt.ylabel('Long SMA (days)', fontsize=12)\n",
    "\n",
    "        # Rotate tick labels for better readability\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "\n",
    "        # Add a text annotation for the best parameters\n",
    "        plt.annotate(\n",
    "            f'Best: Short={best_short_sma}, Long={best_long_sma}\\nSharpe={best_sharpe:.4f}, Trades={best_trades}',\n",
    "            xy=(best_x + 0.5, best_y + 0.5),\n",
    "            xytext=(best_x + 5, best_y + 5),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color='white'),\n",
    "            color='white',\n",
    "            backgroundcolor='black',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"black\", alpha=0.7)\n",
    "        )\n",
    "\n",
    "        # Display and save the plot\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Return the data and best parameters\n",
    "        return data, best_short_sma, best_long_sma, best_sharpe, best_trades\n",
    "\n",
    "    def compute_hierarchical_medoids(X, labels, valid_clusters):\n",
    "        \"\"\"\n",
    "        Compute medoids for each valid cluster from hierarchical clustering\n",
    "        A medoid is the data point in a cluster that has the minimum average distance to all other points in the cluster\n",
    "\n",
    "        Parameters:\n",
    "        X: numpy array of shape (n_samples, n_features) - Original data points (not scaled)\n",
    "        labels: numpy array of shape (n_samples,) - Cluster labels for each data point\n",
    "        valid_clusters: set - Set of valid cluster IDs\n",
    "\n",
    "        Returns:\n",
    "        list of tuples - Each tuple contains (short_SMA, long_SMA, sharpe_ratio, trades) for each medoid\n",
    "        \"\"\"\n",
    "        # Initialize list to store medoids for each cluster\n",
    "        medoids = []\n",
    "\n",
    "        # Process each valid cluster\n",
    "        for cluster_id in valid_clusters:\n",
    "            # Extract points belonging to this cluster\n",
    "            cluster_indices = np.where(labels == cluster_id)[0]\n",
    "            cluster_points = X[cluster_indices]\n",
    "\n",
    "            if len(cluster_points) == 0:\n",
    "                continue\n",
    "\n",
    "            if len(cluster_points) == 1:\n",
    "                # If only one point in cluster, it's the medoid\n",
    "                medoids.append(tuple(cluster_points[0]))\n",
    "                continue\n",
    "\n",
    "            # Calculate pairwise distances within cluster\n",
    "            min_total_distance = float('inf')\n",
    "            medoid = None\n",
    "\n",
    "            for i, point1 in enumerate(cluster_points):\n",
    "                total_distance = 0\n",
    "                for j, point2 in enumerate(cluster_points):\n",
    "                    # Calculate Euclidean distance between points\n",
    "                    distance = np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "                    total_distance += distance\n",
    "\n",
    "                if total_distance < min_total_distance:\n",
    "                    min_total_distance = total_distance\n",
    "                    medoid = point1\n",
    "\n",
    "            if medoid is not None:\n",
    "                medoids.append(tuple(medoid))\n",
    "\n",
    "            # Print medoid info\n",
    "            print(f\"Cluster {cluster_id} medoid: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "                f\"Sharpe={medoid[2]:.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "        return medoids\n",
    "\n",
    "    def create_dendrogram(X_scaled, method='ward', figsize=(12, 8), color_threshold=None, truncate_mode=None,\n",
    "                        p=10, save_path=None):\n",
    "        \"\"\"\n",
    "        Create and display a dendrogram for hierarchical clustering\n",
    "\n",
    "        Parameters:\n",
    "        X_scaled: numpy array - Scaled data points\n",
    "        method: str - Linkage method ('ward', 'complete', 'average', 'single')\n",
    "        figsize: tuple - Figure size\n",
    "        color_threshold: float - The threshold to apply for coloring the dendrogram\n",
    "        truncate_mode: str - 'lastp' for last p branches, 'level' for no more than p levels\n",
    "        p: int - Used with truncate_mode\n",
    "        save_path: str - Path to save the dendrogram image\n",
    "\n",
    "        Returns:\n",
    "        Z: numpy array - The hierarchical clustering linkage matrix\n",
    "        \"\"\"\n",
    "        # Use a sample if there are too many points\n",
    "        if len(X_scaled) > 1000:\n",
    "            np.random.seed(42)  # For reproducibility\n",
    "            sample_indices = np.random.choice(len(X_scaled), size=1000, replace=False)\n",
    "            X_sample = X_scaled[sample_indices]\n",
    "            print(f\"Using a random sample of 1000 points for dendrogram creation (out of {len(X_scaled)} total points)\")\n",
    "        else:\n",
    "            X_sample = X_scaled\n",
    "\n",
    "        # Compute the distance matrix\n",
    "        dist_matrix = pdist(X_sample, metric='euclidean')\n",
    "\n",
    "        # Compute the linkage matrix\n",
    "        Z = shc.linkage(dist_matrix, method=method)\n",
    "\n",
    "        # Print some statistics about the linkage\n",
    "        print(f\"\\nDendrogram using {method} linkage method:\")\n",
    "        print(f\"Number of data points: {len(X_sample)}\")\n",
    "        print(f\"Cophenetic correlation: {shc.cophenet(Z, dist_matrix)[0]:.4f}\")\n",
    "\n",
    "        # Create a new figure\n",
    "        plt.figure(figsize=figsize)\n",
    "\n",
    "        # Set the background color\n",
    "        plt.gca().set_facecolor('white')\n",
    "\n",
    "        # Set a title\n",
    "        plt.title(f'Hierarchical Clustering Dendrogram ({method} linkage)', fontsize=14)\n",
    "\n",
    "        # Generate the dendrogram\n",
    "        dendrogram = shc.dendrogram(\n",
    "            Z,\n",
    "            truncate_mode='lastp',  # Only show the last p merges\n",
    "            p=30,  # Show only 30 merges\n",
    "            leaf_rotation=90.,\n",
    "            leaf_font_size=10.,\n",
    "            show_contracted=True,\n",
    "            color_threshold=color_threshold\n",
    "        )\n",
    "\n",
    "        # Add labels and axes\n",
    "        plt.xlabel('Sample Index or Cluster Size', fontsize=12)\n",
    "        plt.ylabel('Distance', fontsize=12)\n",
    "\n",
    "        # Add a horizontal line to indicate a distance threshold if specified\n",
    "        if color_threshold is not None:\n",
    "            plt.axhline(y=color_threshold, color='crimson', linestyle='--',\n",
    "                        label=f'Threshold: {color_threshold:.2f}')\n",
    "            plt.legend()\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot\n",
    "        save_plot('Hierarchical_Dendrogram.png')\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def hierarchical_cluster_analysis(file_path='sma_all_results.txt', min_trades=MIN_TRADES, max_trades=MAX_TRADES,\n",
    "                        min_elements_per_cluster=MIN_ELEMENTS_PER_CLUSTER):\n",
    "        \"\"\"\n",
    "        Perform hierarchical clustering analysis on SMA optimization results to find robust parameter regions\n",
    "        \n",
    "        Parameters:\n",
    "        file_path: str - Path to the file with SMA results\n",
    "        min_trades: int - Minimum number of trades to consider\n",
    "        max_trades: int - Maximum number of trades to consider\n",
    "        min_elements_per_cluster: int - Minimum number of elements per cluster\n",
    "        \n",
    "        Returns:\n",
    "        tuple - (X_filtered_full, medoids, top_medoids, max_sharpe_point, labels)\n",
    "        \"\"\"\n",
    "        print(f\"\\n----- HIERARCHICAL CLUSTER ANALYSIS -----\")\n",
    "        print(f\"Loading data from {file_path}...\")\n",
    "\n",
    "        # Load the data\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Convert data to numpy array for easier processing\n",
    "        X_full = df[['short_SMA', 'long_SMA', 'sharpe_ratio', 'trades']].values\n",
    "\n",
    "        # STEP 1: First filter by trades and valid SMA combinations\n",
    "        X_filtered_full = X_full[(X_full[:, 0] < X_full[:, 1]) &  # short_SMA < long_SMA\n",
    "                    (X_full[:, 3] >= min_trades) &  # trades >= min_trades\n",
    "                    (X_full[:, 3] <= max_trades)]  # trades <= max_trades\n",
    "\n",
    "        if len(X_filtered_full) == 0:\n",
    "            print(f\"No data points meet the criteria after filtering! Adjust min_trades ({min_trades}) and max_trades ({max_trades}).\")\n",
    "            return None, None, None, None, None\n",
    "\n",
    "        # STEP 2: Use exactly 3 dimensions for clustering - same as K-means\n",
    "        X_filtered = X_filtered_full[:, 0:3]  # Only short_SMA, long_SMA, and sharpe_ratio\n",
    "        \n",
    "        print(f\"Filtered data to {len(X_filtered)} points with {min_trades}-{max_trades} trades\")\n",
    "\n",
    "        # Extract the fields for better scaling visibility\n",
    "        short_sma_values = X_filtered[:, 0]\n",
    "        long_sma_values = X_filtered[:, 1]\n",
    "        sharpe_values = X_filtered[:, 2]\n",
    "        trades_values = X_filtered_full[:, 3]\n",
    "\n",
    "        print(f\"Short SMA range: {short_sma_values.min()} to {short_sma_values.max()}\")\n",
    "        print(f\"Long SMA range: {long_sma_values.min()} to {long_sma_values.max()}\")\n",
    "        print(f\"Sharpe ratio range: {sharpe_values.min():.4f} to {sharpe_values.max():.4f}\")\n",
    "        print(f\"Trades range: {trades_values.min()} to {trades_values.max()}\")\n",
    "\n",
    "        # Scale the data for clustering - using StandardScaler for each dimension\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_filtered)  # Only scale the 3 dimensions we use for clustering\n",
    "\n",
    "        # Print scaling info for verification\n",
    "        print(\"\\nScaled data information:\")\n",
    "        scaled_short = X_scaled[:, 0]\n",
    "        scaled_long = X_scaled[:, 1]\n",
    "        scaled_sharpe = X_scaled[:, 2]\n",
    "\n",
    "        print(f\"Scaled Short SMA range: {scaled_short.min():.4f} to {scaled_short.max():.4f}\")\n",
    "        print(f\"Scaled Long SMA range: {scaled_long.min():.4f} to {scaled_long.max():.4f}\")\n",
    "        print(f\"Scaled Sharpe ratio range: {scaled_sharpe.min():.4f} to {scaled_sharpe.max():.4f}\")\n",
    "\n",
    "        # Create a dendrogram to help visualize (optional, can be commented out)\n",
    "        print(\"\\nCreating dendrogram to visualize hierarchical structure...\")\n",
    "        linkage_method = 'ward'  # Ward minimizes the variance within clusters\n",
    "        Z = create_dendrogram(X_scaled, method=linkage_method, figsize=(12, 8))\n",
    "\n",
    "        # STEP 3: Determine number of clusters - use the same DEFAULT_NUM_CLUSTERS as K-means\n",
    "        print(f\"Using default number of clusters: {DEFAULT_NUM_CLUSTERS}\")\n",
    "        k = DEFAULT_NUM_CLUSTERS\n",
    "\n",
    "        # STEP 4: Apply hierarchical clustering\n",
    "        print(f\"Performing hierarchical clustering with {k} clusters using {linkage_method} linkage...\")\n",
    "        hierarchical = AgglomerativeClustering(\n",
    "            n_clusters=k,\n",
    "            linkage=linkage_method\n",
    "        )\n",
    "\n",
    "        # Fit the model and get cluster labels\n",
    "        labels = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "        # STEP 5: Count elements per cluster\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        cluster_sizes = dict(zip(unique_labels, counts))\n",
    "\n",
    "        print(\"\\nCluster sizes:\")\n",
    "        for cluster_id, size in cluster_sizes.items():\n",
    "            print(f\"Cluster {cluster_id}: {size} elements\")\n",
    "\n",
    "        # STEP 6: Filter clusters with enough elements - SAME as K-means approach\n",
    "        valid_clusters = {i for i, count in cluster_sizes.items() if count >= min_elements_per_cluster}\n",
    "        \n",
    "        if not valid_clusters:\n",
    "            print(f\"No clusters have at least {min_elements_per_cluster} elements! Using all clusters.\")\n",
    "            valid_clusters = set(unique_labels)\n",
    "        \n",
    "        # Don't filter the data points here yet - just keep track of which clusters are valid\n",
    "\n",
    "        # STEP 7: Compute medoids using the original data (not scaled)\n",
    "        print(\"Computing medoids for each cluster...\")\n",
    "        medoids = compute_hierarchical_medoids(X_filtered_full, labels, valid_clusters)\n",
    "\n",
    "        # Find max Sharpe ratio point overall\n",
    "        max_sharpe_idx = np.argmax(df['sharpe_ratio'].values)\n",
    "        max_sharpe_point = df.iloc[max_sharpe_idx][['short_SMA', 'long_SMA', 'sharpe_ratio', 'trades']].values\n",
    "\n",
    "        # STEP 8: Sort medoids by Sharpe ratio\n",
    "        medoids_sorted = sorted(medoids, key=lambda x: float(x[2]), reverse=True)\n",
    "        top_medoids = medoids_sorted[:5]  # Get top 5 medoids by Sharpe ratio\n",
    "\n",
    "        # Print results\n",
    "        print(\"\\n----- HIERARCHICAL CLUSTERING RESULTS -----\")\n",
    "        print(f\"Max Sharpe point: Short SMA={int(max_sharpe_point[0])}, Long SMA={int(max_sharpe_point[1])}, \"\n",
    "            f\"Sharpe={max_sharpe_point[2]:.4f}, Trades={int(max_sharpe_point[3])}\")\n",
    "\n",
    "        print(\"\\nTop 5 Medoids (by Sharpe ratio):\")\n",
    "        for idx, medoid in enumerate(top_medoids, 1):\n",
    "            print(f\"Top {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "                f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "        # Create visualization - pass all the original filtered data and valid_clusters\n",
    "        create_hierarchical_cluster_visualization(X_filtered_full, medoids, top_medoids, max_sharpe_point, labels, valid_clusters)\n",
    "        \n",
    "        return X_filtered_full, medoids, top_medoids, max_sharpe_point, labels\n",
    "\n",
    "    def plot_strategy_performance(short_sma, long_sma, top_medoids=None, big_point_value=1, slippage=0, capital=1000000, atr_period=14):\n",
    "        print(f\"\\n----- PLOTTING STRATEGY PERFORMANCE -----\")\n",
    "        print(f\"Using Short SMA: {short_sma}, Long SMA: {long_sma}\")\n",
    "        print(f\"Trading with ATR-based position sizing (Capital: ${capital:,}, ATR Period: {atr_period})\")\n",
    "        if top_medoids:\n",
    "            print(f\"Including top {len(top_medoids)} medoids\")\n",
    "\n",
    "        # Load data from local file\n",
    "        print(f\"Loading {TICKER} data from local files...\")\n",
    "        data_file = find_futures_file(SYMBOL, DATA_DIR)\n",
    "        if not data_file:\n",
    "            print(f\"Error: No data file found for {TICKER} in {DATA_DIR}\")\n",
    "            exit(1)\n",
    "\n",
    "        print(f\"Found data file: {os.path.basename(data_file)}\")\n",
    "        print(f\"File size: {os.path.getsize(data_file)} bytes\")\n",
    "\n",
    "        # Load the data from local file\n",
    "        all_data = read_ts.read_ts_ohlcv_dat(data_file)\n",
    "        data_obj = all_data[0]\n",
    "        ohlc_data = data_obj.data.copy()\n",
    "\n",
    "        # Convert the OHLCV data to the format expected by the strategy\n",
    "        data = ohlc_data.rename(columns={\n",
    "            'datetime': 'Date',\n",
    "            'open': 'Open',\n",
    "            'high': 'High',\n",
    "            'low': 'Low',\n",
    "            'close': 'Close',\n",
    "            'volume': 'Volume'\n",
    "        })\n",
    "        data.set_index('Date', inplace=True)\n",
    "\n",
    "        # Add a warm-up period before the start date\n",
    "        original_start_idx = None\n",
    "        if START_DATE and END_DATE:\n",
    "            # Calculate warm-up period for SMA calculation (longest SMA + buffer)\n",
    "            warm_up_days = max(short_sma, long_sma) * 3  # Use 3x the longest SMA as warm-up\n",
    "            \n",
    "            # Convert dates to datetime\n",
    "            start_date = pd.to_datetime(START_DATE)\n",
    "            end_date = pd.to_datetime(END_DATE)\n",
    "            \n",
    "            # Adjust start date for warm-up\n",
    "            adjusted_start = start_date - pd.Timedelta(days=warm_up_days)\n",
    "            \n",
    "            # Filter with the extended date range\n",
    "            extended_data = data[(data.index >= adjusted_start) & (data.index <= end_date)]\n",
    "            \n",
    "            # Store the index where the actual analysis should start\n",
    "            if not extended_data.empty:\n",
    "                # Find the closest index to our original start date\n",
    "                original_start_idx = extended_data.index.get_indexer([start_date], method='nearest')[0]\n",
    "            \n",
    "            print(f\"Added {warm_up_days} days warm-up period before {START_DATE}\")\n",
    "            data = extended_data\n",
    "        \n",
    "        # Create a dictionary to store results for each strategy\n",
    "        strategies = {\n",
    "            'Best': {'short_sma': short_sma, 'long_sma': long_sma}\n",
    "        }\n",
    "\n",
    "        # Add medoids in their original order, including original Sharpe and Trades\n",
    "        if top_medoids:\n",
    "            print(\"\\nUSING THESE MEDOIDS (IN ORIGINAL ORDER):\")\n",
    "            for i, medoid in enumerate(top_medoids, 1):\n",
    "                strategies[f'Medoid {i}'] = {\n",
    "                    'short_sma': int(medoid[0]),\n",
    "                    'long_sma': int(medoid[1]),\n",
    "                    'original_sharpe': float(medoid[2]),  # Store the original Sharpe ratio\n",
    "                    'original_trades': int(medoid[3])     # Store the original number of trades\n",
    "                }\n",
    "                print(f\"Medoid {i}: SMA({int(medoid[0])}/{int(medoid[1])}) - Original Sharpe: {float(medoid[2]):.4f}, Trades: {int(medoid[3])}\")\n",
    "\n",
    "        # Apply the proper strategy for each parameter set\n",
    "        for name, params in strategies.items():\n",
    "            # Calculate centered SMAs directly\n",
    "            data[f'SMA_Short_{name}'] = data['Close'].rolling(window=params['short_sma'], center=True).mean()\n",
    "            data[f'SMA_Long_{name}'] = data['Close'].rolling(window=params['long_sma'], center=True).mean()\n",
    "            \n",
    "            # Create a strategy instance for each parameter set\n",
    "            sma_strategy = SMAStrategy(\n",
    "                short_sma=params['short_sma'],\n",
    "                long_sma=params['long_sma'],\n",
    "                big_point_value=big_point_value,\n",
    "                slippage=slippage,\n",
    "                capital=capital,\n",
    "                atr_period=atr_period\n",
    "            )\n",
    "\n",
    "            # Apply the strategy\n",
    "            data = sma_strategy.apply_strategy(\n",
    "                data.copy(),\n",
    "                strategy_name=name\n",
    "            )\n",
    "\n",
    "        # Trim data to the original date range if we added warm-up period\n",
    "        if original_start_idx is not None:\n",
    "            data_for_evaluation = data.iloc[original_start_idx:]\n",
    "            print(f\"Trimmed warm-up period. Original data length: {len(data)}, Evaluation data length: {len(data_for_evaluation)}\")\n",
    "        else:\n",
    "            data_for_evaluation = data\n",
    "        \n",
    "        # Calculate split index for in-sample/out-of-sample using the trimmed data\n",
    "        split_index = int(len(data_for_evaluation) * TRAIN_TEST_SPLIT)\n",
    "        split_date = data_for_evaluation.index[split_index]\n",
    "\n",
    "        # Create color palette for strategies\n",
    "        colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink']\n",
    "\n",
    "        # Create the performance visualization with just two panels\n",
    "        plt.figure(figsize=(14, 12))\n",
    "\n",
    "        # Plot price and SMA (first subplot)\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(data_for_evaluation.index, data_for_evaluation['Close'], label=f'{SYMBOL} Price', color='black', alpha=0.5)\n",
    "        \n",
    "        # Use the centered SMAs for plotting\n",
    "        for name, params in strategies.items():\n",
    "            if name == 'Best':\n",
    "                plt.plot(data_for_evaluation.index, data_for_evaluation[f'SMA_Short_{name}'], \n",
    "                        label=f'Short SMA ({params[\"short_sma\"]})', color='orange')\n",
    "                plt.plot(data_for_evaluation.index, data_for_evaluation[f'SMA_Long_{name}'], \n",
    "                        label=f'Long SMA ({params[\"long_sma\"]})', color='blue')\n",
    "        \n",
    "        # Mark the train/test split\n",
    "        plt.axvline(x=split_date, color='black', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title(f'{SYMBOL} Price and SMA Indicators')\n",
    "        plt.ylabel('Price')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot cumulative P&L (second subplot)\n",
    "        plt.subplot(2, 1, 2)\n",
    "\n",
    "        for i, (name, params) in enumerate(strategies.items()):\n",
    "            color = colors[i % len(colors)]\n",
    "\n",
    "            # Plot full period P&L\n",
    "            plt.plot(data_for_evaluation.index, data_for_evaluation[f'Cumulative_PnL_{name}'],\n",
    "                    label=f'{name} ({params[\"short_sma\"]}/{params[\"long_sma\"]})', color=color)\n",
    "\n",
    "            # Plot out-of-sample portion with thicker line\n",
    "            plt.plot(data_for_evaluation.index[split_index:], data_for_evaluation[f'Cumulative_PnL_{name}'].iloc[split_index:],\n",
    "                    color=color, linewidth=2.5, alpha=0.7)\n",
    "\n",
    "        plt.axvline(x=split_date, color='black', linestyle='--',\n",
    "                    label=f'Train/Test Split ({int(TRAIN_TEST_SPLIT * 100)}%/{int((1 - TRAIN_TEST_SPLIT) * 100)}%)')\n",
    "        plt.axhline(y=0.0, color='gray', linestyle='-', alpha=0.5, label='Break-even')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.title('Strategy Cumulative P&L')\n",
    "        plt.ylabel('P&L ($)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        save_plot('Hierarchical_Strategy_Performance.png')\n",
    "\n",
    "        # Create a list to store performance data for saving to file\n",
    "        performance_data = []\n",
    "\n",
    "        # Print detailed performance metrics for all strategies with in-sample and out-of-sample breakdown\n",
    "        print(\"\\n----- PERFORMANCE SUMMARY -----\")\n",
    "\n",
    "        # IN-SAMPLE PERFORMANCE\n",
    "        print(\"\\nIN-SAMPLE PERFORMANCE:\")\n",
    "        header = f\"{'Strategy':<10} | {'Short/Long':<10} | {'P&L':>12} | {'Sharpe':>7} | {'Trades':>6}\"\n",
    "        separator = \"-\" * len(header)\n",
    "        print(separator)\n",
    "        print(header)\n",
    "        print(separator)\n",
    "\n",
    "        # Calculate in-sample metrics using trimmed data\n",
    "        for name, params in strategies.items():\n",
    "            short = params['short_sma']\n",
    "            long = params['long_sma']\n",
    "\n",
    "            # Get in-sample data from trimmed dataset\n",
    "            in_sample = data_for_evaluation.iloc[:split_index]\n",
    "\n",
    "            # Calculate in-sample metrics\n",
    "            in_sample_daily_pnl = in_sample[f'Daily_PnL_{name}']\n",
    "            in_sample_cumulative_pnl = in_sample[f'Daily_PnL_{name}'].sum()\n",
    "            \n",
    "            # Calculate average position size for in-sample\n",
    "            avg_pos_size = in_sample[f'Position_Size_{name}'].mean()\n",
    "\n",
    "            # Use original Sharpe and Trades for medoids, calculate for Best\n",
    "            if name.startswith('Medoid'):\n",
    "                in_sample_sharpe = params['original_sharpe']\n",
    "                in_sample_trades = params['original_trades']\n",
    "            else:\n",
    "                # Calculate Sharpe ratio (annualized) for Best\n",
    "                if in_sample_daily_pnl.std() > 0:\n",
    "                    in_sample_sharpe = in_sample_daily_pnl.mean() / in_sample_daily_pnl.std() * np.sqrt(252)\n",
    "                else:\n",
    "                    in_sample_sharpe = 0\n",
    "                # Count in-sample trades for Best\n",
    "                in_sample_trades = in_sample[f'Position_Change_{name}'].sum()\n",
    "            \n",
    "            # Print in the original order\n",
    "            row = f\"{name:<10} | {short:>4}/{long:<5} | ${in_sample_cumulative_pnl:>10,.2f} | {in_sample_sharpe:>6.3f} | {in_sample_trades:>6}\"\n",
    "            print(row)\n",
    "            \n",
    "            # Store data for later use\n",
    "            performance_data.append({\n",
    "                'Period': 'In-Sample',\n",
    "                'Strategy': name,\n",
    "                'Short_SMA': short,\n",
    "                'Long_SMA': long,\n",
    "                'PnL': in_sample_cumulative_pnl,\n",
    "                'Avg_Position': avg_pos_size,\n",
    "                'Sharpe': in_sample_sharpe,\n",
    "                'Trades': in_sample_trades\n",
    "            })\n",
    "\n",
    "        print(separator)\n",
    "\n",
    "        # OUT-OF-SAMPLE PERFORMANCE\n",
    "        print(\"\\nOUT-OF-SAMPLE PERFORMANCE:\")\n",
    "        print(separator)\n",
    "        print(header)\n",
    "        print(separator)\n",
    "\n",
    "        # Calculate out-of-sample metrics using trimmed data\n",
    "        for name, params in strategies.items():\n",
    "            short = params['short_sma']\n",
    "            long = params['long_sma']\n",
    "\n",
    "            # Get out-of-sample data from trimmed dataset\n",
    "            out_sample = data_for_evaluation.iloc[split_index:]\n",
    "\n",
    "            # Calculate out-of-sample metrics\n",
    "            out_sample_daily_pnl = out_sample[f'Daily_PnL_{name}']\n",
    "            out_sample_cumulative_pnl = out_sample[f'Daily_PnL_{name}'].sum()\n",
    "            \n",
    "            # Calculate average position size for out-of-sample\n",
    "            avg_pos_size = out_sample[f'Position_Size_{name}'].mean()\n",
    "\n",
    "            # Calculate Sharpe ratio (annualized)\n",
    "            if out_sample_daily_pnl.std() > 0:\n",
    "                out_sample_sharpe = out_sample_daily_pnl.mean() / out_sample_daily_pnl.std() * np.sqrt(252)\n",
    "            else:\n",
    "                out_sample_sharpe = 0\n",
    "\n",
    "            # Count out-of-sample trades\n",
    "            out_sample_trades = out_sample[f'Position_Change_{name}'].sum()\n",
    "            \n",
    "            # Print in the original order\n",
    "            row = f\"{name:<10} | {short:>4}/{long:<5} | ${out_sample_cumulative_pnl:>10,.2f} | {out_sample_sharpe:>6.3f} | {out_sample_trades:>6}\"\n",
    "            print(row)\n",
    "            \n",
    "            # Store data for later use\n",
    "            performance_data.append({\n",
    "                'Period': 'Out-of-Sample',\n",
    "                'Strategy': name,\n",
    "                'Short_SMA': short,\n",
    "                'Long_SMA': long,\n",
    "                'PnL': out_sample_cumulative_pnl,\n",
    "                'Avg_Position': avg_pos_size,\n",
    "                'Sharpe': out_sample_sharpe,\n",
    "                'Trades': out_sample_trades\n",
    "            })\n",
    "\n",
    "        print(separator)\n",
    "\n",
    "        # FULL PERIOD PERFORMANCE\n",
    "        print(\"\\nFULL PERIOD PERFORMANCE:\")\n",
    "        print(separator)\n",
    "        print(header)\n",
    "        print(separator)\n",
    "\n",
    "        # Calculate full period metrics using trimmed data\n",
    "        for name, params in strategies.items():\n",
    "            short = params['short_sma']\n",
    "            long = params['long_sma']\n",
    "\n",
    "            # Calculate full period metrics using trimmed data\n",
    "            full_daily_pnl = data_for_evaluation[f'Daily_PnL_{name}']\n",
    "            full_cumulative_pnl = full_daily_pnl.sum()\n",
    "            \n",
    "            # Calculate average position size for full period\n",
    "            avg_pos_size = data_for_evaluation[f'Position_Size_{name}'].mean()\n",
    "            max_pos_size = data_for_evaluation[f'Position_Size_{name}'].max()\n",
    "\n",
    "            # Calculate Sharpe ratio (annualized)\n",
    "            if full_daily_pnl.std() > 0:\n",
    "                full_sharpe = full_daily_pnl.mean() / full_daily_pnl.std() * np.sqrt(252)\n",
    "            else:\n",
    "                full_sharpe = 0\n",
    "\n",
    "            # Count full period trades\n",
    "            full_trades = data_for_evaluation[f'Position_Change_{name}'].sum()\n",
    "            \n",
    "            # Print in the original order\n",
    "            row = f\"{name:<10} | {short:>4}/{long:<5} | ${full_cumulative_pnl:>10,.2f} | {full_sharpe:>6.3f} | {full_trades:>6}\"\n",
    "            print(row)\n",
    "            \n",
    "            # Store data for later use\n",
    "            performance_data.append({\n",
    "                'Period': 'Full',\n",
    "                'Strategy': name,\n",
    "                'Short_SMA': short,\n",
    "                'Long_SMA': long,\n",
    "                'PnL': full_cumulative_pnl,\n",
    "                'Avg_Position': avg_pos_size,\n",
    "                'Max_Position': max_pos_size,\n",
    "                'Sharpe': full_sharpe,\n",
    "                'Trades': full_trades\n",
    "            })\n",
    "            \n",
    "            # Additional metrics for the best strategy\n",
    "            if name == 'Best':\n",
    "                print(f\"\\nAdditional metrics for Best strategy:\")\n",
    "                print(f\"Maximum position size: {max_pos_size:.2f} contracts\")\n",
    "                print(f\"Average ATR value: {data_for_evaluation['ATR_Best'].mean():.4f}\")\n",
    "                \n",
    "                # Calculate drawdown using trimmed data\n",
    "                peak = data_for_evaluation[f'Cumulative_PnL_{name}'].cummax()\n",
    "                drawdown = data_for_evaluation[f'Cumulative_PnL_{name}'] - peak\n",
    "                max_drawdown = drawdown.min()\n",
    "                \n",
    "                print(f\"Maximum drawdown: ${max_drawdown:.2f}\")\n",
    "                \n",
    "                # Calculate win rate using trimmed data\n",
    "                daily_win_rate = (data_for_evaluation[f'Daily_PnL_{name}'] > 0).mean() * 100\n",
    "                print(f\"Daily win rate: {daily_win_rate:.2f}%\")\n",
    "\n",
    "        print(separator)\n",
    "\n",
    "        # Save performance data to CSV in the ticker output directory\n",
    "        performance_df = pd.DataFrame(performance_data)\n",
    "        save_results(performance_df, f\"{SYMBOL}_hierarchical_performance_summary.csv\")\n",
    "\n",
    "        return data_for_evaluation  # Return the trimmed data instead of full data\n",
    "\n",
    "    def bimonthly_out_of_sample_comparison(data, best_short_sma, best_long_sma, top_medoids, min_sharpe=0.2, \n",
    "                                big_point_value=big_point_value, slippage=slippage,\n",
    "                                capital=capital, atr_period=atr_period):\n",
    "        \"\"\"\n",
    "        Compare bimonthly (2-month) performance between the best Sharpe strategy and a portfolio of top medoids\n",
    "        using ATR-based position sizing.\n",
    "        \n",
    "        Parameters:\n",
    "        data: DataFrame with market data (should already be trimmed to exclude warmup period)\n",
    "        best_short_sma: int - The short SMA period for the best Sharpe strategy\n",
    "        best_long_sma: int - The long SMA period for the best Sharpe strategy\n",
    "        top_medoids: list - List of top medoids, each as (short_sma, long_sma, sharpe, trades)\n",
    "        min_sharpe: float - Minimum Sharpe ratio threshold for medoids to be included\n",
    "        big_point_value: float - Big point value for the futures contract\n",
    "        slippage: float - Slippage value in price units\n",
    "        capital: float - Capital allocation for position sizing\n",
    "        atr_period: int - Period for ATR calculation\n",
    "        \"\"\"\n",
    "        print(f\"\\n----- BIMONTHLY OUT-OF-SAMPLE COMPARISON -----\")\n",
    "        print(f\"Best Sharpe: ({best_short_sma}/{best_long_sma})\")\n",
    "        print(f\"Using ATR-based position sizing (Capital: ${capital:,}, ATR Period: {atr_period})\")\n",
    "        \n",
    "        # Handle the case where top_medoids is None\n",
    "        if top_medoids is None:\n",
    "            print(\"No medoids provided. Comparison cannot be performed.\")\n",
    "            return None\n",
    "        \n",
    "        # ADDED: Ensure we have trimmed data\n",
    "        if data is None:\n",
    "            # Load data from local file\n",
    "            print(f\"Loading {TICKER} data from local files...\")\n",
    "            data_file = find_futures_file(SYMBOL, DATA_DIR)\n",
    "            if not data_file:\n",
    "                print(f\"Error: No data file found for {TICKER} in {DATA_DIR}\")\n",
    "                exit(1)\n",
    "            \n",
    "            print(f\"Found data file: {os.path.basename(data_file)}\")\n",
    "            print(f\"File size: {os.path.getsize(data_file)} bytes\")\n",
    "\n",
    "            # Load the data from local file\n",
    "            all_data = read_ts.read_ts_ohlcv_dat(data_file)\n",
    "            data_obj = all_data[0]\n",
    "            ohlc_data = data_obj.data.copy()\n",
    "\n",
    "            # Convert the OHLCV data to the format expected by the strategy\n",
    "            data = ohlc_data.rename(columns={\n",
    "                'datetime': 'Date',\n",
    "                'open': 'Open',\n",
    "                'high': 'High',\n",
    "                'low': 'Low',\n",
    "                'close': 'Close',\n",
    "                'volume': 'Volume'\n",
    "            })\n",
    "            data.set_index('Date', inplace=True)\n",
    "            \n",
    "            # Add a warm-up period before the start date\n",
    "            original_start_idx = None\n",
    "            if START_DATE and END_DATE:\n",
    "                # Calculate warm-up period for SMA calculation (longest SMA + buffer)\n",
    "                warm_up_days = max(best_short_sma, best_long_sma) * 3  # Use 3x the longest SMA as warm-up\n",
    "                \n",
    "                # Convert dates to datetime\n",
    "                start_date = pd.to_datetime(START_DATE)\n",
    "                end_date = pd.to_datetime(END_DATE)\n",
    "                \n",
    "                # Adjust start date for warm-up\n",
    "                adjusted_start = start_date - pd.Timedelta(days=warm_up_days)\n",
    "                \n",
    "                # Filter with the extended date range\n",
    "                extended_data = data[(data.index >= adjusted_start) & (data.index <= end_date)]\n",
    "                \n",
    "                # Store the index where the actual analysis should start\n",
    "                if not extended_data.empty:\n",
    "                    # Find the closest index to our original start date\n",
    "                    original_start_idx = extended_data.index.get_indexer([start_date], method='nearest')[0]\n",
    "                \n",
    "                print(f\"Added {warm_up_days} days warm-up period before {START_DATE}\")\n",
    "                data = extended_data\n",
    "                \n",
    "                # Trim data to the original date range\n",
    "                if original_start_idx is not None:\n",
    "                    data = data.iloc[original_start_idx:]\n",
    "                    print(f\"Trimmed warm-up period. Data length: {len(data)}\")\n",
    "        \n",
    "        # The top_medoids are already sorted by Sharpe ratio in cluster_analysis\n",
    "        # Just print them for verification\n",
    "        print(\"\\nUSING TOP MEDOIDS (BY SHARPE RATIO):\")\n",
    "        for i, medoid in enumerate(top_medoids, 1):\n",
    "            print(f\"Medoid {i}: ({int(medoid[0])}/{int(medoid[1])}) - Sharpe: {float(medoid[2]):.4f}, Trades: {int(medoid[3])}\")\n",
    "        \n",
    "        # Take at most 3 medoids and filter by minimum Sharpe\n",
    "        filtered_medoids = []\n",
    "        for i, m in enumerate(top_medoids[:3]):\n",
    "            # Check if we can access the required elements\n",
    "            try:\n",
    "                # Extract Sharpe ratio and check if it meets the threshold\n",
    "                short_sma = m[0]\n",
    "                long_sma = m[1]\n",
    "                sharpe = float(m[2])  # Convert to float to handle numpy types\n",
    "                trades = m[3]\n",
    "                \n",
    "                if sharpe >= min_sharpe:\n",
    "                    filtered_medoids.append(m)\n",
    "                    print(f\"Selected medoid {i+1} with Sharpe {sharpe:.4f}\")\n",
    "            except (IndexError, TypeError) as e:\n",
    "                print(f\"Error processing medoid: {e}\")\n",
    "        \n",
    "        if not filtered_medoids:\n",
    "            print(f\"No medoids have a Sharpe ratio >= {min_sharpe}. Comparison cannot be performed.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Creating portfolio of {len(filtered_medoids)} medoids with Sharpe ratio >= {min_sharpe}:\")\n",
    "        for i, medoid in enumerate(filtered_medoids, 1):\n",
    "            print(f\"Final Medoid {i}: ({int(medoid[0])}/{int(medoid[1])}) - Sharpe: {float(medoid[2]):.4f}\")\n",
    "        \n",
    "        # Create strategies\n",
    "        strategies = {\n",
    "            'Best': {'short_sma': best_short_sma, 'long_sma': best_long_sma}\n",
    "        }\n",
    "        \n",
    "        # Add filtered medoids\n",
    "        for i, medoid in enumerate(filtered_medoids, 1):\n",
    "            strategies[f'Medoid_{i}'] = {'short_sma': int(medoid[0]), 'long_sma': int(medoid[1])}\n",
    "        \n",
    "        # Apply each strategy to the data\n",
    "        for name, params in strategies.items():\n",
    "            strategy = SMAStrategy(\n",
    "                short_sma=params['short_sma'],\n",
    "                long_sma=params['long_sma'],\n",
    "                big_point_value=big_point_value,\n",
    "                slippage=slippage,\n",
    "                capital=capital,\n",
    "                atr_period=atr_period\n",
    "            )\n",
    "            \n",
    "            # Apply the strategy\n",
    "            data = strategy.apply_strategy(\n",
    "                data.copy(),\n",
    "                strategy_name=name\n",
    "            )\n",
    "        \n",
    "        # Get the out-of-sample split date\n",
    "        split_index = int(len(data) * TRAIN_TEST_SPLIT)\n",
    "        split_date = data.index[split_index]\n",
    "        print(f\"Out-of-sample period starts on: {split_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        # Get out-of-sample data\n",
    "        oos_data = data.iloc[split_index:].copy()\n",
    "        \n",
    "        # Add a year and bimonthly period columns for grouping (each year has 6 bimonthly periods)\n",
    "        oos_data['year'] = oos_data.index.year.astype(int)\n",
    "        oos_data['bimonthly'] = ((oos_data.index.month - 1) // 2 + 1).astype(int)\n",
    "        \n",
    "        # Create simplified period labels with just the start month (YYYY-MM)\n",
    "        oos_data['period_label'] = oos_data.apply(\n",
    "            lambda row: f\"{int(row['year'])}-{int((row['bimonthly'] - 1) * 2 + 1):02d}\",\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Create a DataFrame to store bimonthly Sharpe ratios\n",
    "        bimonthly_sharpe = []\n",
    "        \n",
    "        # Group by year and bimonthly period, calculate Sharpe ratio for each period\n",
    "        for period_label, group in oos_data.groupby('period_label'):\n",
    "            # Skip periods with too few trading days\n",
    "            if len(group) < 10:\n",
    "                continue\n",
    "                \n",
    "            # Create a bimonthly result entry\n",
    "            year, start_month = period_label.split('-')\n",
    "            year = int(year)\n",
    "            start_month = int(start_month)\n",
    "            \n",
    "            bimonthly_result = {\n",
    "                'period_label': period_label,\n",
    "                'date': pd.Timestamp(year=year, month=start_month, day=15),  # Middle of first month in period\n",
    "                'trading_days': len(group),\n",
    "            }\n",
    "            \n",
    "            # Calculate Sharpe ratio for each strategy in this period\n",
    "            for name in strategies.keys():\n",
    "                # Get returns for this strategy in this period\n",
    "                returns = group[f'Daily_PnL_{name}']\n",
    "                \n",
    "                # Calculate Sharpe ratio (annualized)\n",
    "                if len(returns) > 1 and returns.std() > 0:\n",
    "                    sharpe = returns.mean() / returns.std() * np.sqrt(252)\n",
    "                else:\n",
    "                    sharpe = 0\n",
    "                    \n",
    "                bimonthly_result[f'{name}_sharpe'] = sharpe\n",
    "                bimonthly_result[f'{name}_return'] = returns.sum()  # Total P&L for the period\n",
    "            \n",
    "            # Calculate the normalized average of medoid Sharpe ratios\n",
    "            medoid_sharpes = [bimonthly_result[f'Medoid_{i}_sharpe'] for i in range(1, len(filtered_medoids) + 1)]\n",
    "            bimonthly_result['Avg_Medoid_sharpe'] = sum(medoid_sharpes) / len(filtered_medoids)\n",
    "            \n",
    "            # Calculate the normalized average of medoid returns\n",
    "            medoid_returns = [bimonthly_result[f'Medoid_{i}_return'] for i in range(1, len(filtered_medoids) + 1)]\n",
    "            bimonthly_result['Avg_Medoid_return'] = sum(medoid_returns) / len(filtered_medoids)\n",
    "            \n",
    "            bimonthly_sharpe.append(bimonthly_result)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        bimonthly_sharpe_df = pd.DataFrame(bimonthly_sharpe)\n",
    "        \n",
    "        # Sort the DataFrame by date for proper chronological display\n",
    "        if not bimonthly_sharpe_df.empty:\n",
    "            bimonthly_sharpe_df = bimonthly_sharpe_df.sort_values('date')\n",
    "        else:\n",
    "            print(f\"WARNING: No bimonthly periods found for {SYMBOL}. Cannot create chart.\")\n",
    "            return None\n",
    "        \n",
    "        # Add rounded values to dataframe for calculations\n",
    "        bimonthly_sharpe_df['Best_sharpe_rounded'] = np.round(bimonthly_sharpe_df['Best_sharpe'], 2)\n",
    "        bimonthly_sharpe_df['Avg_Medoid_sharpe_rounded'] = np.round(bimonthly_sharpe_df['Avg_Medoid_sharpe'], 2)\n",
    "        \n",
    "        # Print detailed comparison of Sharpe ratios\n",
    "        print(\"\\nDetailed Sharpe ratio comparison by period:\")\n",
    "        print(f\"{'Period':<12} | {'Best Sharpe':>12} | {'Medoid Portfolio':>16} | {'Difference':>12} | {'Portfolio Wins':<14}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for idx, row in bimonthly_sharpe_df.iterrows():\n",
    "            period = row['period_label']\n",
    "            best_sharpe = row['Best_sharpe']\n",
    "            avg_medoid_sharpe = row['Avg_Medoid_sharpe']\n",
    "            best_rounded = row['Best_sharpe_rounded']\n",
    "            avg_medoid_rounded = row['Avg_Medoid_sharpe_rounded']\n",
    "            \n",
    "            diff = avg_medoid_sharpe - best_sharpe\n",
    "            portfolio_wins = avg_medoid_sharpe > best_sharpe\n",
    "            \n",
    "            print(f\"{period:<12} | {best_sharpe:12.6f} | {avg_medoid_sharpe:16.6f} | {diff:12.6f} | {portfolio_wins!s:<14}\")\n",
    "        \n",
    "        # Calculate win rate using raw values\n",
    "        portfolio_wins = sum(bimonthly_sharpe_df['Avg_Medoid_sharpe'] > bimonthly_sharpe_df['Best_sharpe'])\n",
    "        total_periods = len(bimonthly_sharpe_df)\n",
    "        win_percentage = (portfolio_wins / total_periods) * 100 if total_periods > 0 else 0\n",
    "        \n",
    "        # Calculate win rate using rounded values (for alternative comparison)\n",
    "        rounded_wins = sum(bimonthly_sharpe_df['Avg_Medoid_sharpe_rounded'] > bimonthly_sharpe_df['Best_sharpe_rounded'])\n",
    "        rounded_win_percentage = (rounded_wins / total_periods) * 100 if total_periods > 0 else 0\n",
    "        \n",
    "        print(f\"\\nBimonthly periods analyzed: {total_periods}\")\n",
    "        print(f\"Medoid Portfolio Wins: {portfolio_wins} of {total_periods} periods ({win_percentage:.2f}%)\")\n",
    "        print(f\"Using rounded values (2 decimal places): {rounded_wins} of {total_periods} periods ({rounded_win_percentage:.2f}%)\")\n",
    "        \n",
    "        # Create a bar plot to compare bimonthly Sharpe ratios\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Set up x-axis dates\n",
    "        x = np.arange(len(bimonthly_sharpe_df))\n",
    "        width = 0.35  # Width of the bars\n",
    "        \n",
    "        # Create bars\n",
    "        plt.bar(x - width/2, bimonthly_sharpe_df['Best_sharpe'], width, \n",
    "            label=f'Best Sharpe ({best_short_sma}/{best_long_sma})', color='blue')\n",
    "        plt.bar(x + width/2, bimonthly_sharpe_df['Avg_Medoid_sharpe'], width, \n",
    "            label=f'Medoid Portfolio ({len(filtered_medoids)} strategies)', color='green')\n",
    "        \n",
    "        # Add a horizontal line at Sharpe = 0\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Create medoid description for the title\n",
    "        medoid_desc = \", \".join([f\"({int(m[0])}/{int(m[1])})\" for m in filtered_medoids])\n",
    "        \n",
    "        # Customize the plot - using rounded win percentage instead of raw\n",
    "        plt.title(f'Bimonthly Sharpe Ratio Comparison (Out-of-Sample Period)\\n' + \n",
    "                f'Medoid Portfolio [{medoid_desc}] outperformed {rounded_win_percentage:.2f}% of the time', \n",
    "                fontsize=14)\n",
    "        plt.xlabel('Bimonthly Period (Start Month)', fontsize=12)\n",
    "        plt.ylabel('Sharpe Ratio (Annualized)', fontsize=12)\n",
    "        \n",
    "        # Simplified x-tick labels with just the period start month\n",
    "        plt.xticks(x, bimonthly_sharpe_df['period_label'], rotation=45)\n",
    "        \n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Create legend with both strategies - moved to bottom to avoid overlap with title\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2,\n",
    "                frameon=True, fancybox=True, framealpha=0.9, fontsize=10)\n",
    "        \n",
    "        # Add a text box with rounded win percentage instead of raw - moved to right side\n",
    "        plt.annotate(f'Medoid Portfolio Win Rate: {rounded_win_percentage:.2f}%\\n'\n",
    "                    f'({rounded_wins} out of {total_periods} periods)\\n'\n",
    "                    f'Portfolio: {len(filtered_medoids)} medoids with Sharpe  {min_sharpe}\\n'\n",
    "                    f'ATR-Based Position Sizing (${capital:,}, {atr_period} days)',\n",
    "                    xy=(0.7, 0.95), xycoords='axes fraction',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "                    fontsize=12)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])  # Add extra space at the bottom for the legend\n",
    "        save_plot('Hierarchical_Bimonthly_Comparison.png')\n",
    "        \n",
    "        # Save bimonthly comparison to CSV\n",
    "        save_results(bimonthly_sharpe_df, f\"{SYMBOL}_hierarchical_bimonthly_comparison.csv\")\n",
    "        \n",
    "        # Save win percentage and medoid parameters to Excel file\n",
    "        try:\n",
    "            import openpyxl\n",
    "            \n",
    "            # Path to the Excel file\n",
    "            excel_file = r\"C:\\Users\\Admin\\Documents\\darbas\\Results.xlsx\"\n",
    "            \n",
    "            # Check if file exists\n",
    "            if not os.path.exists(excel_file):\n",
    "                print(f\"Excel file not found at: {excel_file}\")\n",
    "                return bimonthly_sharpe_df\n",
    "                \n",
    "            print(f\"Updating Excel file with Hierarchical results for {SYMBOL}...\")\n",
    "            \n",
    "            # Load the workbook\n",
    "            wb = openpyxl.load_workbook(excel_file)\n",
    "            \n",
    "            # Get the active sheet\n",
    "            sheet = wb.active\n",
    "            \n",
    "            # Find the row with the ticker symbol or the first empty row\n",
    "            row = 3  # Start from row 3 (assuming rows 1-2 have headers)\n",
    "            ticker_row = None\n",
    "            \n",
    "            while True:\n",
    "                cell_value = sheet.cell(row=row, column=1).value\n",
    "                if cell_value == SYMBOL:\n",
    "                    # Found the ticker symbol\n",
    "                    ticker_row = row\n",
    "                    break\n",
    "                elif cell_value is None:\n",
    "                    # Found an empty row\n",
    "                    ticker_row = row\n",
    "                    # Write the ticker symbol in column A\n",
    "                    sheet.cell(row=ticker_row, column=1).value = SYMBOL\n",
    "                    break\n",
    "                row += 1\n",
    "            \n",
    "            # Round the win percentage to one decimal place\n",
    "            rounded_win_percentage_1dp = round(rounded_win_percentage, 1)\n",
    "            \n",
    "            # Write the win percentage in column C (Hierarchy)\n",
    "            sheet.cell(row=ticker_row, column=3).value = rounded_win_percentage_1dp\n",
    "            \n",
    "            # Write the medoid parameters in the respective cluster columns\n",
    "            # Hierarchy clusters start at column I (9)\n",
    "            for i, medoid in enumerate(filtered_medoids):\n",
    "                if i >= 3:  # Only use up to 3 clusters\n",
    "                    break\n",
    "                    \n",
    "                # Calculate column index: I=9, J=10, K=11 for Cluster1, Cluster2, Cluster3\n",
    "                column_idx = 9 + i\n",
    "                \n",
    "                # Format as \"short/long\" (e.g., \"5/20\")\n",
    "                param_value = f\"{int(medoid[0])}/{int(medoid[1])}\"\n",
    "                \n",
    "                # Write to Excel\n",
    "                sheet.cell(row=ticker_row, column=column_idx).value = param_value\n",
    "            \n",
    "            # Write the best Sharpe parameters in column M (13)\n",
    "            best_sharpe_params = f\"{best_short_sma}/{best_long_sma}\"\n",
    "            sheet.cell(row=ticker_row, column=13).value = best_sharpe_params\n",
    "            \n",
    "            # Save the workbook\n",
    "            wb.save(excel_file)\n",
    "            \n",
    "            print(f\"Excel file updated successfully. Added {SYMBOL} with Hierarchical win rate {rounded_win_percentage_1dp}% in row {ticker_row}\")\n",
    "            print(f\"Added best Sharpe parameters {best_sharpe_params} in column M\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating Excel file: {e}\")\n",
    "        \n",
    "        # Return the bimonthly Sharpe ratio data\n",
    "        return bimonthly_sharpe_df\n",
    "\n",
    "    def create_hierarchical_cluster_visualization(X_filtered_full, medoids, top_medoids, max_sharpe_point, labels, valid_clusters):\n",
    "        \"\"\"\n",
    "        Create a continuous heatmap visualization with hierarchical cluster centers overlaid.\n",
    "        \n",
    "        Parameters:\n",
    "        X_filtered_full: array - All filter-compliant data points with shape (n_samples, 4)\n",
    "        medoids: list - List of medoids from valid clusters\n",
    "        top_medoids: list - List of top medoids by Sharpe ratio\n",
    "        max_sharpe_point: array - Point with maximum Sharpe ratio\n",
    "        labels: array - Cluster labels for each point in X_filtered_full\n",
    "        valid_clusters: set - Set of valid cluster IDs that meet the min_elements_per_cluster requirement\n",
    "        \"\"\"\n",
    "        print(\"Creating hierarchical cluster visualization...\")\n",
    "\n",
    "        # Load the full dataset for pivot table creation\n",
    "        data = pd.read_csv('sma_all_results.txt')\n",
    "        \n",
    "        # Create filtered dataframe from X_filtered_full but include ALL points\n",
    "        # We will NOT filter by valid_clusters yet - this matches the K-means approach\n",
    "        filtered_df = pd.DataFrame(X_filtered_full, columns=['short_SMA', 'long_SMA', 'sharpe_ratio', 'trades'])\n",
    "        \n",
    "        # Create a pivot table for the heatmap using ALL filtered data points (not just valid clusters)\n",
    "        heatmap_data = filtered_df.pivot_table(\n",
    "            index='long_SMA',\n",
    "            columns='short_SMA',\n",
    "            values='sharpe_ratio',\n",
    "            fill_value=np.nan  # Use NaN for empty cells\n",
    "        )\n",
    "\n",
    "        # Create the heatmap visualization\n",
    "        plt.figure(figsize=(12, 10))\n",
    "\n",
    "        # Create mask for invalid combinations (where short_SMA >= long_SMA)\n",
    "        # and also for NaN values (which represent filtered-out points)\n",
    "        mask = np.zeros_like(heatmap_data, dtype=bool)\n",
    "        for i, long_sma in enumerate(heatmap_data.index):\n",
    "            for j, short_sma in enumerate(heatmap_data.columns):\n",
    "                if short_sma >= long_sma or np.isnan(heatmap_data.iloc[i, j]):\n",
    "                    mask[i, j] = True\n",
    "\n",
    "        # Plot the base heatmap with ALL filtered data\n",
    "        ax = sns.heatmap(\n",
    "            heatmap_data,\n",
    "            mask=mask,\n",
    "            cmap='coolwarm',  # Blue to red colormap\n",
    "            annot=False,      # Don't annotate each cell with its value\n",
    "            fmt='.4f',\n",
    "            linewidths=0,\n",
    "            cbar_kws={'label': 'Sharpe Ratio'}\n",
    "        )\n",
    "\n",
    "        # Invert the y-axis so smaller long_SMA values are at the top\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        # Plot max Sharpe point (Green Star)\n",
    "        try:\n",
    "            best_x_pos = np.where(heatmap_data.columns == max_sharpe_point[0])[0][0] + 0.5\n",
    "            best_y_pos = np.where(heatmap_data.index == max_sharpe_point[1])[0][0] + 0.5\n",
    "            plt.scatter(best_x_pos, best_y_pos, marker='*', color='lime', s=200,\n",
    "                        edgecolor='black', zorder=5)\n",
    "        except IndexError:\n",
    "            print(f\"Warning: Max Sharpe point at ({max_sharpe_point[0]}, {max_sharpe_point[1]}) not found in heatmap coordinates\")\n",
    "\n",
    "        # Only plot medoids (these are already from valid clusters)\n",
    "        if medoids:\n",
    "            for medoid in medoids:\n",
    "                try:\n",
    "                    x_pos = np.where(heatmap_data.columns == medoid[0])[0][0] + 0.5\n",
    "                    y_pos = np.where(heatmap_data.index == medoid[1])[0][0] + 0.5\n",
    "                    plt.scatter(x_pos, y_pos, marker='s', color='black', s=75, zorder=4)\n",
    "                except IndexError:\n",
    "                    print(f\"Warning: Medoid at ({medoid[0]}, {medoid[1]}) not found in heatmap coordinates\")\n",
    "\n",
    "        # Plot top 5 medoids (Purple Diamonds)\n",
    "        if top_medoids:\n",
    "            for medoid in top_medoids:\n",
    "                try:\n",
    "                    x_pos = np.where(heatmap_data.columns == medoid[0])[0][0] + 0.5\n",
    "                    y_pos = np.where(heatmap_data.index == medoid[1])[0][0] + 0.5\n",
    "                    plt.scatter(x_pos, y_pos, marker='D', color='purple', s=100, zorder=5)\n",
    "                except IndexError:\n",
    "                    print(f\"Warning: Top medoid at ({medoid[0]}, {medoid[1]}) not found in heatmap coordinates\")\n",
    "\n",
    "        # Create custom legend\n",
    "        max_sharpe_handle = mlines.Line2D([], [], color='lime', marker='*', linestyle='None',\n",
    "                                        markersize=15, markeredgecolor='black', label='Max Sharpe')\n",
    "        medoid_handle = mlines.Line2D([], [], color='black', marker='s', linestyle='None',\n",
    "                                    markersize=10, label='Medoids')\n",
    "        top_medoid_handle = mlines.Line2D([], [], color='purple', marker='D', linestyle='None',\n",
    "                                        markersize=10, label='Top 5 Medoids')\n",
    "\n",
    "        # Add legend\n",
    "        plt.legend(handles=[max_sharpe_handle, medoid_handle, top_medoid_handle],\n",
    "                loc='best')\n",
    "\n",
    "        # Set labels and title\n",
    "        plt.title('Hierarchical Clustering Analysis (Sharpe Ratio)', fontsize=14)\n",
    "        plt.xlabel('Short SMA (days)', fontsize=12)\n",
    "        plt.ylabel('Long SMA (days)', fontsize=12)\n",
    "\n",
    "        # Rotate tick labels\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "\n",
    "        # Display plot\n",
    "        plt.tight_layout()\n",
    "        save_plot('Hierarchical_Cluster_Analysis.png')\n",
    "\n",
    "    # Main execution block\n",
    "    # Main execution block\n",
    "    if __name__ == \"__main__\":\n",
    "            # Set matplotlib backend explicitly\n",
    "            # Main execution block\n",
    "            # Set matplotlib backend explicitly\n",
    "            matplotlib.use('Agg')  # Use non-interactive backend for headless environments\n",
    "\n",
    "            print(\"Starting ATR-based SMA strategy analysis...\")\n",
    "\n",
    "            # Run the basic analysis first\n",
    "            data, best_short, best_long, best_sharpe, best_trades = analyze_sma_results()\n",
    "\n",
    "            if data is None:\n",
    "                print(\"Error: Failed to load or analyze SMA results data.\")\n",
    "                exit(1)\n",
    "\n",
    "            print(\"\\nProceeding with cluster analysis...\")\n",
    "\n",
    "            # Run the cluster analysis to get medoids\n",
    "            X_filtered, medoids, top_medoids, max_sharpe_point, labels = hierarchical_cluster_analysis()\n",
    "\n",
    "            if X_filtered is None or medoids is None:\n",
    "                print(\"Error: Cluster analysis failed.\")\n",
    "                exit(1)\n",
    "\n",
    "            # Re-sort top_medoids to ensure they're in the right order\n",
    "            if top_medoids is not None:\n",
    "                print(\"Re-sorting top medoids by Sharpe ratio...\")\n",
    "                # CRITICAL FIX: Use float() to ensure proper numeric comparison\n",
    "                top_medoids = sorted(top_medoids, key=lambda x: float(x[2]), reverse=True)\n",
    "                for idx, medoid in enumerate(top_medoids, 1):\n",
    "                    print(f\"Verified Medoid {idx}: Short SMA={int(medoid[0])}, Long SMA={int(medoid[1])}, \"\n",
    "                        f\"Sharpe={float(medoid[2]):.4f}, Trades={int(medoid[3])}\")\n",
    "\n",
    "            print(\"\\nPlotting strategy performance...\")\n",
    "\n",
    "            # Plot strategy performance with the best parameters AND top medoids using ATR-based position sizing\n",
    "            market_data = plot_strategy_performance(\n",
    "                best_short, best_long, top_medoids, \n",
    "                big_point_value=big_point_value,\n",
    "                slippage=slippage,\n",
    "                capital=capital,\n",
    "                atr_period=atr_period\n",
    "            )\n",
    "            \n",
    "            # Run the bimonthly out-of-sample comparison between best Sharpe and top medoids\n",
    "            if top_medoids and len(top_medoids) > 0:\n",
    "                print(\"\\nPerforming bimonthly out-of-sample comparison with hierarchical clustering...\")\n",
    "                bimonthly_sharpe_df = bimonthly_out_of_sample_comparison(\n",
    "                    market_data, \n",
    "                    best_short, \n",
    "                    best_long, \n",
    "                    top_medoids,  # Pass the entire list of top medoids\n",
    "                    big_point_value=big_point_value,\n",
    "                    slippage=slippage,\n",
    "                    capital=capital,\n",
    "                    atr_period=atr_period\n",
    "                )\n",
    "            else:\n",
    "                print(\"No top medoids found. Cannot run bimonthly comparison.\")\n",
    "                \n",
    "            print(\"\\nAnalysis complete! All plots and result files have been saved to the output directory.\")\n",
    "            print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Call the main function to execute the script\n",
    "if __name__ == \"__main__\":\n",
    "    data_analysis_hierarchy_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f0f7e",
   "metadata": {},
   "source": [
    "## 7. Main Execution\n",
    "This cell will run the complete analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis_for_ticker(ticker):\n",
    "    global TICKER  # We'll modify the global TICKER variable\n",
    "    TICKER = ticker  # Set the ticker for this run\n",
    "    print(f\"\\n=== STARTING ANALYSIS FOR {TICKER} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Run data gathering\n",
    "        print(\"=== RUNNING DATA GATHERING ===\")\n",
    "        data_gather_main()\n",
    "        \n",
    "        # Run K-means analysis\n",
    "        print(\"\\n=== RUNNING K-MEANS ANALYSIS ===\")\n",
    "        data_analysis_main()\n",
    "        \n",
    "        # Run hierarchical analysis\n",
    "        print(\"\\n=== RUNNING HIERARCHICAL ANALYSIS ===\")\n",
    "        data_analysis_hierarchy_main()\n",
    "        \n",
    "        print(f\"\\nCompleted analysis for {TICKER}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {TICKER}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main(tickers=None):\n",
    "    \"\"\"\n",
    "    Run analysis for specified tickers or all underlyings\n",
    "    Args:\n",
    "        tickers: List of tickers to analyze. If None, uses all underlyings\n",
    "    \"\"\"\n",
    "    if tickers is None:\n",
    "        tickers = underlyings\n",
    "    elif isinstance(tickers, str):\n",
    "        tickers = [tickers]  # Convert single ticker to list\n",
    "    \n",
    "    successful = []\n",
    "    failed = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        print(f\"\\nProcessing {ticker}...\")\n",
    "        if run_analysis_for_ticker(ticker):\n",
    "            successful.append(ticker)\n",
    "        else:\n",
    "            failed.append(ticker)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== ANALYSIS SUMMARY ===\")\n",
    "    print(f\"Successfully analyzed: {len(successful)} tickers\")\n",
    "    print(f\"Failed to analyze: {len(failed)} tickers\")\n",
    "    if failed:\n",
    "        print(\"Failed tickers:\", failed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08051c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
